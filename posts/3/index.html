
<!DOCTYPE HTML>
<html>
<head>
	<script data-cfasync="false" type="text/javascript" src="//use.typekit.net/axj3cfp.js"></script>
	<script data-cfasync="false" type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<meta charset="utf-8">
	<title>  | Wing of Dream 梦境之翼</title>

<meta name="author" content="Han Yi"> 

<meta name="description" content="一年前，我们曾介绍并讨论了展示在siggraph'10 Talks上、作为Disney&rsquo;s 50th Feature Animation的《Tangled》。在今年8月7日至11日的温哥华siggraph'11大会上，工业光魔携首部动画长篇《Rango》成为了“Let there &hellip;"> <meta name="keywords" content="">

	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Wing of Dream 梦境之翼" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script type="text/javascript" src="/javascripts/jquery.fancybox.pack.js"></script>

<script language="Javascript" type="text/javascript">
$(document).ready(
  function() {
    (function($) {
      $(".fancybox[data-content-id]").each(function() {
        this.href = $(this).data('content-id');
      });
      $(".fancybox").fancybox({
        beforeLoad: function() {
          var el, 
              id = $(this.element).data('title-id');

          if (id) {
            el = $('#' + id);

            if (el.length) {
              this.title = el.html();
            }
          }
          if ($(this).data('content')) {
            this.content = $(this).data('content');
          }
        },
        helpers: {
          title: {
            type: 'inside'
          }
        }
      });
    })(jQuery);
  }
);
</script>

	
</head>



<body>
	<header id="header" class="inner"><h1><a href="/">Wing of Dream 梦境之翼</a></h1>
<h4></h4>
<nav id="main-nav"><ul>
	<li><a href="/">Blog</a></li>
	<li><a href="/about">About</a></li>
	<li><a href="/portfolio">Portfolio</a></li>
	<li><a href="/archives">Archive</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul>
	<li><a href="/">Blog</a></li>
	<li><a href="/about">About</a></li>
	<li><a href="/portfolio">Portfolio</a></li>
	<li><a href="/archives">Archive</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="https://www.google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:www.hanyi.name">
			</form>
		</div>
	</div>
</nav>


</header>

	<div id="content" class="inner">


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/10/22/cong-SIGGRAPH-11-Rango/">
		
			从SIGGRAPH&#8217;11看《Rango》</a>
	</h2>
	<div class="entry-content">
		<p>一年前，我们曾介绍并讨论了展示在siggraph&#8217;10 Talks上、作为Disney&rsquo;s 50th Feature Animation的《Tangled》。在今年8月7日至11日的温哥华siggraph&#8217;11大会上，工业光魔携首部动画长篇《Rango》成为了“Let there be light”（加点儿灯光？）单元当之无愧的主角（尽管去年的Tangled和今年的Cars 2同时均有入选）。</p>

<p>《Rango》中文译名“兰戈”，由执导了前三部加勒比海盗系列的Gore Verbinski担任导演，该片作为工业光魔在Feature Animation（FA）领域的处女之作，使得后者在首次试水FA后即登上了巅峰。影片主要讲述了一只不慎被遗失在西部荒漠中的蜥蜴，历经艰难、最终找回自我的故事。影片无论在情节构思、画面、配乐上的精细程度都堪称前所未有。本文依旧从Siggraph&#8217;11上的三篇文章出发，试图探索堪称计算机图形学史上迄今最先进的工业级CG技术。</p>

<p><strong> 使用面向FX工具制作动画角色和光照特效</strong></p>

<p>在拍摄初期，工作人员在虚拟转台上测试了整个ILM的材质库，其意在测试材质的环境光遮蔽和阴影效果。然而在影片中，除了60名主要角色以外，还需要大约1000个道具。为了简化这部分工作，这里还使用了部分置换贴图以丰富材质信息。同时，再将这些材质用于不同环境的转台程序下，以便迅速发现问题并制作新的Shader；</p>

<p>过去通常使用光板或参考集设置光线，然而《Rango》则例外，这部分影片中引入了新的方法。每一组镜头中的光照设置非常简单，即包括关键光、环境光、反射光三部分，该方法被用于全部的室外光和一些室内光。技术人员使用Digital Matte部门开发的虚拟集渲染器创建环境光和反射光映射，而非直接进行360度捕获。上述映射被用于非静止的、处于快速移动物体中的反射/折射光精确表示。设置灯光时，关键光通过一个受可切变天空顶约束的远程光Shader定义，并通过复制角色的远程关键光，达到较好的交互和阴影效果。同时，用户还可以通过旋转整个装置以达到尽可能广泛的变化，或只旋转关键光以改变复制光，或通过特定的扭曲修改角色局部的复制光。</p>

<p>除个别场景外，影片几乎所有的室外场景都使用了前文所述的格式化远程关键光、环境光和反射光装置。该装置包含两个开关：一是激活阳光反射卡片（模拟光晕效果），另一个用于开启基于距离的阴影模糊。</p>

<p>假设你在一个阳光明媚的室外场所，挥动手臂并在一张白纸上映上阴影效果，你会发现当你升起手臂时，阴影边界的模糊度随之逐级递增，这种边界阴影模糊在物理学中被称为“半影”现象。物理学中已经证明，半影宽度除以障碍物至阴影面的距离值约为常数1/112，它同时也是太阳以弧度为单位角直径值。工作人员实现了基于距离的关键光阴影边界模糊并对其进行严格控制，力求在符合真实世界效果的基础上尽可能满足导演对夸张手法的要求。</p>

<p>简易的关键光/环境光设置方法是设置全局光照参数，并将其作为一组镜头的初始光照。整部影片也可以通过该方法初始化并检测通用关键光的特效和镜头效果。每当TD（Technical Directors）布置新任务时，他们就可以通过添加特定的光照效果以丰富镜头。出于对主角显著区别于背景的绘制需要，工作人员对其进行了逐帧细化和调整。</p>

<p>角色合成方面起初的构想十分简单，然而随后愈加复杂，这种复杂程度甚至超过了真实的角色电影。导演对每一个场景均有特定的灯光、编排和镜头要求，还包括变化的气候、照片级真实感绘制（光畸变，正确的燃烧扭曲，热涟波和相机抖动等）。额外还包括了沙漠、指纹、水、尘土等各种细节效果。为此制作团队特组织了一批强大的计算机开发团队，通过创建自定义通用脚本制作了多个工具辅助影片拍摄。</p>

<p><strong> 动物皮毛的穿透避免和动力学</strong></p>

<p>在《Tango》中需要对多个鸟类角色建立羽毛模型，而这种羽毛模型通常需要手动放置主要的羽毛茎秆曲线，再在不允许插值的条件下实例化整个皮毛几何体。此外，影片对动物角色的要求还包括了日常姿态、呼吸动作、受风、动量、惯性影响的效果等。这里制作团队开发了一种尽可能保持原设计形态的皮毛动力学模拟和自由碰撞技术。</p>

<p>鸟类的羽毛是通过分层有序叠加的，开发人员应允许设计师能够任意设计皮毛的基本外形、层次结构，且不必处心积虑地避免皮毛穿透问题。设计师通过在角色皮肤上放置引导曲线，以构建基本的皮毛外观，同时在其径向上实例化一个矩形几何框，用户可据此设置宽度、扭曲度、与起始点法线相似度等参数。该四边形面片同时还被用于在绘制阶段实现多种纹理效果。</p>

<p>制作团队共使用了两个实例化的皮毛集合，一是目标集，包括了许多皮毛间的相互穿透效果。二是特征皮毛根集，这部分使用狭长的面片实现，由于其更接近法线方向，故存在较少的互穿透。后者用于对某些动物的皮毛进行初始化和带穿透避免的布料模拟。与[Weber and Gornowicz 2009]提出的复杂方法不同，下一步是使用非身体力设置布料模拟的皮毛。每一个布片的基被限制在一个曲面片上，据此检测面片间的相互碰撞。目标集则是通过布料解析器限制下点接点的受力建立。在模拟过程中，布料面片将面向目标集方向生长、然后由碰撞检测部分避免穿透。碰撞避免的方法可以使用基于三角形或是基于边的排斥方法。在第一步完成后，仍需要进一步对角色动作过程中的皮毛穿透现象进行检测， 同时还要保证对原设计形态的维持。</p>

<p><strong> 插值毛发的碰撞精度</strong></p>

<p>毛发的动力学常常是通过引导毛发和成员插值构成的，因此这部分也被称为插值毛发。碰撞精度是毛发模拟技术中最费时的阶段之一，因此通过限制毛发的碰撞精度从而提升动力学性能是非常重要的。在许多情况下，只需要视觉感官上可被接受即可不需增加碰撞精度，遗憾的是，我们的运气往往没有这么好：）</p>

<p>影片中许多动物都穿有衣服，那么即使是一根发丝从衣服布料中伸出来都是不可接受的。然而对整个已生成的毛发集合进行穿透检测和避免也是不现实的：首先，这种技术非常耗时，插值后的毛发往往是引导毛发的成百上千倍；其次，这种方法并不能用于目前的管线设置，因为在模拟阶段插值头发尚不存在；最后，基于物理的碰撞检测方法通常是使用多边形面片，而非Catmull-Clark细分曲面表示，即使是极小的曲面差异也很难保证碰撞检测的正确性。</p>

<p>这里制作团队提出了一个新的穿透修正方法，该方法主要解决的问题包括精度、效率和时间相关性，其主要贡献是对平滑曲面表示进行碰撞检测，沿着毛发生长方向进行连续碰撞检测，并在每一帧进行碰撞检测。该方法最终在影片多处被用到，它既保证了高效率，同时精度也达到了无需进行后期修正的程度。</p>

<p>文中工作人员使用了[Verma et al. 2009]将CC曲面转换为Bezier表示，这种显式曲面允许通过精确交叉查询解决碰撞检测的问题。其次，使用层次数据结构对毛发和Bezier面片进行候选碰撞检测。由于当前配对可能发生了穿透，将毛发的生长方向再移动至其参数空间中的上层部分。通过沿参数化的曲线空间搜索连续毛发-曲面交叉，其中使用Newton法计算毛发和曲面交叉部分。</p>

<p>在穿透避免时很容易造成无法保持原始形态的问题，这里使用了一种简便方法以保持形态，即在穿透避免后使用[Goldenthal et al. 2007]提出的快速投影法修正其差异长度，然后在碰撞精度和应用不可延展间进行迭代，直到满足两方面需求即停止。实际应用中只需要很少的迭代次数即可完成计算。</p>

<p>插值头发绘制时逐步生成的，同时还要同步进行穿透避免计算。因此需要在每一帧进行独立的穿透避免计算（并行处理），这条限制有一定隐患，即可能导致发生时间无干性：毛发可能在连续帧中发生不平滑。这里通过尽量使其朝向生长方向以降低不平滑的程度，尽管尚不完美，但已经足够达到目的了。</p>

<p><strong> 总结</strong></p>

<p>《Rango》无疑是siggraph&#8217;11大会上一颗耀眼的明珠。与去年的《Tangled》相比，该片的制作成本要少一亿三千万美元（估计成本一亿三千五百万），前者更是花费了Disney Animation Studio长达六年时间。不过，这次ILM的口碑和舆评显然超过了Disney，正如片尾Rango骑着白鸡，发表了如下慷慨激昂的讲话：</p>

<p>My fellow comrades.</p>

<p>There will be times when you doubt yourself.</p>

<p>When you feel pummeled by the cataclysms of life,</p>

<p>Remember this moment. Remember me.</p>

<p>Know that I will be there watching you,</p>

<p>Sometimes at inappropriate moments.</p>

<p>That&rsquo;s part of the deal.</p>

<p>And remember, within all of us resides</p>

<p>The true spirit of the&hellip;</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-10-22T02:45:54+08:00" pubdate data-updated="true">2011-10-22</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/ke-shi-hua/'>可视化</a>

</div>


	
		<span class="comments"><a href="/blog/2011/10/22/cong-SIGGRAPH-11-Rango//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/09/24/KINECT-lai-le-jie-xi-jie-xi-SDK/">
		
			KINECT来了——解析SDK(MS SDK 2)</a>
	</h2>
	<div class="entry-content">
		<p><strong>NUI图像数据流概述</strong></p>

<p>NUI的流数据是通过连续静态图像序列传递的。在上下文初始化阶段，应用程序将识别需要读取的流数据，并对其进行附加的流相关设置，包括数据解析度、图像类型、用于存储输入帧的缓冲区数量等内容。在应用程序检索并释放相关帧之前，如果运行时数据占满了缓冲区，那么系统将自动丢弃最旧的帧并重用缓冲区，也就是说，帧数据是可被丢弃的。同时系统最多允许请求四个缓冲区，而在大多数应用情形下通常只需要其中的两个。应用程序可通过API获取如下类型的图像信息：彩色图像数据、深度图像数据、用户分割数据等。下面将分别对上述三种类型的数据进行一些说明。</p>

<p>彩色图像数据：系统提供两种格式的彩色图像数据，包括32位X8R8G8B8格式的sRGB位图数据和16位的UYVY格式的YUV位图数据。由于两者实际来自同一图像数据，因此两种格式的最终图像实际上没有任何差别。不过后者要求图像保持640x480的分辨率和15FPS的帧率，同时内存需求更小。应注意，由于系统采用USB连接，传感器层会首先将1280x1024分辨率的Bayer彩色滤波马赛克图像压缩并转换为RGB格式传输，运行时系统再对该数据进行解压缩操作。上述特性可以保证数据帧率可达30FPS，但解码操作会损失一定的图像精度。</p>

<p>深度图像数据：深度图像帧中的每一个像素点都表示从摄像头所在平面到视野内最近物体的笛卡尔坐标系距离，单位为毫米。系统目前支持630x480、320x240、80x60三种规格的深度图像帧。应用程序可根据深度图像数据跟踪人物动作、识别并忽略背景物体。深度图像数据含有两种格式，其一是唯一表示深度值：那么像素的低12位表示一个深度值，高4位未使用；其二是既表示深度值又含有人物序号，则低三位保存人物序号，其余数据位表示深度值。应注意如果获取的深度值为0，则说明物体距离摄像头过近或过远以致超出了设备规格。</p>

<p>用户分割数据：SDK beta目前支持读取两个用户的分割映射数据，其数据帧的相关像素分别记录了用户的序号。尽管用户分割数据是独立生成的数据流，在实际应用中仍可以将深度数据和用户分割数据整合成一个帧，其中像素值的高13位保存了深度值，低三位保存用户序号，其中序号为0则表示无用户，1和2分别表示两个不同的用户。在实际应用中，应用程序往往利用用户分割数据在深度图像和原始彩色图像中获取ROI感兴趣信息。</p>

<p><strong> 基本编程模型</strong></p>

<p>基于NUI API的编程模型本质上就是获取传感器图像数据的过程。应用程序往往通过相关代码首先将图像的最后一帧读入至缓冲区中，如果该帧已预备好，那么其将进入缓冲区，如果帧数据尚未就绪，代码仍可选择是否继续挂起等待或暂时释放并稍后重试。NUI摄像头API绝对不会多次传递相同的图像数据。框架包含的基本编程模型如下：</p>

<p>1、POLLing模型，该模型较为基础易用。首先应开启图像数据流，然后请求并设置等待下一帧的时间，范围允许从0到无穷大，单位为毫秒；如果帧数据尚未就绪，则系统将等待刚才指定的时间然后返回。如果帧数据成功返回，则应用程序可请求下一帧数据并在同一线程执行其它操作。通常一个C++应用程序应调用NuiImageStreamOpen函数首先启动一个彩色或深度数据流，并忽略可选事件。托管代码则需调用ImageStream.Open方法。请求彩色或深度图像帧的C++函数为NuiImageStreamGetNextFrame，C#为ImageStream.GetNextFrame方法。</p>

<p>2、事件模型，事件模型允许将获取骨骼帧的功能精确、灵活地集成入应用程序引擎。在该模型中，C++程序首先调用NuiImageStreamOpen函数并传入一个事件句柄。每当一个新的图像帧数据可用时，事件信号将被触发。任何相关的等待线程将被唤醒并通过调用NuiImageGetNextFrame函数获取骨骼信息，与此同时事件将被系统重置。托管代码应绑定Runtime.DepthFrameReady和Runtime.ImageFrameReady事件到相关的处理函数，当新数据可用时，处理函数可调用ImageStream.GetNextFrame获取该数据。</p>

<p><strong>NUI骨骼跟踪应用</strong></p>

<p>NUI还包括一个Skeleton骨骼跟踪模块，该部分提供最多两名用户的详细位置和朝向信息。骨骼跟踪的输出是一个点集，称作Skeleton Positions，该点集表示了一个完整的人体骨骼信息，如下图所示。</p>

<p>[singlepic id=44 w=320 h=240 mode=watermark float=center]</p>

<p>骨骼位置信息表示了用户当前的位置和姿态，如果要使用骨骼跟踪功能，应用程序应在NUI初始化阶段设置相关内容。NUI骨骼数据获取的方式与NUI Image部分基本一致，其基本编程模型也包括Polling和Event两种。前者的C++函数为NuiSkeletonGetNextFrame，托管函数为SkeletonEngine.GetNextFrame；后者的C++句柄绑定操作由NuiSkeletonTrackingEnable完成，并在处理线程内调用NuiSkeletonGetNextFrame；C#则使用Runtime.SkeletonFrameReady绑定事件，然后调用SkeletonEngine.GetNextFrame获取相关信息。</p>

<p>骨骼跟踪模块通过深度数据计算地板裁切面，其基本方法将在后文进行介绍。如果应用程序在NUI初始化时开启了骨骼跟踪功能，则其每处理完一套深度数据则就放出相应的骨骼数据信号，而无论该深度数据中是否真的包含骨骼信息。应用程序使用地板裁切面数据获取骨骼框架，返回的骨架信息将携带一个时间戳以和相关的深度信息进行匹配。</p>

<p>NUI骨骼跟踪分主动和被动两种模式，提供最多两副完整的骨骼跟踪数据。主动模式下需要调用相关帧读取函数获得用户骨骼数据，而被动模式下还支持额外最多四人的骨骼跟踪，但是在该模式下仅包含了用户的位置信息。对于所有获取的骨骼数据，其至少包含以下信息：</p>

<p>1、相关骨骼的跟踪状态，被动模式时仅包括位置数据，主动模式包括完整的骨骼数据。</p>

<p>2、唯一的骨骼跟踪ID，用于分配给视野中的每个用户。</p>

<p>3、用户质心位置，该值仅在被动模式下可用。</p>

<p>4、对于主动模式下的骨骼跟踪数据，还包括用户完整的骨骼数据。</p>

<p>5、对于被动模式下的骨骼跟踪数据，仅包括用户位置信息，不包括详细的骨骼数据。</p>

<p><strong>NUI坐标变换原理</strong></p>

<p>深度图像空间：这是一个仅包含物体到传感器法平面的深度数据的投影空间，其z值是唯一有效的，而x、y值仅仅是进行了插值计算的结果，其数据实际并无任何物理意义；</p>

<p>骨骼空间：用户骨骼位置使用x、y、z三维坐标表示，与深度图像空间坐标系不同的是，该空间的单位为米，且是一个传感器朝向为z轴正向的右手系。应注意，骨骼空间坐标系与Kinect位置息息相关，如果传感器被放置在一个非水平面上，那么计算得到的坐标系可能并非是标准形式，最终的用户数据也有可能是倾斜的。</p>

<p>地板裁剪面的确定：骨骼数据均需要包含一个地板裁剪面向量，该向量保存了与地板平面方程有关的系数coefficients，骨骼跟踪模块通过地板裁剪平面除去背景，并将用户图像分割出来。对于平面的一般式方程Ax+By+Cz+D=0，该方程经过规范化即D值实际是指传感器到地板平面的高度值。如果地板不可见，那么地板裁剪平面向量实际为0。地板裁剪面向量值可在NUI_SKELETON_FRAME结构的vFloorClipPlane成员中获取。托管代码则保存在SkeletonFrame.FloorClipPlane域中。</p>

<p>骨骼镜像：默认的用户图像实际上是一个镜像数据，也就是说该数据表示了用户当前面朝屏幕内部。然而有时确实需要图像面向用户本人一侧，那么需要确定一个镜像变换矩阵以达到此类要求。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-09-24T23:09:39+08:00" pubdate data-updated="true">2011-09-24</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/09/24/KINECT-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/09/17/KINECT-lai-le-jie-xi-jie-xi-SDK/">
		
			KINECT来了——解析SDK(MS SDK 1)</a>
	</h2>
	<div class="entry-content">
		<p>在8月初的<a href="http://www.hanyi.name/blog/?p=330" target="_blank">文章</a>里，我们曾对Microsoft推出的官方Kinect SDK beta进行了初步介绍，本文将在此基础上对其做出进一步说明。值得注意的是对托管代码和非托管代码的选择，从社区反馈看来似乎前者占据了绝对上风，coding4fun推荐的项目也基本都是以WPF编程为主。不过考虑到平台的潜在可扩展，这里还是推荐使用标准C++构建非托管程序。下面先给出两种编程方式在VS2010中的配置方法：</p>

<p>对于托管C#程序，添加Add Reference中.Net标签页的Microsoft.Research.Kinect.dll，使用using Microsoft.Research.Kinect.Nui引用NUI API的声明文件，如果还要引用音频API，还要加上using Microsoft.Research.Kinect.Audio。</p>

<p>对于标准C++程序，代码中必须加入&lt;windows.h&gt;头文件，其中NUI API需要包含MSR_NuiApi.h头文件、音频API需要包含MSRKinectAudio.h。另外需要添加相应的静态链接库，同时确保安装的DLL文件存放在PATH路径中。这里给出SDK所包含的头文件说明：</p>

<p>MSR_NuiApi.h，该文件包含所有关于NUI API声明，包括初始化调用和访问函数，如NuiInitialize、NuiShutdown、MSR_NuiXxx以及INuiInstance，其主要功能是枚举设备并对其进行访问调用。MSR_NuiImageCamera.h，该文件包含对NUI图像和摄像头功能API的声明，其一般形式为NuiCameraXxx和NuiImageXxx：此类API的功能为调整摄像头倾角和仰角，并启动数据流并读入图像帧。MSR_NuiProps.h，该文件包含对NUI属性枚举API函数的声明。MSR_NuiSkeleton.h，该文件包括NUI骨架API函数的声明，其一般形式为NuiSkeletonXxx and NuiTransformXxx：功能为打开/关闭骨架跟踪、获取骨架数据、将骨架数据进行变换以实现平滑绘制。MSRKinectAudio.h，该文件包括对音频API函数的声明，其中ISoundSourceLocalizer接口可返回波束方向和音源位置。NuiImageBuffer.h，该文件定义了一个帧缓冲器，其作用类似DirectX9的纹理缓冲。</p>

<p><strong>Kinect for Windows应用程序架构</strong></p>

<p>[singlepic id=43 w=320 h=240 mode=watermark float=center]</p>

<p>上图给出了beta SDK的整体组件架构，硬件层我们已经在上月进行了详细介绍。驱动的内核模式包含了设备驱动程序，上层的数据交互统一使用WinUSB数据栈，其中设备栈主要用于设备的配置和访问，摄像头栈用于视频数据流控制，USBAudio栈用于音频数据流控制；用户模式为API提供了访问和控制接口。应用层API上包括了三部分组件，其中MS SDK beta直接提供了NUI API即基本API集和KinectAudio DMO，后者的功能主要是提供波束成形和音源定位功能。此外，如果要使用Kinect的所有功能，还需要自己准备Windows 7 SDK中的音频、语音、媒体API集以及微软语音识别SDK，上述组件并未包含在SDK中。近两篇文章会主要介绍NUI API部分的设计原理和编程说明。</p>

<p><strong> NUI API概述</strong></p>

<p>NUI API是Kinect SDK的核心组成部分，其主要功能包括：提供连接至PC的Kinect传感器元件的访问接口、提供对由Kinect成像传感器生成的图像和深度数据流访问接口、通过经处理的图像和深度数据实现骨骼跟踪。下面我们分别使用托管和非托管代码介绍整个Kinect API上下文环境的创建和销毁。</p>

<p>与OpenNI不同的是，Kinect SDK允许方便地同时使用多台Kinect设备，但是每台设备（或传感器）同一时间只能被一个程序实例使用。<strong>下面是使用C++代码实现传感器枚举和访问的整个过程：</strong></p>

<p>情形一：应用程序同一时间仅使用一台Kinect设备：</p>

<p>1、调用NuiInitialize，该函数首先会初始化一个Kinect传感器设备实例；</p>

<p>2、调用NuiXxx，该函数集的功能是传输图像和骨骼数据流，并管理并配置相关摄像头；</p>

<p>3、调用NuiShutdown结束。</p>

<p>情形二：应用程序同一时间使用多台Kinect设备：</p>

<p>1、调用MSR_NuiDeviceCount，查看可用Kinect设备的数量；</p>

<p>2、调用MSR_NuiCreateInstanceByIndex创建指定设备索引的实例，该函数将返回该实例的INuiInstance接口指针；</p>

<p>3、调用INuiInstance::NuiInitialize，该函数用于初始化针对该实例的NUI API上下文；</p>

<p>4、调用针对INuiInstance接口的其它方法，用于传输图像和骨骼数据流并管理相关摄像头；</p>

<p>5、调用INuiInstance::NuiShutdown，销毁某个设备实例的NUI API上下文；</p>

<p>6、调用MSR_NuiDestroyInstance函数销毁该实例；</p>

<p><strong>接下来是使用C#代码实现传感器枚举和访问的整个过程：</strong></p>

<p><strong></strong>情形一：应用程序同一时间仅使用一台Kinect设备：</p>

<p>1、创建一个新的运行时对象，将其参数列表留空，例如nui = new Runtime()，调用该构造函数表示在系统中创建了一个Kinect传感器设备实例；</p>

<p>2、调用Runtime.Initialize函数初始化NUI API上下文；</p>

<p>3、调用其它托管接口，用于传输图像和骨骼数据流，并管理和配置相关摄像头；</p>

<p>4、调用Runtime.Shutdown销毁实例。</p>

<p>情形二：应用程序同一时间使用多台Kinect设备：</p>

<p>1、调用MSR_NuiDeviceCount函数获取可用设备数量；</p>

<p>2、创建一个新的运行时对象，参数为设备索引号，如nui = new Runtime(index)，调用该构造函数表示创建了针对某个Kinect传感器的系统实例；</p>

<p>3、调用Runtime.Initialize函数初始化NUI API上下文；</p>

<p>4、调用其它托管接口方法，用于传输图像和深度数据、并管理和配置相关摄像头；</p>

<p>5、调用Runtime.Shutdown函数销毁实例。</p>

<p>NUI API的数据处理是通过一个多级管线实现的，在初始化阶段，应用程序需要指定相应级别的子系统，这样运行时系统才能系统所需的部分管线。在初始化时系统通常允许进行以下配置：</p>

<p>彩色：应用程序指定从设备中获取彩色图像数据流；</p>

<p>深度：应用程序指定从设备中获取深度图像数据流；</p>

<p>深度和人物索引：应用程序指定从设备中获取深度数据流，并请求骨骼跟踪引擎当前生成的人物索引；</p>

<p>骨骼：获取骨骼位置数据；</p>

<p>上述设置指定了有效数据流类型和解析度，例如当一个应用程序没有在NUI API初始化阶段指定深度数据时，它随后也无法开启一个深度数据流。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-09-17T23:41:18+08:00" pubdate data-updated="true">2011-09-17</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/09/17/KINECT-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/09/03/Kinect-lai-le-jie-xi-jie-xi-SDK/">
		
			Kinect来了——解析SDK(OpenNI Framework 4)</a>
	</h2>
	<div class="entry-content">
		<p><strong>配置工作节点的一般方法</strong></p>

<p>前文的例子中已经说明了在程序中动态配置节点信息的一般过程，节点配置完成的主要标志一般是对xn::Generator::StartGenerating()函数的调用。除此之外，OpenNI还提供了基于外置XML文件的节点配置方法，使用XML配置文件能够显著降低代码复杂度，同时提高应用程序的可用性和灵活性，本文将主要讨论上述方式结合OpenNI编程。</p>

<p>OpenNI支持的XML脚本功能非常强大，仅使用外置的文件就能够实现节点的创建、配置，甚至操作上下文属性（例如增加相应的许可证等）。通常可以使用xn::Context::RunXmlScript()直接执行脚本字符串，或通过xn::Context::RunXmlScriptFromFile()调用外部文件。</p>

<p>一个XML内容中必须包含一个OpenNI结点，该结点内共包含三个子结点：Licenses，Log和Production Nodes。其中Licenses结点提供了系统需要的额外的许可证，其格式一般如下：</p>

<p>[c]&lt;Licenses&gt;
&lt;License vendor=&quot;vendor1&quot; key=&quot;key1&quot;/&gt;
&lt;License vendor=&quot;vendor2&quot; key=&quot;key2&quot;/&gt;
&lt;/Licenses&gt;[/c]</p>

<p>Log结点用于配置OpenNI的日志系统，该结点内可添加下列可选元素属性：writeToConsole，设置日志信息是否在控制台中显示，默认为false。writeToFile:，设置日志信息是否将被写入文件，该文件将被放置在工作目录下，默认为false。writeLineInfo，设置是否每一条日志记录都需要包含文件名和行信息，默认为true。此外，Log还包含三个子结点：LogLevel，携带value属性，其值为0 (verbose), 1 (info), 2 (warnings) or 3 (errors)，其中3为默认值，该结点决定了日志记录的粒度。Masks，包含一个mask元素列表，设置相应mask的开启和关闭。Dumps，包含一个dump元素列表，设置相应dump的开启和关闭。下面的XML脚本演示了一段日志配置信息：</p>

<p>[c]&lt;Log writeToConsole=&quot;false&quot; writeToFile=&quot;false&quot; writeLineInfo=&quot;true&quot;&gt;
&lt;LogLevel value=&quot;3&quot;/&gt;
&lt;Masks&gt; &lt;Mask name=&quot;ALL&quot; on=&quot;true&quot; /&gt; &lt;/Masks&gt;
&lt;Dumps&gt; &lt;Dump name=&quot;SensorTimestamps&quot; on=&quot;false&quot; /&gt; &lt;/Dumps&gt;
&lt;/Log&gt;[/c]</p>

<p>ProductionNodes结点包含了工作结点的创建和配置信息，它包括了若干个子结点：
GlobalMirror，设置Global Mirror属性的开启和关闭，相当于调用xn::Context::SetGlobalMirror()函数；
Recording，设置是否启动一个记录，其file属性包括了记录文件名；
Node，该子结点可以包含多个，它将引导OpenNI枚举并创建一个工作结点，其作用类似xn::Context::CreateAnyProductionTree()函数。其type属性表示枚举类型，值可包括下列内容：Device (XN_NODE_TYPE_DEVICE)</p>

<p>Depth (XN_NODE_TYPE_DEPTH)</p>

<p>Image (XN_NODE_TYPE_IMAGE)</p>

<p>IR (XN_NODE_TYPE_IR)</p>

<p>Audio (XN_NODE_TYPE_AUDIO)</p>

<p>Gesture (XN_NODE_TYPE_GESTURE)</p>

<p>User (XN_NODE_TYPE_USER)</p>

<p>Scene (XN_NODE_TYPE_SCENE)</p>

<p>Hands (XN_NODE_TYPE_HANDS)</p>

<p>Recorder (XN_NODE_TYPE_RECORDER)</p>

<p>Node结点的name属性允许设置一个工作节点名称。</p>

<p>Query，Node还含有一个Query子结点，用于枚举时的查询操作。Query可包含下列子结点：&#8221;Vendor&#8221;、&#8221;Name&#8221;、&#8221;MinVersion&#8221;、&#8221;MaxVersion&#8221;、&#8221;Capabilities&#8221;、&#8221;MapOutputModes&#8221;、&#8221;MinUserPositions&#8221;、&#8221;NeededNodes&#8221;，上述结点均可设置不同的子结点和属性，用于查询条件的选择，如果有多个子结点同时出现，那么OpenNI将按照AND进行逻辑整合。下列XML脚本演示了创建一个自定义属性的深度生成器：</p>

<p>[c]&lt;Node type=&quot;Depth&quot; name=&quot;MyDepth&quot;&gt;
&lt;Query&gt;
&lt;Vendor&gt;vendor1&lt;/Vendor&gt;
&lt;Name&gt;name1&lt;/Name&gt;
&lt;MinVersion&gt;1.0.0.0&lt;/MinVersion&gt;
&lt;MaxVersion&gt;3.1.0.5&lt;/MaxVersion&gt;
&lt;Capabilities&gt; &lt;Capability&gt;UserPosition&lt;/Capability&gt; &lt;Capability&gt;Mirror&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;/MapOutputModes&gt; &lt;MinUserPositions&gt;2&lt;/MinUserPositions&gt;
&lt;NeededNodes&gt; &lt;Node&gt;MyDevice&lt;/Node&gt;&lt;/NeededNodes&gt;
&lt;/Query&gt;
&lt;/Node&gt;[/c]</p>

<p>Configuration子结点实际上代表了对工作节点的动态配置，其指令内容将是顺序执行的。同时，该结点几乎对应了OpenNI所有的Set配置操作，下面的例子演示了分别创建图像、深度和音频节点的过程：</p>

<p>[c]&lt;ProductionNodes&gt; &lt;Node type=&quot;Image&quot;&gt;
&lt;Query&gt;
&lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;320&quot; yRes=&quot;240&quot; FPS=&quot;60&quot;/&gt; &lt;/MapOutputModes&gt;
&lt;Capabilities&gt; &lt;Capability&gt;Cropping&lt;/Capability&gt; &lt;Capability&gt;Mirror&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;/Query&gt;
&lt;Configuration&gt; &lt;MapOutputMode xRes=&quot;320&quot; yRes=&quot;240&quot; FPS=&quot;60&quot;/&gt; &lt;PixelFormat&gt;RGB24&lt;/PixelFormat&gt; &lt;Cropping enabled=&quot;true&quot; xOffset=&quot;28&quot; yOffset=&quot;28&quot; xSize=&quot;200&quot; ySize=&quot;160&quot; /&gt; &lt;Mirror on=&quot;true&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt; &lt;Node type=&quot;Depth&quot;&gt;
&lt;Query&gt; &lt;Vendor&gt;VendorX&lt;/Vendor&gt; &lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;/MapOutputModes&gt;&lt;Capabilities&gt; &lt;Capability&gt;UserPosition&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;/Query&gt;
&lt;Configuration&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;UserPosition index=&quot;0&quot;&gt; &lt;Min x=&quot;128&quot; y=&quot;128&quot; z=&quot;500&quot;/&gt; &lt;Max x=&quot;600&quot; y=&quot;400&quot; z=&quot;2000&quot;/&gt; &lt;/UserPosition&gt; &lt;Property type=&quot;int&quot; name=&quot;VendorXDummyProp&quot; value=&quot;3&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt;
&lt;Node type=&quot;Audio&quot;&gt;
&lt;Configuration&gt; &lt;WaveOutputMode sampleRate=&quot;44100&quot; bitsPerSample=&quot;16&quot; channels=&quot;2&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt;
&lt;/ProductionNodes&gt;[/c]</p>

<p>通常在上述配置信息载入后，应手动调用xn::Context::StartGeneratingAll()函数启动数据流。然而也可以在XML中加入默认的Start信息，即在ProductionNodes和其Node子结点中均包含有startGenerating属性，其中如果前者的该属性为true，那么将意味着执行StartGeneratingAll()，否则只执行相应为true的工作节点。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-09-03T21:48:15+08:00" pubdate data-updated="true">2011-09-03</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/09/03/Kinect-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/24/Kinect-lai-le-jie-xi-jie-xi-SDK/">
		
			Kinect来了——解析SDK(OpenNI Framework 3)</a>
	</h2>
	<div class="entry-content">
		<p><strong>基本编程方法</strong></p>

<p>本节的主要内容是结合OpenNI编程。</p>

<p>首先假设使用MS VS作为开发工具，那么应先在工程中配置OpenNI库：其中分别需要添加环境变量$(OPEN_NI_INCLUDE)和$(OPEN_NI_LIB)，以及静态链接库OpenNI.lib；此外代码文件中还要添加相应的头文件，C语言工程需包含XnOpenNI.h，C++工程为XnCppWrapper.h。</p>

<p>下面的代码段演示了如何初始化一个上下文对象，并从深度节点中创建和读取数据：</p>

<p>[c]XnStatus nRetVal = XN_STATUS_OK;
xn::Context context;       //声明上下文对象
nRetVal = context.Init();        //初始化并返回异常编码
xn::DepthGenerator depth;        //声明深度生成器节点</p>

<p>nRetVal = depth.Create(context);        //初始化节点，返回异常编码
nRetVal = context.StartGeneratingAll();        //启动深度节点数据生成
while (bShouldRun)
{
nRetVal = context.WaitOneUpdateAll(depth);        //等待数据更新
if (nRetVal != XN_STATUS_OK)        //数据更新异常
{
printf(&quot;Failed updating data: %s\n&quot;,xnGetStatusString(nRetVal));
continue;        //不间断通信
}
//深度数据获取正常
const XnDepthPixel* pDepthMap = depth.GetDepthMap();        //获取数据缓冲区指针
}
context.Shutdown();        //关闭上下文对象[/c]</p>

<p>通常配置一个工作链需要实现大量代码，OpenNI提供了工作链查询接口，使得程序员可以通过查询关键字的形式找到系统内已有的相关工作链配置信息，考虑到符合查询条件的结果数量可能不唯一，OpenNI的查询机制结果总是以集合的形式返回。下面给出相关代码：</p>

<p>[c]//创建查询对象
xn::Query query;
nRetVal = query.SetVendor(&quot;MyVendor&quot;);        //设置查询条件，这里设置了有关提供商名称
query.AddSupportedCapability(XN_CAPABILITY_SKELETON);        //这里的查询条件指定数字骨架数据支持
xn::NodeInfoList possibleChains;
nRetVal = context.EnumerateProductionTrees(XN_NODE_TYPE_USER, &amp;query, possibleChains, NULL);        //根据查询对象枚举全部的工作链配置
xn::NodeInfo selected = *possibleChains.Begin();        //返回最相似的查询结果，此时仍是但节点信息
nRetVal = context.CreateProductionTree(selected);        //根据节点信息创建相应工作节点
xn::UserGenerator userGen;
nRetVal = selected.GetInstance(userGen);        //获取节点实例
&hellip;&hellip;        //进一步操作[/c]</p>

<p>实际上上述代码在OpenNI编程中非常常见，因为通过编程配置有关工作链实际上是一件很烦琐的事情。但显然对相关节点的查询并不一定能得到理想的结果，有时确实会发生查询结果为零的情况。例如该功能类型的节点尚未配置入系统中，或者节点的许可证序号不一致等异常情况。为方便查看节点测试失败的原因，OpenNI提供了一种针对节点类型的全局测试机制，该机制允许记录节点初始化失败的原因。相关代码如下：</p>

<p>[c]xn::EnumerationErrors errors;        //实际上是一个异常结果集合
xn::HandsGenerator handsGen;
nRetVal = context.CreateAnyProductionTree(XN_NODE_TYPE_HANDS, NULL, handsGen, &amp;errors);
//如果没有一个节点测试成功
if (nRetVal == XN_STATUS_NO_NODE_PRESENT) {
//遍历所有的异常信息，并输出
for (xn::EnumerationErrors::Iterator it = errors.Begin(); it != errors.End(); ++it) {
XnChar strDesc[512];
xnProductionNodeDescriptionToString(&amp;it.Description(), strDesc, 512);
printf(&quot;%s failed to enumerate: %s\n&quot;, xnGetStatusString(it.Error()));
}
return (nRetVal);
}
//如果存在相关节点，但无法验证
else if (nRetVal != XN_STATUS_OK) {
printf(&quot;Create failed: %s\n&quot;, xnGetStatusString(nRetVal)); return (nRetVal);
}[/c]</p>

<p>下面这段代码演示了从创建深度生成器，到节点设置配置信息，再从中获取数据并最终显示的全过程。</p>

<p>[c]XnStatus nRetVal = XN_STATUS_OK;
Context context;
nRetVal = context.Init();        //创建上下文
DepthGenerator depth;
nRetVal = depth.Create(context); //创建深度生成器
XnMapOutputMode mapMode;
mapMode.nXRes = XN_VGA_X_RES;
mapMode.nYRes = XN_VGA_Y_RES;
mapMode.nFPS = 30;
nRetVal = depth.SetMapOutputMode(mapMode);        //设置其输出模式为VGA+30FPS，
nRetVal = context.StartGeneratingAll();        //开始获取数据
XnUInt32 nMiddleIndex = XN_VGA_X_RES * XN_VGA_Y_RES/2 + XN_VGA_X_RES/2;        //计算中心像素索引
while (TRUE) {
nRetVal = context.WaitOneUpdateAll(depth);       //刷新数据缓冲区
const XnDepthPixel* pDepthMap = depth.GetDepthMap();
printf(&quot;Middle pixel is %u millimeters away\n&quot;, pDepthMap[nMiddleIndex]);        //输出中心像素的深度值
}
context.Shutdown();[/c]</p>

<p>音频生成器（AG）的操作与其它节点相比有一定区别，原因在于当程序执行UpdateData()方法后AG才会将之前记录的音频数据存入数据缓冲区中，而且许多情况下音频生成器的数据缓冲区大小是未知的，因此需要经常使用xn::AudioGenerator::GetDataSize()方法获取当前环境下的数据缓冲区大小。下面的例子中首先创建了一个AG，然后对其进行配置，最后读取音频数据。</p>

<p>[c]Context context;
nRetVal = context.Init();        //初始化上下文
AudioGenerator audio;
nRetVal = audio.Create(context);         //初始化AG
XnWaveOutputMode waveMode;
waveMode.nSampleRate = 44100;
waveMode.nChannels = 2;
waveMode.nBitsPerSample = 16;
nRetVal = audio.SetWaveOutputMode(waveMode);        //设置AG配置属性
while (TRUE) {
nRetVal = context.WaitOneUpdateAll(audio);        //刷新数据缓冲区
const XnUChar* pAudioBuf = audio.GetAudioBuffer();
XnUInt32 nBufSize = audio.GetDataSize();        //获取当前数据大小
}
context.Shutdown();[/c]</p>

<p>最后，我们来看看OpenNI提供的录制与缩放功能（NSSDK未提供该功能）。执行录制的前提是首先创建一个Recorder节点对象,并设置好它的待保存文件名。然后应用程序在该节点中添加需要进行录制目标节点，当在Recorder中加入一个节点后，Recorder将首先保存其配置信息，然后对其进行记录。同时Recorder会持续监听目标节点的所有触发事件，以便当节点配置发生改变时能及时获得响应。一旦所有节点添加至Recorder中，应用程序就可以使用Recorder记录节点的数据流了。需要注意的是，针对数据的记录可以使用xn::Recorder::Record()方法，或者通过UpdateAll函数进行。</p>

<p>此外，使用外部XML文件初始化的OpenNI程序可方便地记录其session信息，从而避免修改相应代码。具体只需要在XML文件中为Recorder创建一个新的节点，并将所有节点添加至其中，每当应用程序调用一类UpdataAll函数时均会自动触发记录操作。下面的代码演示了记录一个深度生成器的操作：</p>

<p>[c]DepthGenerator depth;
nRetVal = depth.Create(context);          // 创建深度生成器
RetVal = context.StartGeneratingAll();
Recorder recorder;
nRetVal = recorder.Create(context);        //创建一个Recorder
nRetVal = recorder.SetDestination(XN_RECORD_MEDIUM_FILE, &quot;c:\temp\tempRec.oni&quot;);        //配置Recorder属性
nRetVal = recorder.AddNodeToRecording(depth, XN_CODEC_16Z_EMB_TABLES);        //将深度节点加入Recorder
while (TRUE) {
nRetVal = context.WaitOneUpdateAll(depth);        //注意UpdataAll函数既刷新了数据缓冲区，同时对其中的帧数据进行了记录
}[/c]</p>

<p>录制数据的重放需要首先调用xn::Context::OpenFileRecording()函数，OpenNI读入该文件后先针对文件中的每一个节点创建一个对应的模拟节点，然后使用记录的配置信息进行重配置。应用程序可以随时调用xn::Context::FindExistingNode()获取任何需要的节点，并能像正常节点一样使用。但是应注意，由播放器创建的节点处于锁定状态，且无法被更改，因此其配置信息只能维持记录中保存的状态。同时 ，使用XML文件的OpenNI程序可以轻松替换其载入录制内容，即直接从XML的Recorder节点中读取相应数据。下面的代码就演示了如何载入录制文件，并获取其中保存的深度节点信息。</p>

<p>[c]Context context;
nRetVal = context.Init();        //初始化上下文
nRetVal = context.OpenFileRecording(&quot;c:\temp\tempRec.oni&quot;);        //载入录制文件
DepthGenerator depth;
nRetVal = context.FindExistingNode(XN_NODE_TYPE_DEPTH, depth);        //获取已录制的深度生成器节点[/c]
 在关于OpenNI的最后一篇文章中，我们将重点关注有关节点配置的内容。</pre></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-24T23:24:07+08:00" pubdate data-updated="true">2011-08-24</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/24/Kinect-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/23/Kinect-lai-le-jie-xi-jie-xi-SDK/">
		
			Kinect来了——解析SDK(OpenNI Framework 2)</a>
	</h2>
	<div class="entry-content">
		<p>前文主要介绍了OpenNI FrameWork的基本概念和整体构架，在正式进入API概览之前，我们需要对工作节点（Production Nodes）进行一些补充说明。</p>

<p><strong> 工作节点的数据创建/销毁机制</strong></p>

<p>通常把能够输出数据的工作节点称为“生成器”，这里我们可以把“生成器”看作一个对象。而当“生成器”对象被首次创建时，这些对象往往不会立即执行有关数据处理的代码，而需要等待上层应用针对该节点输入相关配置信息之后才进行。实际上“生成器”必须等候相关应用的执行指令到达之后才进行创建数据操作。这一操作往往通过下列虚方法实现：</p>

<p>[c]xn::Generator::StartGenerating();[/c]</p>

<p>应注意 ，上面的StartGenerating函数的作用是开始执行数据生成操作，有时我们希望停止生成新数据，但最好能保留该工作节点的配置信息。那么就没有必要销毁该工作节点对象，只需调用下面的函数即可：</p>

<p>[c]xn::Generator::StopGenerating();[/c]</p>

<p>事实上每一个“生成器”节点都需要实现数据输入操作，但有时可能会遇到这种情况：程序运行可能需要上一次处理的数据结果，为了避免频繁I/O，每个“生成器”节点必须在内部保存一份数据结果副本，直到上层代码显示调用更新数据备份区的函数。此类函数通常是以下形式：</p>

<p>[c]xn::Generator::WaitAndUpdateData();[/c]</p>

<p>调用该函数就意味着通知工作节点需要重置自己的数据备份区，并等待下一次新的数据生成操作。在一条完整的工作链上，程序员需要一次性刷新所有工作节点的数据备份区，那么依照不同情况可以选择相应的简便处理方式：</p>

<p>[c]xn::Context::WaitAnyUpdateAll();        //当任意节点产生新数据时，所有节点都刷新自己的数据备份区；</p>

<p>xn::Context::WaitOneUpdateAll();        //只有当某一个指定节点产生新数据时，所有节点才刷新自己的数据备份区，这种情况通常适用于只有一个控制节点的应用程序；</p>

<p>xn::Context::WaitNoneUpdateAll();        //所有节点立即刷新数据备份区；</p>

<p>xn::Context::WaitAndUpdateAll();        //只有当全部节点都产生新数据时，所有节点才刷新自己的数据备份区；[/c]</p>

<p>上述函数均会在延迟两秒后自动退出。</p>

<p>官方建议程序员应尽量使用形如xn::Context::Wait…UpdateAll()的函数进行数据备份区刷新，除非确实需要只刷新某一个节点，上述方式也会在一定程度上简化程序员的设计和编码工作。</p>

<p><strong>主要数据对象</strong></p>

<p>OpenNI的最基本的数据对象是指上下文对象，上下文对象保存了应用程序使用OpenNI的所有状态信息，包括所有工作链。应注意同一个应用程序可同时创建多个上下文对象，但上下文之间无法共享信息。同时上下文对象必须进行初始化后才能首次使用，且需保证所有的附加模块均得到正确的载入和确认。应用程序可以通过调用shutdown函数释放上下文对象所使用的内存空间。</p>

<p>OpenNI中还定义了一种元数据对象，元数据主要用于保存与特殊数据类型有关的属性信息。例如在一套深度映射数据中，其解析度的大小就属于元数据范畴。任何从生成器节点产生的数据都拥有相应的元数据对象。设计元数据对象的目的在于，由于节点配置可由人为随机干预，任何生成器节点的配置信息和其所产生的数据信息始终无法保证完全一致，因此有必要对每套数据也封装进相应的属性信息，这就是元数据对象的作用。</p>

<p><strong> 配置变更</strong></p>

<p>每一个OpenNI中所包含有关配置的选项都携带以下方法：</p>

<p>1、Set函数，用于修改当前配置；</p>

<p>2、Get函数，用于返回当前配置内容；</p>

<p>3、Register和Unregister函数，用于为配置变更事件注册一个回调函数；</p>

<p><strong>数据生成器</strong></p>

<p>1、基本映射图像生成器，OpenNI中用于生成任意数据映射类型的最基本的映射生成器，它的功能主要包括配置控制、特性分配、可替换的视点特性以及帧同步特性；</p>

<p>2、深度映射生成器，主要功能包括获取深度映射、获取设备的最大深度范围、设备的视野属性配置、用户位置特性等；</p>

<p>3、图像生成器，主要功能包括获取彩色图像、获取像素格式属性；</p>

<p>4、红外数据生成器，用于获取当前的红外映射图像；</p>

<p>5、场景分析器，用于获取传感器原始数据，并对原始数据内容进行分类标记和输出，其主要功能包括获取分类映射图像（每一个像素都携带相应的标签）、获取水平面坐标；</p>

<p>6、音频生成器，主要功能包括获取音频缓冲区、配置音频设备属性，包括采样率、通道数以及采样带宽等；</p>

<p>7、姿态生成器，用于对特定的人物或手势进行跟踪，主要功能包括姿态添加/删除、获取活动姿态、注册/反注册姿态存在回调函数、注册/反注册姿态变更毁掉函数；</p>

<p>8、手部标记点生成器，用于跟踪手部标记点，主要功能包括开始/停止手部跟踪、注册/反注册手部跟踪回调函数（包括手部的进/出以及位置变化）；</p>

<p>9、用户生成器，用于生成场景中有关人物的数据，主要功能包括返回用户数据、获取用户信息、获取用户质心CoM、获取携带用户标记像素的场景图像、注册/反注册用户进/出回调函数。</p>

<p>最后，我们将介绍几个OpenNI的基本程序框架。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-23T00:18:19+08:00" pubdate data-updated="true">2011-08-23</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/23/Kinect-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/21/Kinect-lai-le-jie-xi-jie-xi-SDK/">
		
			Kinect来了——解析SDK(OpenNI Framework 1)</a>
	</h2>
	<div class="entry-content">
		<p>本文和后文主要介绍PrimeSense OpenNI Framework的基本架构和设计思路，以及应用开发方面的一些基本问题。应注意这两篇文章不能完全替代官方User Guide的作用，但能帮助读者快速理解并上手OpenNI。此外，由于截至目前微软并不承认除Microsoft Kinect SDK beta之外的其它中间件和应用层API产品，使用OpenNI研究Kinect技术只能局限于研究学习活动，而且此举潜在风险尚无法完全暴露出来。</p>

<p><strong> 1 概述</strong></p>

<p>OpenNI实际上首先指开放自然交互联盟，该组织由PrimeSense于2010年底发起，现有成员包括一家核心技术提供商、两家应用开发商和一家设备制造商。OpenNI目前致力于构建OpenNI自然交互应用程序框架，这个框架包括与底层视频和音频传感器进行通信的接口，以及建立其上的高级中间件构件，例如视觉计算和语音识别技术等。下图大致说明了OpenNI体系的层次抽象视图：</p>

<p>[singlepic id=42 w=320 h=240 mode=watermark float=center]</p>

<p>上图的底层设备主要是指视频/深度、音频传感器等，但不仅限于此；OpenNI API主要扮演了与底层设备和顶层应用程序通信的角色，这部分的设计将直接影响应用程序的构建和部署；中间件构件实际上囊括了自然交互所有的核心技术，它的作用就是针对OpenNI采集的原始数据，利用先进的视觉和语音识别算法对原始数据进行精确分析和计算，并生成能够反映真实世界中人物（或其它任何物体）的空间位置、姿态、语音特征，再通过模式分类算法对上述特征进行有效识别和归纳，最终返回有知识意义的N维离散信息。OpenNI定义的中间件构件应包括以下几个部分：</p>

<p>1、全身分析中间件：该组件从原始数据中提取并生成与人体相关的离散信息，这里典型的数据结构包括对关节点、朝向、质心等的描述；</p>

<p>2、手部分析中间件：该组件从原始数据中提取并生成与手部相关的位置信息，该信息通常是指一个点的三维坐标；</p>

<p>3、姿态检测中间件：该组件从原始数据中提取并分析与人体姿态相关的视频信息，并通过分类算法对其进行标识，并反馈识别结果（例如对一个挥手动作的识别，该组件应能及时反馈目标挥手的通知信息）；</p>

<p>4、场景分析中间件：该组件从原始数据中提取并分析整个场景信息，并从中分离出有意义的前景（如人物）和背景信息、场景的平面坐标表示、以及对场景中人物个体的识别。</p>

<p>OpenNI API将这些后处理信息提交至上层应用程序，相关应用程序根据已知的自然交互信息和自身的决策算法进行相应的计算和反馈。</p>

<p>由于PrimeSense的关系，OpenNI目前只支持PS提供的<a href="http://www.hanyi.name/blog/?p=335" target="_blank">硬件方案</a>，但这并不意味着今后的OpenNI只会向着一个方向发展（实际上OpenNI组织的建立恐怕就是考虑到这一点）。不过目前OpenNI只支持3D传感器、RGB摄像头、红外摄像头以及音频传感器设备（也就是说，如果你能够自己凑齐上述物件，那么结合OpenNI构建自然交互程序并不困难，Kinect只是一种便利廉价的硬件方案之一）。</p>

<p><strong> 2 工作节点（Production Nodes）</strong></p>

<p>OpenNI使用了工作节点的定义来描述自己的工作流，所谓的工作节点就是能够接收并产生数据的组件，每一个工作节点均可以调用底层节点提供的数据（甚至更改节点配置），并能将数据传递至上层节点。若干工作节点的有效串联就构成了一个工作链，工作链描述了OpenNI中各工作节点的拓扑关系，并提供了一种可行的工作流模型。</p>

<p>对应OpenNI的层次抽象设计，工作节点被分为两大类：一种是传感器节点，另一种是中间件节点。</p>

<p>1、传感器节点：</p>

<p>设备节点，该节点表示某一个硬件设备，如RPG摄像头，设备节点主要用于对相关硬件进行配置；</p>

<p>深度生成节点，能够产生场景的深度映射数据，该节点可由与OpenNI体系保持兼容的任意3D传感器实现；</p>

<p>图像生成节点，产生RGB彩色图像，该节点可由与OpenNI体系保持兼容的任意视频传感器实现；</p>

<p>红外数据生成节点，产生红外图像映射数据，该节点可由与OpenNI体系保持兼容的任意红外传感器实现；</p>

<p>音频生成节点，产生音频数据流，该节点可由与OpenNI体系保持兼容的任意音频传感器实现；</p>

<p>2、中间件节点</p>

<p>姿态警告节点，当某一特定姿态被识别时会产生特殊的回调信息；</p>

<p>场景分析节点，分析场景，分离前景和背景、识别场景中的人物、检测水平面。该节点会生成一个已识别的深度映射数据，其中的每个像素都被标记为不同状态以标识人物和背景部分；</p>

<p>手部生成节点，支持手部的检测和跟踪，当手部被检测到时会产生特殊的回调信息，当手部被持续跟踪时实时更改其位置信息；</p>

<p>用户生成节点，产生一个三维场景中的身体信息表示。</p>

<p>值得一提的是，OpenNI还定义了数据回放接口，其中包括数据记录、回放和记录的编解码操作接口。</p>

<p><strong> 3 节点特性</strong></p>

<p>OpenNI为每个工作节点配置了特性属性，特性类似于一种扩展功能。对每个工作节点来说，这些特性的配置非常灵活，不同的中间件提供商可根据需要对自己开发的工作节点进行相应的配置。同时，每个工作节点所对应的特性集合是可选的，用户可根据自身需要对其进行合理配置。应注意，目前OpenNI所提供的几种节点特性并不是固定的设计，在今后会添加更多的节点特性并对已有特性进行调整，现在看来这部分可能是未来设计时需要慎重考虑的地方之一。</p>

<p>关于节点的可用特性列表，读者可查阅OpenNI的官方网站和用户指南。</p>

<p><strong> 4 其它</strong></p>

<p>除上述基本属性组成外，OpenNI还设计了若干机制以扩展其上层应用的适应性。</p>

<p><strong>模拟节点</strong> 模拟节点实际上是指空节点，也就是不实现任何数据生成的逻辑代码，而类似一种抽象的数据接口层。这种设计往往被用于应用程序的构建当中。</p>

<p>应用程序间的设备共享和节点锁定 OpenNI的工作节点所产生的数据通常只来源于唯一的硬件设备，而该设备可能同时被若干个应用程序所调用。这也就意味着OpenNI的工作节点需要具备一定的同步机制以保证上述任务的并发执行。</p>

<p>就OpenNI这种应用程序框架而言，实现操作系统的进程级并发是很困难的。因此这里OpenNI默认所有的设备节点处于全局共享模式，即任何应用程序均可更改节点配置，同时节点配置一旦发生变更则立即产生回调信息，并通知所有注册至该节点的应用程序。另外，所有工作节点还可处于一种锁定模式，在该模式下OpenNI将屏蔽对应节点&#8221;Set&#8221;方法的调用，如果该节点恰是设备节点，那么就意味着该设备节点的“Lock Aware”特性被开启。</p>

<p><strong>许可机制</strong> OpenNI提供了一种模块与应用间简单的许可机制，任何一个OpenNI上下文对象均保有一份当前已载入的许可证列表，在任何层次均可检查该列表，并搜索特定的许可证。许可证一般由发行商名称和许可证编号组成，发行商可以利用该许可机制控制其知识产权的使用权。</p>

<p><strong> 通用框架工具集</strong> OpenNI集成该工具集的目的是降低跨平台构建应用程序的复杂度，而事实上标准的OpenNI并没有包含这一部分，因此该部分仅仅处于概念阶段。</p>

<p><strong>记录</strong> 记录经常被用于调试操作。OpenNI支持记录整个工作链，包括每个工作节点的配置信息和节点间的数据流。OpenNI包含一套完整的记录和回放机制，标准API中是由nimRecorder模块实现的，该模块还定义了一个新的文件格式“.ONI”，用以保存上述信息。</p>

<p><strong>工作节点的异常状态反馈</strong> 每一个工作节点均包含相应的异常或错误状态定义。默认的异常状态为“OK”，只有当出现真正的异常或错误时，该状态才会变为Error Status中定义的任何一种信息，同时没有实现异常状态反馈机制节点的异常状态将始终为“OK”。应用程序可以通过为每一个节点注册回调函数以接收其异常状态反馈。此外还可以通过定义全局异常状态来接收任何节点的异常信息，后者的方法通常是最容易实现的。</p>

<p><strong>向后兼容性</strong> OpenNI需要保证完全的向后兼容，这就意味着所有当前构建的应用程序均能完美运行在后续版本的OpenNI框架中，而无需重新编码或编译。</p>

<p>下面我们将重点关注OpenNI的具体API设计和程序基本框架。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-21T20:46:27+08:00" pubdate data-updated="true">2011-08-21</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/21/Kinect-lai-le-jie-xi-jie-xi-SDK//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/20/wan-xian-jian-de-dou-shi-ke-lian-de-ju-qing-kong/">
		
			玩仙剑的都是可怜的剧情控!</a>
	</h2>
	<div class="entry-content">
		<p>的确，本文将肯定是一篇牢骚之作。[nggallery id=7]</p>

<p>也许有人以为，与十六年前相比，今天的RPG市场存在玩家流失、开发周期漫长、盗版泛滥，甚至幸存下来的RPGers眼光越来越高、以至开发团队吃力不讨好等众多问题。然而我要说的是，这回的五代之坑爹完全不出我对北软的预料，在仙剑败笔史上恐怕是坐稳了第二把交椅（这里就暂不提带头大哥了）。</p>

<p>去年一共有两款国产中文RPG面市，包括年初的《云之遥》和夏天的《古剑奇谭》。在一口气玩过《古剑》二周目后，我觉得中文RPG越来越讲究游戏品质，尽管一如既往地缺乏游戏性，但在脚本剧情上仙剑系列自坑王二代以后就基本没再弱过：这种局面在2007年的四代和去年的《古剑》（尽管寄予厚望，但我仍希望把它当做是仙剑系列的衍生作品，然而现在看来，若干年之后这种看法可能会受到严格的质疑）上甚至达到了一个顶峰，玩家反馈甚至可以称得上是好评如潮。</p>

<p>然而这次果断被五代坑了。由于没赶上《古剑》的预售，这次我特意提前一个月在淘宝上预定了仙五，好在卖家RP不差，7月6号顺丰到货（正式的全国首发是7月7号）。遗憾的是此后在学校忙了一个多月，8月17号放假，昨晚历经30个小时通关。通关后的唯一感想就是：仙五外传到底还有没有必要再出了？</p>

<p>从剧情上看，尽管五代同样是一个全新的故事，但为迎合系列的世界观，游戏固然脱离不了一部分原始设定。问题在于无论从对原设的衔接还是发展两方面考虑，本作企划都显得十分生硬。例如蜀山七圣，到最后给人感觉不过是个噱头，除了后期的闯七宫情节外基本上是彻底的酱油群体。诚然北斗七星传统上是被道教奉为星神，但设定上有点抄袭小说里的“全真七子”，七星伏魔阵也有参照天罡北斗阵的嫌疑，蜀山倒不如改称全真派算了。还有关于魔界的设定方面，这次发展出了魔界“八族”说，和三代以后的魔尊说完全不符，也不排除是为了外传做铺垫的可能。</p>

<p>另一方面，本作主题为“心愿”，但故事的发展过程并不很贴近主题，许多主线情节基本上可归为支线，因此给人有些故意“充数”的感觉，致使玩家对后续剧情发展逐渐丧失兴趣。</p>

<p>游戏系统上，五代与前作相比几乎没有任何改进，贴符/炼符系统只不过是新瓶旧酒流，战斗系统中新增的合击系统也毫无新意。可见自上软团队于2007年末解散后，仙剑系列经历了两年的沉寂，又经两年研发，最终出来的还是一个成本受到严格控制的“续作”，关于这点也适用于这次的音乐部分，续创太多，原创太少，精品更少。</p>

<p>引擎利用上，还是因为成本问题，仙五的RenderWare引擎必然还会用到外传中。我想说的是，这么一款看重剧情和画面表现力的RPG游戏，坚持使用落后引擎的结果只能是让人不断失望——更何况系列到现在已经如此混乱的世界观了（轩辕剑系列就还好，只是六代已是遥遥无期了）。</p>

<p>值得一提的是，19号晚百游官网上正式发布了仙五的第一部免费DLC和语音包，至此我才得以体验到视角解锁后的游戏场景——事实上五代的运镜设置非常不利于鼠标操作，使得非但没有进一步提高游戏的表现力，反而给玩家造成一些困扰。</p>

<p>总而言之，仙五这次能号称八十万套销量，基本可以说是近年来几部类似作品共同发力的结果。而制作团队无谓的坚持，只能让人担心接下来的续作可能注定将成为仙剑十六年的狗尾续貂了。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-20T13:38:05+08:00" pubdate data-updated="true">2011-08-20</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/pcyou-xi/'>pc游戏</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/20/wan-xian-jian-de-dou-shi-ke-lian-de-ju-qing-kong//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/16/Kinect-lai-le-ying-jian-ying/">
		
			Kinect来了——硬件篇</a>
	</h2>
	<div class="entry-content">
		<p>作为一款集成了诸多先进视觉技术的自然交互设备，Kinect在学术和工业界均享有较高的关注度。本文将在参考PrimeSense传感器技术若干专利文献的基础上，试图探寻Kinect硬件技术的物理原理及其发展前景。为证明本文所描述的内容与Kinect的相关性，还要感谢ifixit写于去年底的一篇<a href="http://www.ifixit.com/Teardown/Microsoft-Kinect-Teardown/4066/1" target="_blank">Microsoft Kinect Teardown</a>拆解教程，这位前人使得Kinect所有的内部元件得以被我们尽收眼底，从而为我们的“推测”提供了事实依据。当然主要还是因为我是绝对不会把啃奶拆着玩的:)</p>

<p><strong> Kinect硬件系统概览</strong></p>

<p>Kinect的硬件系统其实并不复杂（所以理应便宜到爆？），如下图所示：</p>

<p>[singlepic id=33 w=320 h=240 mode=watermark float=center]</p>

<p>Kinect使用NEC uPD720114的USB 2.0集线器控制器作为数据集成接口，主要控制芯片包括Allegro Microsystems A3906（低电压步进器和单/双路直流电机驱动器）、Marvell AP102（带摄像机接口控制器的SoC）/PrimeSense PS1080-A2（成像处理器SoC）、TI TAS1020B（USB音频控制器）和其它辅助计算/存储设备。本文的剩余内容将依次分析Kinect的三大硬件原理：姿态调整、音频输入、视频输入。</p>

<p><strong>1 转动电机系统</strong></p>

<p>尽管Kinect提供了可跟踪目标物体的物理姿态调整机制，然而该部分相对比较简单，因为这些电机和塑料齿轮看起来有够简陋&hellip;实际上在官方出品的Programming Guide中描述了tilt机制的基本规格：±28°V，而Kinect成像系统自身的视角大小为43° V/57°H。同时手册还建议避免频繁调用tilt功能，其最低标准是每秒不超过1次（或每20秒不超过15次）调用。目前看来tilt功能脆弱且基本发挥不了作用，当然今后对应的商业版本可能会是个例外。</p>

<p><strong>2 音频采集系统</strong></p>

<p>Kinect的音频系统采用了四元线性麦克风阵列技术。一般而言，麦克风阵列中包含四个相互独立的小型麦克风，每个设备之间相距数厘米，其排列可呈线形或“L”形。与一般的单麦克风数据相比较，阵列技术包含有效的噪音消除和回波抑制（acoustic echo cancellation，AEC）算法，同时采用波束成形（Beamforming）技术通过每个独立设备的响应时间确定音源位置，并尽可能避免环境噪音的影响。上述Beamforming算法的细节来源于微软研究院，有兴趣的读者可以参考《A NEW BEAMFORMER DESIGN ALGORITHM FOR MICROPHONE ARRAYS》这篇文章，原文发表于IEEE-Proceedings of ICASSP 2005 USA。</p>

<p>从元件上看，除了Kinect所有的四元麦克风阵列以外，还配置了Wolfson Microelectronics WM8737G（配置了前置放大器的24bits立体声ADC）用于进行本地的音频信号处理。关于Kinect Audio系统的软件部分我们将在后续的API专题中进行详细介绍。</p>

<p><strong>3 视频成像系统</strong></p>

<p>Kinect的成像系统来源于PrimeSense的专利技术，尽管微软官方一直遮遮掩掩，但很容易通过分析PS的设计来了解Kinect。下面首先给出Kinect视频传感器的规格：</p>

<p>帧率：30FPS，深度/RGB数据；</p>

<p>帧解析度：深度数据QVGA320x240，RGB数据VGA640x480；</p>

<p>作用范围：1.2-3.5米，深度/RGB数据。</p>

<p>值得一提的是，PrimeSense官网上给出的<a href="http://www.primesense.com/?p=514" target="_blank">Reference Design</a>的规格要高出许多，可以看出Kinect主要追求的是经济效益。我们注意到Kinect的电源和USB是同一个接口，而仅连接PC后Kinect的LED会点亮，但还不能执行主要功能。只有再接入电源后才能正式启动（Kinect的功率达到了12W，而普通USB一般是2.5W）。</p>

<p>[singlepic id=34 w=320 h=240 mode=watermark float=center]</p>

<p>上图从左向右，分别是OG12/0956/D306/JG05A红外发射器、VNA38209015彩色CMOS以及Microsoft/X853750001/VCA379C7130红外CMOS。中间的摄像头提供了彩色VGA图像，剩余的两个元件主要用于提供QVGA深度数据。</p>

<p>那么关键问题就是，PrimeSense是如何获取这些深度数据呢？</p>

<p>截至目前，最精确可行的光学测距方法可能就是ToF（time of flight），例如LDM激光测距、IDM红外测距等等具体技术已经实现了产品化；另一方面，如今许多三维扫描仪都采用了三角测距法，特别是对手持式扫描设备而言。然而上述这些技术都不太适用于Kinect这种家用设备：首先是测量环境的限制，其次还要考虑成本因素。</p>

<p>PrimeSense的测距技术类似一部分结构光技术，“结构光”指一些具有特定模式的光，其pattern的图案可以是线、点、面等多种图形。结构光扫描法的原理是首先将结构光投射至物体表面，再使用摄像机接收该物体表面反射的结构光图案，由于接收图案必会因物体的立体形状而发生变形，那么就可以试图通过该图案在摄像机上的位置和形变程度来计算物体表面的空间信息。普通的结构光方法仍然是部分采用了三角测距原理进行深度计算。</p>

<p>参考Google Patents上的Range mapping using speckle decorrelation（No. US7433024B2）以及DEPTH MAPPING USING PROJECTED PATTERNS（No. 0118123 A1）两篇技术文档，已经有前人对PrimeSense的方法进行了详细解释。</p>

<p>PrimeSense将其深度测量技术命名为Light coding，与结构光法不同的是，Light coding的光源被称为“激光散斑（laser speckle）”，是当激光照射到粗糙物体或穿透毛玻璃后形成的随机衍射斑点。这些散斑具有高度的随机性，而且会随着距离的不同变换图案。也就是说空间中任意两处的散斑图案都是不同的。只要在空间中打上这样的结构光，整个空间就都被做了标记，把一个物体放进这个空间，只要看看物体上面的散斑图案，就可以知道这个物体在什么位置了。</p>

<p>当然，在这之前要把整个空间的散斑图案都记录下来，所以要先做一次光源的标定。在PrimeSense的专利上，标定的方法是这样的：每隔一段距离，取一个参考平面，把参考平面上的散斑图案记录下来。假设Natal规定的用户活动空间是距离电视机1米到4米的范围，每隔10cm取一个参考平面，那么标定下来我们就已经保存了30幅散斑图像。需要进行测量的时候，拍摄一副待测场景的散斑图像，将这幅图像和我们保存下来的30幅参考图像依次做互相关运算，这样我们会得到30幅相关度图像，而空间中有物体存在的位置，在相关度图像上就会显示出峰值。把这些峰值一层层叠在一起，再经过一些插值，就会得到整个场景的三维形状了。</p>

<p>值得注意的是，尽管没有对PrimeSense下手，微软在前不久先后收购了3DV和Canesta，两家均握有大量ToF视频跟踪技术专利的科技公司。可以想见，Kinect作为消费市场的零号机，可能很快就会有更加强大的商用同伴诞生。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-16T12:51:46+08:00" pubdate data-updated="true">2011-08-16</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/16/Kinect-lai-le-ying-jian-ying//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2011/08/11/ms-MS-Kinect-kai-fa-she-qu-de-zui-xin-jin-zhan/">
		
			MS Kinect开发社区的最新进展</a>
	</h2>
	<div class="entry-content">
		<p>三个月前，我们展示了一套配合使用OpenNI+SensorKinect+NITE在Windows平台上驱动Kinect的Demo程序，事实证明，上述框架已完全可满足基于最新版本的Kinect的PC项目开发了。不过，微软在连续跳票一个多月后终于坐不住了，于北京时间6月17日凌晨开放The Kinect for Windows SDK beta下载页面，并作为正规军首次杀入这个吸引了众多发烧友和研究人员的技术萌芽领域。</p>

<p>由于最近一段时间非常忙，我们几乎没有空闲时间用于对快速发展中的Kinect技术进行潜心研究。幸运的是，我得以在暑假一天半的假期里花几个小时研究来自微软的SDK。很显然，因为她的姗姗来迟，越来越多的国内开发者尝试进入这个领域，而又由于Kinect技术在应用层面实际上要求比较高的知识和技术门槛，这也成了Kinect社区拥有一批具有较高技术素养的核心成员的重要原因。</p>

<p>首先应当说明的是，我拿到的SDK其实是微软在7月29日发布的1.00.12版，msdn上给出的Refresh Details解释新版本在驱动和运行库部分相较旧版而言得到了一些增强，目前也没有发现存在任何兼容问题。须知MS的Kinect SDK beta只支持Windows 7操作系统，开发环境要求VS2010/.Netframework 4.0。关于硬件部分的要求在试验阶段其实几乎可以忽略，除非你买到了非盒装的Kinect（拆机版也可额外购买电源/USB数据接口）。</p>

<p>对于初学者而言，还有一些组件需要额外安装，但这些都不是必须的。例如几个经典的Demo程序，例如骨骼跟踪，就需要Microsoft DirectX® SDK - June 2010 or later version和Runtime for Microsoft DirectX® 9才可以编译运行；另外对于语音命令识别的Sample程序，还需要安装Microsoft Speech Platform Runtime, version 10.2 (x86 edition)、Microsoft Speech Platform - Software Development Kit,version 10.2 (x86 edition)以及Kinect for Windows Runtime Language Pack, version 0.9(acoustic model from Microsoft Speech Platform for the Kinect for Windows SDK Beta。上述开发库提供了内容绘制、图像和图形处理、语音识别等非Kinect原生功能，仅仅这些，Kinect综合技术的一角其实已经显现出来了。</p>

<p>那么下面就是一个运用骨骼跟踪和语音识别技术的官方Demo：</p>

<p>[singlepic id=32 w=320 h=240 mode=watermark float=center]</p>

<p>在试用了几个不同的Demo后，我认为微软这次放出的SDK其实并非想象中好。问题主要在于开发平台比较单调，尽管支持非托管的C++代码，但是官方demo基本上都使用C#.NET/WPF开发，在与现有资源相结合的部分还做得不够好，而且demo的bug比较多；另外与OpenNi/NITE组合相比前者似乎性能较低——当然其精度较之后者能好一些，但区别并不明显。</p>

<p>荷兰人Jasper Brekelmans是一名长期关注Kinect技术的开发人员，他在<a href="http://www.brekel.com/?page_id=671" target="_blank">博客</a>上对截至目前微软的beta版和PrimeSense的OpenNi进行了比较。我们总结了他的文章，并将其划分为四个主要问题：</p>

<p><strong>问：什么是Microsoft’s Kinect SDK (Beta)能而OpenNi还不能做到的？</strong></p>

<p>答：目前微软的产品提供了音频支持、调整倾角的转动电机motor/tilt、在全身跟踪骨骼跟踪方面：非标准姿势检测（相对于OpenNi的投降姿势&hellip;），头部、手、脚、锁骨检测以及关节遮挡等细节上的处理更为细致（但精度是否更高还不能确定）。此外，SDK beta还提供了多机接口，使许多尚在设想中的特殊应用有直接变为现实的可能。当然，MS的开发库在安装上非常容易，也基本不需要任何设置即可使用。提供了可用视频和深度图输入的事件触发机制。</p>

<p><strong>问：那么微软的产品没有提供什么功能？</strong></p>

<p>答：对许多开发者而言最大的问题可能是微软对非商业使用的限制，而OpenNi则不存在这方面问题。此外，微软的SDK目前并未提供手势识别和跟踪功能，而在前一篇文章中我们已经看到用NITE的手势识别和跟踪组件控制鼠标光标的例子。</p>

<p>另一方面，SDK beta未能实现RGB图像/深度图像的互对齐，只是提供了对个体坐标系的对齐，而该功能在许多应用中实际上是很有必要的。</p>

<p>在全身骨骼跟踪中，SDK只计算了关节的位置，并未得出其旋转角度。从可移植的角度来看，SDK beta只能用于Kinect/Win7平台，而OpenNi还至少支持华硕的WAVI Xtion体感设备，今后支持的硬件平台还可能更多。相比较而言SDK beta不支持Unity3D游戏引擎、不支持记录/回放数据写入磁盘、不支持原始红外视频数据流、也不支持像OpenNi一样的角色入场和出场的事件响应机制。</p>

<p><strong>问:能不能再说说OpenNi/NITE提供的功能？</strong></p>

<p>答：可用于商业开发、包含手势识别和跟踪功能、可自动对齐深度图像和RGB图像，全身跟踪、关节旋转角度计算、看起来性能较好、跨平台多设备支持、已有众多游戏产品应用、支持记录/回放数据写入磁盘、支持原始红外视频数据流、支持角色入场和出场的事件响应机制。</p>

<p><strong> 问：那么OpenNi/NITE不能做什么？</strong></p>

<p>答：未提供音频功能、不支持调整倾角的转动电机motor/tilt、在全身跟踪骨骼跟踪方面：无法跟踪头部、手、脚和锁骨的旋转动作，需要标准姿势检测（即著名的投降姿势&hellip;），关节遮挡等细节上的处理似乎存在算法bug。不能自动安装并识别Kinect多机环境。安装过程较为繁琐，特别是NITE还要申请开发证书编码。OpenNi也没有提供可用视频和深度图输入的事件触发机制。</p>

<p><strong>总结：</strong> OpenNI最大的优势就是允许跨平台多设备，以及商业应用。但从原始数据的采集和预处理技术上看，微软的SDK似乎更稳定一些，况且还提供了不错的骨骼和语音支持。对于部分身体部位识别方面的功能，SDK beta没有提供局部识别和跟踪，这需要自己的后续开发（至少在相当一段时期内微软可能都不会提供此类功能）。OpenNi/NITE虽然提供了手势识别和跟踪，然而在全身骨骼姿势识别和跟踪上还要更多借鉴微软的产品。</p>

<p>因此，按照目前在社区中的表现，SDK beta和OpenNi/NITE孰优孰劣还真无法一下子确定。而且随着越来越多的开发者加入微软这一方，SDK beta的普及可能会更快，但在更高层次的应用上，对二者的选用往往是需要一定智慧的。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2011-08-11T10:47:36+08:00" pubdate data-updated="true">2011-08-11</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/kinect/'>kinect</a>, <a class='category' href='/blog/categories/xiang-mu/'>项目</a>

</div>


	
		<span class="comments"><a href="/blog/2011/08/11/ms-MS-Kinect-kai-fa-she-qu-de-zui-xin-jin-zhan//posts/3/index.html#disqus_thread">Comments</a></span>
	
</div></article>

<nav id="pagenavi">
    
    	
        	<a href="/posts/2" class="prev">Prev</a>
        
    
    
        <a href="/posts/4" class="next">Next</a>
    
    <div class="center"><a href="/archives">Blog Archives</a></div>
</nav></div>
	<footer id="footer" class="inner">Copyright &copy; 2015

    Han Yi

<br>
Powered by Octopress.
</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'hanyi';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





</body>
</html>
