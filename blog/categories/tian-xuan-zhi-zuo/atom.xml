<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 天选之作 | Wing of Dream 梦境之翼]]></title>
  <link href="http://www.hanyi.name/blog/categories/tian-xuan-zhi-zuo/atom.xml" rel="self"/>
  <link href="http://www.hanyi.name/"/>
  <updated>2015-08-31T11:54:27+08:00</updated>
  <id>http://www.hanyi.name/</id>
  <author>
    <name><![CDATA[Han Yi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 本质与陷阱]]></title>
    <link href="http://www.hanyi.name/blog/2015/07/21/microservices-why-it-is-trap/"/>
    <updated>2015-07-21T17:32:35+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/07/21/microservices-why-it-is-trap</id>
    <content type="html"><![CDATA[<p>最近两年，“微服务”这个词从最初的争议，到随后的逐渐接受，如今已经成为许多时髦的技术人员常挂在嘴边的词汇。在他们看来，似乎你不“微服务”就落伍了——因为你看，好像全世界都在“微服务”。然而当一些自诩为技术先驱的人开始尝试将其落地，并且后来发现自己当初根本就不了解到底“微服务”真正意味着什么时，我们才发现，要想革现代软件工程的命，远没发几篇文章、或者侃侃而谈来的容易。</p>

<h3>1. 微服务的本质</h3>

<p>可能你见过或者听过某些自称正在构建“微服务架构”的人，坦白的说，“微服务架构”真的不是指什么具体的软件架构，和大多数与之相关的早期技术一样，也没有一个标准告诉你什么是微服务、如何通过以下几个步骤构建微服务、或是让你参加培训，结束之后取得XXX认证，就证明你掌握了微服务架构。</p>

<p>其实微服务更像是一系列前沿软工思想的汇集，至于它为什么叫“微服务”，本系列的第一篇文章就已经提到过（虽然至今都有争议），但你可能会觉得它有点标题党——这也影响不了它拥有这个名字的既定事实。撇开名字，我们来看看微服务所承继的几个软件领域：</p>

<h4>基于业务概念建模</h4>

<p>从稳定性的角度考虑，围绕业务边界上下文所构建的接口要比来源于技术概念的接口更加不易变更，从而降低风险。这部分来源于领域驱动设计的主张。</p>

<h4>自动化</h4>

<p>自动化涉及方方面面，包括自动化测试、自动部署、自动配置管理乃至整个持续交付流程，都被认为是微服务落地的重要途径。</p>

<h4>隐藏内部实现细节</h4>

<p>隐藏实现的好处在于，一方面数据隐藏能够显著降低数据访问代码的耦合性；另外，服务内可以采用异构技术栈来实现，无论使用何种技术，你都可以用REST、RPC来实现服务间通信。</p>

<h4>一切去中心化</h4>

<p>去中心化的核心是组织自服务团队。近年来我们已经尝过自动化带来的好处，但鉴于Conway法则，设计开始来自领域而非现有的技术架构，而为了保证每个团队能够独立开发、测试和部署，按照服务划分现有团队是一个趋势（而非现实）。自服务团队能显著降低实施自动化的复杂度，更加贴近业务领域，并且形成一个分散的系统架构。与ESB或类似的编配系统完全相反的是，去中心化强调采用编排以及哑管道，并且在服务边界上采用智能终端来表现关联逻辑和数据，从而实现低耦合。</p>

<h4>独立部署</h4>

<p>强调每个服务都必须是可独立部署的，甚至当发生重大变更时，也倾向于采用版本共存终端来保证各自的独立性。独立部署能够带来更快的feature发布、强化自动化部署流程的一致性（而非经常需要人工编配部署管线）。推荐采用单服务－单主机的部署架构，从而避免部署单个服务时带来的边际效应；采用蓝绿部署/金丝雀发布来降低出错的风险；并且使用消费者驱动契约测试来保证集成接口的一致性。</p>

<h4>错误隔离</h4>

<p>分布式系统能够做到比单一系统更加可靠——前提是做好应对各种错误的万全准备（这也是人类毕生追求的目标&hellip;）。如果我们不能保证下游服务的正确性，系统就有可能发生级连式的灾难，且比单一系统脆弱的多。因此要时刻考虑网络的不可靠性、服务间通信的各种潜在问题：超时、隔板、断路器等在应对灾难时的作用、理解网络分区带来的后果、以及权衡一致性和可用性两种方案。</p>

<h4>完备监控</h4>

<p>传统意义上的监控，恐怕只是一个简单的dashboard，列出所有运行中节点的状态，并且当某个节点的指标超过阈值时产生警告。但随着去中心化的深入，传统监控方式根本不能应对复杂架构出现的各种问题。因此，采用语义化监控、综合化监控去模拟用户的真实行为，聚合用户日志和统计数据，采用关联ID跟踪消息在系统内的传递路径。</p>

<p>严格的说，不具备上述特征的系统就不能被称作微服务。事实上，当你开始尝试上述某个或某几个新的领域时，多数情况下你的系统无疑蕴涵了极大的风险——因为只有整体采用才能实现微服务的闭环，否则只能是一个摇摇欲坠的怪胎。有的人可能认为可以从简单模型开始演进——但注意，这里所谓的演进并不是从某个独立的方面出发，而是要在一开始就要做通盘设计，并且以简单方式先落地而已。</p>

<p>那么企业费尽千辛万苦、从事如此巨大的迁移工程——效果真的能够立竿见影吗？</p>

<h3>2. 发觉陷阱</h3>

<p>一些谨慎的开发者认为，可以先从单一系统入手，再逐渐向微服务过渡。对此，微服务的拥簇者声称：“你将无法（或很难）精炼出微服务架构，如果不在项目的最开始就设计成那样”。</p>

<p><em>&ldquo;这无疑又在扯过早做大型设计的蛋（That&rsquo;s BDUF Baloney）。&rdquo;</em></p>

<p>Uncle Bob更倾向于“迫使无知”，即好的架构应尽量保证组件对部署和内部通信方式的透明，这样就可以根据实际情况选择部署成微服务，或多个单一系统＋应用服务器，亦或纯粹的单一系统。这种“实际情况”有时可能是需要大规模扩展的、只需要少量部署的、以及单个节点足矣的。</p>

<p>一旦打破了“迫使无知”，则很有可能会造成过度设计，使得太多因素过早被考虑进来，最终却形成一堆烂摊子。对于微服务强调的其它几个方面，例如跨语言的服务实现——确实，采用REST的通信架构使得服务间实现强解耦，包括语言、数据库等。但是，什么样的业务需求会导致项目初期就要切割成不同语言、甚至采用不同的数据库技术呢？这种高配置复杂度的架构随之带来的好处又能有多少？</p>

<p>而对于“单一系统”的说法，Uncle Bob更认为这种广告式的宣传，使得采用非微服务形式的系统听起来就像是一块又大又丑的“石头”，造成受众对微服务架构的盲目崇拜，进而为布道者带来利益。</p>

<p>比起微服务，Uncle Bob更加倾向于一种“<a href="http://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">整洁架构</a>”的风格，后者是对近年来一些流行系统架构的全面总结，焦点主要集中在代码架构方面，而对微服务强调的独立部署、异构数据库等持开放的观点。</p>

<h3>3. 稳步向前</h3>

<p>当然，无论是微服务，还是整洁架构，能最终轻松玩转的仍然是极少数。例如，从源头的设计出发，对领域的理解会影响边界上下文的稳定性，你要确信自己真的“彻底认识”了该领域，否则后面的一切都是空谈，还不如老老实实把单一系统做好。</p>

<p>另外，如果任何环节存在未知领域，就尽量不要采取过于激进的做法——这是遏制风险的关键。</p>

<p>对于任何一种架构，只有当整体规模增加到一定限值时，才能够真正考验架构的有效性，而面向未知又十分危险——因此强烈不建议如此激进地改变根本架构。毕竟在整个生态圈中，技术团队只是起到实现和保障作用，过早、过度引入风险是完全没有必要的。</p>

<p>那么何时才能选择向前呢？如今声称自己正或多或少实现微服务架构的团队包括Amazon、Netflix、REA和Gilt等，无一不是从现有实践中持续发掘存在的问题，并为此开发了一系列工具、平台甚至整体实践，其中的部分已经成为流行的开源项目——可见，对于某些技术能力领先世界的公司来说，微服务之路尚且存在各种困难，而剩下积累不深却大谈微服务的团队，要么是盲目追新，要么就是醉翁之意不在酒了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 层级演进（下）]]></title>
    <link href="http://www.hanyi.name/blog/2015/07/19/microservices-scale-3/"/>
    <updated>2015-07-19T17:32:35+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/07/19/microservices-scale-3</id>
    <content type="html"><![CDATA[<h3>8. 服务发现</h3>

<p>随着服务数量的增加，如何获取某个服务的信息则成为挑战，同时它也是部署监控的前提条件。另一些开发中遇到的例子，如：一、想获取当前环境中运行的服务（也就是采集监控对象）；二、想获取当前系统中的API列表从而避免重新发明轮子；这些需求其实都隐含了一种新的需求：针对“服务”的发现机制，在近年来发展非常迅速，已经形成了几个相当成熟的解决方案。它们基本上都遵循了如下工作模式：首先提供一种针对服务的注册过程，当启动新实例时就将其注册进系统；然后，提供一种发现系统内已注册服务的途径。我们先来看一种历史最悠久也最简单的解决方案：</p>

<h4>DNS</h4>

<p>DNS是解决服务发现问题最直接、简便和有效的手段。因为借助DNS，尽管实例IP可能随时发生变化，但域名是不变的。因此在早期，大多数应用都会采用类似<code>&lt;service-name&gt;-&lt;environment&gt;.company.com</code>这样的二级域名来指定服务地址。有的甚至去掉了environment，直接搭建不同的DNS服务器并设置不同的映射表，从而实现配置管理的一致性。后者看起来很方便，但成熟的解决方案不多，亚马逊的Route53服务是少数优秀的代表。</p>

<p>然而采用DNS也存在一定风险性。在DNS协议中存在TTL的概念，也就是说主机内部会维护一个DNS表，当TTL未失效时系统仍将采用本地的DNS映射表，这种设置让实例的更新带来一定风险。为了解决这一问题，DNS并不直接指向实例IP，而是若干实例前设置的反向代理（这个通常是固定不变的）。</p>

<p>DNS方案由于其易用性而得到了大量的应用，但随着微服务架构规模的增加，这种方法的有效性和可维护性将逐渐降低，特别是DNS服务器节点的存在，于是就出现了更多动态服务发现方案。</p>

<h3>9. 服务注册</h3>

<p>由于DNS本身的局限性，许多基于服务注册的发现方案被提出并得到了广泛应用。</p>

<h4>Zookeeper</h4>

<p>Zookeeper最初是Hadoop项目的一部分，其用途非常广泛，包括配置管理、同步服务间数据、选举Leader、消息队列以及命名服务（这才是本文重点）。Zookeeper首先是高可用的，其后台是一个集群并且实现数据Replica；Zookeeper的核心提供一种存储信息的层级命名空间，客户端可以在该层级插入、修改和查询节点；此外，它们能够添加针对节点的监视以观察该节点是否发生变化；也就是说，我们可以把服务的地址信息保存在这种存储结构，并且当其中的信息发生改动时执行其它变更操作，例如关闭上游服务。</p>

<p>Zookeeper是一个非常通用的系统，你可以只把它看作是一个能够保存信息树，并且能在任何变化时发出警告的高可用服务。由于其底层性和单一性，目前针对Zookeeper的扩展库也非常之多。从现在的时间来看，Zookeeper可能有些过时——因为与更多后起之秀相比，它看起来不是那么开箱即用，但不可否认其曾经十分流行。必须承认，由于Zookeeper底层算法PAXOS实现起来非常困难，而该产品又饱经考验，任何企图重新发明轮子的组织都在此吃过不少苦头。这正是分布式协调系统复杂性的体现。</p>

<h4>Consul</h4>

<p>Consul同样支持配置管理和服务发现，但提供了更多高级功能，从而更加易用。例如针对服务发现的HTTP接口，以及一个简单的DNS服务器，甚至还提供SRV记录，能够同时提供给定名字的IP和端口信息。这就使得曾经采用DNS服务的公司迁移到Consul变得非常容易。</p>

<p>Consul还提供了节点健康检查功能，这就包含了一部分监控系统的能力，并且由于其高容错设计和依赖临时节点机制的系统的针对性，使其有能力在某些情况下完全替代Nagios和Sensu。</p>

<p>Consul的RESTful HTTP接口覆盖了从服务注册、查询信息、以及健康检查等大部分功能，使其与现有系统集成变的十分简便。其底层依赖的Serf能够检测集群内节点，进行错误管理以及发出警告。良好的设计使Consul如今成为该领域极具竞争力的明星之一。</p>

<h4>Eureka</h4>

<p>Eureka是由Netflix开源的、只关注服务发现领域的系统。Eureka提供基本的负载均衡功能，基于REST的集成终端，以及基于Java的客户端库。但是，由于Eureka是基于JVM并且实现了所有客户端，这对于多语言环境来说可能会是一个挑战。</p>

<h4>自己动手</h4>

<p>在某些公有云服务器中，服务发现可以借助一些内置功能来实现，当然这要建立在你对该产品非常熟悉，且系统规模较小的情况之下，通常会与采用上述重型框架相比省去很多麻烦。另一方面，服务发现除了是对分布式系统内部提供信息，任何现有的report和dashboard都可以借此提供实时的节点信息，从而允许人工检查。</p>

<h3>10. 服务文档化</h3>

<p>随着微服务架构的实施，更多时候我们希望能够以API的形式暴露一部分服务缝隙，从而允许其他开发者接入更多类型的服务。但是构建开放API的过程是非常复杂的，特别是在采用微服务架构时，如何保证服务文档的实时性将成为挑战。这里介绍目前流行的两个服务文档技术：Swagger和HAL。</p>

<h4>Swagger</h4>

<p>Swagger能够通过一组Web界面展示当前的API信息，而你只需要定义一套POST模版即可。Swagger要求相关服务暴露一个符合标准的文件（可以通过Swagger提供的标准库实现），例如对于Java你可以使用方法标注描述相关API。</p>

<h4>HAL和HAL浏览器</h4>

<p>HAL即Hypertext Application Language，是一种超媒体资源的描述标准，HAL同样提供用户界面和标准库。通过Web界面，用户不仅可以查看API信息，还能直接调用相关API。不同的是，HAL中所有的文档信息都基于超媒体控制，因此很容易就能够将信息对外展示。这就限制了用户只能使用超媒体描述系统内资源，而从现有系统迁移则远没那么容易。</p>

<h3>11. 自描述系统</h3>

<p>在SOA的发展初期，类似全局描述、发现以及集成UDDI标准被用于帮助人们理解当前运行着什么服务。事实证明，UDDI对大多数项目来说都非常重，从而导致更多替代产品的出现。</p>

<p>不可否认，获取系统的整个框图并理解其架构对于团队成员来说十分重要。通过关联ID跟踪下游服务并理解调用链、从而让我们更快理解系统原理。采用类似Consul的服务发现技术，能够让我们查看当前运行的微服务。HAL能展示出当前终端上包含的所有服务能力，同时状态监控页面和相关系统能够使我们整体和局部的状态。上述信息都能够用于为进一步设计提供线索，且比只有一个简单到随时会过期的wiki页面强得多。尽管多数情况下我们都是从后者开始的，但不要忘了把上述信息逐渐引入到系统中，随着系统复杂性增加，描述信息的演化能让我们更好的理解系统。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 层级演进（中）]]></title>
    <link href="http://www.hanyi.name/blog/2015/07/18/microservices-scale-2/"/>
    <updated>2015-07-18T17:34:06+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/07/18/microservices-scale-2</id>
    <content type="html"><![CDATA[<h3>5. 缓存</h3>

<p>当某个操作的结果可以被临时存储，从而使后续的相同操作能够直接采用上述结果，而非花费同样时间和资源在重复操作上，这种性能优化方法就是缓存技术。如今面临大规模的HTTP访问量，缓存已经成为解决性能瓶颈的大杀器之一。</p>

<p>当然，即使是单一系统，目前也存在众多缓存方案可供选择。但对微服务架构来说，可选择的就更多——问题是对于分布式系统而言，由于节点间的相互对等性，缓存存在于客户端或是服务端，会是一个艰难的选择。</p>

<h4>客户端、代理和服务端缓存</h4>

<p>对于三个主要的缓存承载对象之一，客户端缓存主要是把数据放在客户端中，并由客户端决定是否从服务端刷新数据。一种理想的情况是，由服务端负责提供是否需要刷新的线索，然后由客户端决定是否刷新，这种技术被广泛用于HTTP缓存。</p>

<p>代理是指位于客户端和服务端之间的设施，由代理决定缓存机制的包括反向代理或CDN技术。</p>

<p>另一种方法是由服务端负责缓存技术，如采用Redis或Memcache，或者更简单的一些片内缓存技术。</p>

<p>对上述方案的选择主要取决于优化的目标。例如，客户端缓存主要目的是降低网络负载，并且实现起来最为简便；缺点是一旦实行成熟的客户端缓存机制，服务端要想做出什么修改就比较困难，特别是容易引发未经验证的脏数据问题（下文会做进一步讨论）。而对代理缓存来说，其对客户端/服务端都是完全透明的，且针对普通HTTP通信实现也很简单，例如采用反向代理如Squid或Varnish。尽管代理会引入更多网络路由节点，但这对随后的性能提升来说基本是可以忽略不计。服务端缓存能够保证对客户端实现透明化，而且一旦缓存位于服务边界上，就能很容易解决脏数据、分析和优化命中率。同时服务端缓存对于系统性能的提升是最为有效和迅速的——但实现起来又稍复杂。</p>

<p>一般缓存技术的选择可以是混合性的，但也有分布式系统内部完全不设缓存的情况，因此要具体问题具体分析。</p>

<h4>HTTP缓存</h4>

<p>HTTP协议提供了全面的缓存技术，包括客户端和服务端。首先，HTTP头的cache-control指令告诉客户端是否应该缓存当前响应信息，以及缓存时间。同样，还包含一个Expires属性，用于标记该信息在某个时间点后过期，而非持续多长时间。对于静态网站数据，如图像、CSS等，使用cache-control的time to live（TTL）就能够很好解决这一问题。</p>

<p>除了cache-control和Expires，HTTP还提供了一种Entity Tags（ETags）机制。ETag其实是用来标记当前信息是否已经发生变化，即使URI不变，也能识别该信息是否是最新的。应当明确，ETags本身不提供缓存机制，而是通过Conditioanl GET实现，后者是说在GET请求中添加某些头信息，从而告诉服务端只有当该条件满足时才返回整个信息。</p>

<p>当然，ETags还少不了要打上cache-control，只有当TTL或Expires满足时，客户端才会发送信息并携带If-None-Match: ETag值。服务端通过读取If-None-Match信息，判断客户端的信息是否已经过时，如果已是最新，则返回304 Not Modified，否则返回整体信息和200 OK。这些方法在反向代理上被广泛采用，甚至包括CDN如AWS的CloudFront和Akamail。一般的HTTP标准库也都包含这些功能。</p>

<p>无论是cache-control、Expires或ETags，其在功能上相互存在一定重叠。而这也确实会对HTTP通信带来一些问题，特别是混合使用时。建议参考《REST in Practice》或HTTP/1.1协议的13章。</p>

<p>即使不采用HTTP协议进行消息缓存，也应当参考这部分的设计思路，无论对客户端/服务端的缓存实施都是很有价值的。</p>

<h4>写缓存</h4>

<p>对写操作设置缓存在某些场景下是必要的，例如当写操作突然增加时，短期间内频繁I/O会降低系统性能，好的做法是先把数据写入本地缓存中，之后再统一更新至下游节点。另一方面，当下游服务发生不可用的情况时，写操作的数据会被较好的保存，并不会造成数据丢失的问题。</p>

<h4>恢复缓存</h4>

<p>缓存也能够被用于灾难恢复。例如对客户端缓存来说，当下游节点出现问题，客户端就可以把本地缓存作为临时数据，而不是直接导致不可用。Web技术早起曾经有一种Guardian技术，作用是定期生成整个网站的静态内容，当系统离线时就采用静态版本提供服务，尽管数据不是最新的，但仍可以向用户提供服务。</p>

<h4>隐藏源</h4>

<p>由于未命中缓存的存在，一般系统的源都会面临未缓存请求的访问，当请求数量突然增加时就可能导致源崩溃，继而影响整个分布式系统。因此，在某些系统中，请求永远不能直接访问源，当缓存未命中时会立即得到错误响应，同时缓存节点也会把相应的请求信息异步发送至源，再由源把所需数据同步至缓存节点。这种做法的好处就是完全避免了源遭遇大访问量的情况，从而提高系统整体稳定性。</p>

<h4>缓存建立的原则</h4>

<p>目前业界有一种趋势，就是多级缓存＋各类缓存。实际上，缓存就意味着脏数据的可能：当缓存层级增加，脏数据的概率和数量也会随之增加，对于某些业务场景而言这是不可容忍的。特别是当采用微服务架构，类似后果的影响会被无限放大。因此采用缓存的原则就是：简单、单一、有效。</p>

<p>另一方面，采用缓存要时刻保持警惕，因为脏数据对某些业务场景来说几乎是零容忍的。更何况不同缓存所带来的约束也各不相同——例如HTTP缓存中的Expires: Never，就可能导致某些用户理论上永远无法更新数据，其后果可想而知。因此，一定要在完全理解概念的基础上，再谨慎引入缓存技术。</p>

<h3>6. 自动扩展</h3>

<p>自动扩展的前提是基础设施的全面自动化，此类问题在前几篇文章已经提到多次。但是，仅有运维自动化还不足以实现自动扩展——应用至少应部署在公有/私有云之上。例如，在一天中的某个时间段内系统负载总是面临峰值考验，如采用AWS公有云服务的话，动态开启/关闭节点可以节省成本，同时还能在面临高负载时应对自如，当然前提是你手中有足够的运营数据。这种根据数据决定基础扩展策略的方式，通常被称作预测式扩展。</p>

<p>另一种自动扩展方法是反应式扩展。如果某个节点出现问题，或系统负载突然达到一个危险的峰值，则自动启动新节点以增强吞吐能力。反应式方法的关键在于发现问题、启动节点的速度，当然一方面是靠团队Devops的能力，另一方面还要得到云服务商的支持。</p>

<p>实践证明，无论是预测式扩展，或是反应式扩展，其对提高系统整体可用性都是非常有效的。这里的建议是，当团队初次涉足该领域时，应优先考虑设置节点失败条件下的反应式扩展功能，然后再根据更多数据采取进一步行动。</p>

<h3>7. 分布式系统与CAP原则</h3>

<p>十多年来，CAP原则成为构建分布式系统时不可逃避的话题。尽管CAP原则在其科学性上倍受<a href="http://markburgess.org/blog_cap.html">质疑</a>，但其作为权威的实践经验仍不失指导性。</p>

<p>CAP分别表示一致性、可用性和分区容错性。一致性是指任何节点上的数据都是保持一致的；可用性意味着任何请求都会得到响应；而分区容错性指当系统内部通信出现问题时，依然有能力保证整体可靠性。</p>

<p>CAP原则的基本内容就是，分布式系统始终无法同时囊括上述三种属性，多数情况下只能择其二设计。在实践中，与CAP原则相符的例子比比皆是，假设一个分布式系统包含A、B两个实例，后台数据存储在Ad和Bd两个Replica数据节点上。当Ad和Bd间的网络通信中断时，如果不关闭A或B的其中一个，则Replica的数据就会无法实现同步，从而造成不一致。而如果关闭实例A或B，则会降低系统可用性。这就显示出一致性和可用性之间的矛盾——分布式系统无法放弃分区容错：因为其本身就是为此而生。</p>

<h4>AP系统</h4>

<p>如果强调可用性，就意味着一致性可能出现问题。一般情况下，如果多个数据节点间发生不一致，也可以在某个时间后使其交换数据以达到同步，而非立即实行。这就是强调多次的最终一致性。最终一致性系统预示着用户将有可能收到脏数据，这就要建立在具体的业务需求基础上了。</p>

<h4>CP系统</h4>

<p>有时我们不可避免地面临强一致性，即使因为无法达到一致而被迫拒绝服务——这就是牺牲可用性的强一致性系统。然而，在分布式领域，稳定可靠的强一致性系统实现非常困难，一方面是由于分布式事务操作的低效性，另一方面是实现分布式锁算法的困难性（即使是单进程锁，要踩的坑也很多）。因此如果确实要构建强一致系统，采用现有成熟方案非常重要——因为它们通常都经历了时间的考验。当然对比前者，建立AP系统可能就要容易许多。</p>

<h4>孰优孰劣？</h4>

<p>AP和CP，选谁？前者只能实现“最终一致”，后者实现起来则“困难重重”。然而不能否认，在金融系统中，一致性几乎是必须的；而对大部分系统而言，AP则是更加现实的做法。要知道，在大多数情况下，选择AP和CP并非零和游戏，整个系统中可以同时存在AP和CP两种架构，以满足不同需求。要提醒的是，如果采用CP模式，调研好现有方案并择优采用是前提。AP则是更加具有普适性的大众选择。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 层级演进（上）]]></title>
    <link href="http://www.hanyi.name/blog/2015/07/11/microsevices-scale/"/>
    <updated>2015-07-11T08:21:19+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/07/11/microsevices-scale</id>
    <content type="html"><![CDATA[<p>通过过去的数篇文章，我们分别介绍了微服务架构从设计、开发到实施等各个方面，并以业界的前沿应用给出了一定实践案例。然而世界总不是你我想象中的美好，面临千奇百怪的业务需求，随着架构的不断变化，我们总会遇到各式各样的冲突和风险。因此，微服务架构并不适用于一拥而上——循序渐进才是可靠途径。</p>

<h3>1. 从可用性（Availability）说起</h3>

<p>分布式系统领域存在一个经典的谬误理论：<a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">Fallacies of distributed computing</a>，基本总结了不可靠通信给系统架构带来的影响。由于网络通信在分布式系统中的极端重要性，使得网络可靠性成为此类系统永恒的梦魇。任何网络的不可靠都会引发各种灾难，且难以被察觉、定位和修复。即使有能力购买先进的硬件设施，以及最可靠的第三方系统，也不能消除出错的可能。</p>

<p>实际上，问题在于设置一个容错率。因为对跨功能的需求是永不枯竭的，根据业务特点设置容错率，然后按需堆叠架构，才是一个看起来合理的策略。在测试一文中我们已经提到跨功能需求，例如对于响应时间/延迟的需求，应根据现有设施进行合理评估，再和业务需求阈值比较，从而决定改进方案。对于可用性，是否必须是一个24/7类的业务？或者用户完全能够容忍某个偶尔的不可用？对于数据的持续性，业务需求的数据保存时限如何？只有业务需求明确，才有必要开始考虑进一步的演进。</p>

<p>当需求明确时，我们就可以考虑演进现有架构了——但别忘了前面提到的：分布式系统总会带来不可靠。那么如何度量当前系统的可靠性呢？最有效的手段就是利用现有的监控系统，统计出错率。在正常情况下，系统运行数据能够反映出一定的可靠度，但对某些意外情况，例如网络或其它硬件设备损坏，就很难真的实现了。例如，当服务A需要调用服务B的API时，B开始发生某些本地错误，且不能及时断开链接。那么A将持续保持对B的链接，直到链接超时自动断开，但随着A继续接收上游请求并继续发起对B的链接，从而造成服务A维持一个大规模HTTP链接池，当A的系统资源耗尽，灾难就会随之发生了。</p>

<p>上面提到的例子在分布式架构中已经非常常见，开发人员已经知道若干种解决方案，包括合理超时、改进链接池、以及开发断路器等。但首要问题是如何发现此类问题？</p>

<p>Google每年会组织一次灾难恢复测试（DiRT，Disaster Recovery Test）练习，练习内容包括了针对大规模不可抗拒灾难的演练（例如地震）。Netflix则更进一步，每天会通过工具随机破坏生产环境，用来测试系统的可靠性和恢复能力，此类工具被称作Simian Army。Chaos Monkey是其中比较著名的一种，其基本功能是每天随机性关闭AWS上的某些节点；Chaos Gorilla能够断开整个可用的数据中心；Latency Monkey能够模拟节点间通信缓慢的情况；上述工具都已经被Netflix贡献给了开源社区。</p>

<p>因此，在互联网分布式系统的世界中，首先需要改变的就是传统的思维模式：无论系统有多健壮，失败必然发生。那么接下来我们需要考虑的就是如何在失败发生时采取补救措施——大多数情况下这比减少失败来得更容易。</p>

<h4>超时</h4>

<p>超时是一项经常被人忽略的关键反馈，因为它预示着下游服务出现了不可预知的问题。但难点在于多长时间才算超时？其实也有简单解决办法，首先，可能引发超时的操作必须在进程外执行；超时时限应在全系统内保持唯一值；一旦发生超时，即记录进日志系统。定期查看超时部分的记录，进行有效改善。</p>

<h4>断路器</h4>

<p>想象一下家中电表里的空气开关，当电流达到峰值时，空开自动关闭，从而避免对家中电器造成损坏。当屋内长期无人时，也可以手动关闭空开，防止意外发生。空气开关就是断路器的一种，而在《Release It!》中，断路器则成为提高系统可用性的关键手段。</p>

<p>再思考一下超时的情况。一旦下游服务故障，服务调用将发生超时，由于时限是默认设置的，越来越多的超时将发生，并进一步拖慢系统，直至崩溃（前文已提过）。断路器的作用是，当超时发生次数超过一个阈值，就切断到某个服务的通信，即超时时限趋近于零。即“快速死亡”。</p>

<p>断路器能够避免错误发生时，故障蔓延至整个系统。在无法确定分布式系统可靠性的情况下，也确实是一个十分有效的解决方案。但这里需要注意，断路器并不是什么万能神器，只不过是起到“壮士断腕”的作用——想象一下，万一断路器“过早”启动了呢？在实践中，如果服务调用是异步的，那么采用消息队列进行处理也没什么问题。而如果是同步调用，就需要考虑一下引入断路器提高可用性了。</p>

<h4>隔板</h4>

<p>《Release It!》中提到一种隔板模式。这种模式可以解释成船舱中的隔板结构，当船舱进水时，为了避免整船沉没，通常会选择关闭已经进水的船舱。一个简单的例子是采用服务——连接池模式，即对于每一个下游服务都保持独立的连接池，即使某个下游服务出现问题，也只会导致对应的连接池拥塞，而不会影响整个系统。</p>

<p>在实际应用时，隔板方法几乎是标配的做法，而超时和断路器则可根据情况添加，因为前者是预防问题，而后者更多是收拾残局。例如对于同步调用，配置断路器则是必须。上述模式在实现起来也有很多开源选择，例如Netflix的Hystrix，.Net的Polly，以及Ruby下的circuit_breaker。</p>

<h3>2. 幂等操作</h3>

<p>无论同一个操作执行多少遍（大于等于1），输出结果不变，这就被称为幂等操作。幂等性在构建Web应用时非常重要，因为消息可能因为某些原因被重放多次，而当这些消息都到达服务器时，不应把这些消息认为是不同的请求进行多次处理。保持幂等操作可以提高应用的可靠性，这在实践时非常重要。</p>

<p>值得一提的是，HTTP协议天然定义了GET、PUT操作应是幂等性的，如果破坏了这一原则，则可能会给系统带来许多问题。</p>

<h3>3. 应用层扩展</h3>

<p>扩展的目的无非有二：容错和高性能。最直观的扩展方法就是更换CPU和I/O设备，被称作纵向扩展，这种方法实际上成本非常高昂，特别是当前芯片研发和生产已经陷入烧钱的地步，即使不差钱，要让应用充分利用多核和高速I/O架构也非易事。因此实际中更多采用的是横向扩展，特别是当虚拟化技术普及，若干廉价服务器组成的集群就可以很好解决扩展的问题。微服务架构的扩展技术实际上与单一系统类似，但也有一些区别。</p>

<h4>分担负载</h4>

<p>之前讨论过单服务－单主机的微服务部署方式，在实际应用中，为了节省成本更多团队选择在同一台主机上运行多个服务，但是随着负载需求的增加，采用单对单部署则是一种相对简单有效的解决方法。</p>

<p>另一方面，如果某个服务负载确实很高，单纯的主机数量增加不能直接解决微服务架构的问题，那就要进一步考虑做拆分了（这种拆分避免不了对业务的考量，如果是非核心功能，还是建议直接独立出来）。</p>

<h4>平摊风险</h4>

<p>横向扩展能把风险平摊到不同主机，从而提高可用性，但具体操作起来需要考虑许多因素。首先，基于现有的虚拟化平台，即使实现单对单部署，所谓的“主机”也不过是一台虚拟主机，不能保证“主机”不在同一台物理机上，那也就不能实现平摊风险的效果，不过还好许多平台目前已经支持了跨不同主机运行服务的功能。</p>

<p>在企业内部虚拟化平台中曾流行采用Storage Area Network，即SAN。SAN是一种十分昂贵的存储区域网络，旨在达到无错运行。而一旦采用SAN且后者挂掉，则整个虚拟集群就会受到影响，因此实际上还是不能避免单点失败的问题。</p>

<p>另一个常见做法是采用多数据中心运行，这其实和IaaS的提供商有关。例如AWS采用的多区域数据中心架构，其保证对EC2实现99.95%的正常服务概率每月每区域，为了实现更高可用性，在此基础上部署应用到不同区域即可实现该目的。</p>

<p>无论如何，对服务商的Service Level Agreement，SLA的熟悉也十分重要，当灾难真的发生，如何“维权”也就在于你对该协议的理解程度了。</p>

<h4>负载均衡</h4>

<p>负载均衡是一种十分简单有效的横向扩展技术，目前基本成为多实例架构的标配。负载均衡组件能够接收来源请求，根据自身算法把该请求发送至多个平行实例中的某一个，从而实现负载分担。</p>

<p>现在，负载均衡技术非常成熟，包括从硬件方案到软件方案（例如mod_proxy）。不过其功能都类似，当某个实例不可用时将其屏蔽，一旦恢复则将其重新开启。一些负载均衡设施提供更加高级的功能，例如SSL终端。我们已经讨论过采用HTTPS防止中间人攻击和消息泄密的问题，但对系统内部的服务间通信来说，HTTPS会影响其整体运行性能。因此SSL终端就起到了把HTTPS转换成HTTP从而在内网通信的作用。</p>

<p>商业负载均衡，如AWS的ELBs就支持上述功能，同时还可以在系统内部构建VLAN，从而实现内部的高效通信。</p>

<p>对于硬件负载均衡设备来说，自动化始终是一个问题。如果企业更信任硬件方案，也不妨在后端搭设一个软件负载均衡设施，从而提高自动化的能力。</p>

<h4>基于Worker的架构</h4>

<p>负载均衡并非分担系统负载的唯一选择，基于Worker的系统架构同样有效。Worker被用于如Hadoop批处理进程模型和共享队列的异步事件监听，擅长批处理和异步处理。如图片处理、发送邮件和生成报表等。一旦达到系统峰值，新的Worker能被添加进当前系统，从而增加吞吐量。</p>

<p>对Worker架构来说，尽管每一个Worker可以是不可靠的，但组织Worker的中央系统应保证可靠性。例如采用持久消息广播Zookeeper。</p>

<h4>重新上路</h4>

<p>当目前的架构无法应对越来越多的用户，重新设计可能是必然的选择，因此初期预留扩展接口是有必要的。当然，如今许多人会选择一开始就预估一个巨大的访问量，从而在此基础上直接构建大规模架构。这是一件非常危险的事，甚至有可能引发灾难。因为在初期我们更强调精益化，而非大而全。后者明显会引发无用的付出。</p>

<p>因此要时刻了解：架构的演进是必须的过程，我们能做的只是留个心眼儿。何况系统扩展的需求并非标志着失败，而是巨大的成功。</p>

<h3>4. 数据层扩展</h3>

<p>前文提到的扩展，通常只适用于针对无状态的服务。而如果是操作数据库的服务，情况则不同。最为直接的方法就是，借助数据库产品自身提供的扩展特性，而这就需要在项目初期就按需确定数据库产品的选择。</p>

<h4>服务可用 vs 数据持久</h4>

<p>应当明确的是，这两者完全不是一回事，因此解决问题的思路并不一致。例如当产品数据库出现问题，由于数据本身存有备份，因此持久性没太大问题。但数据库本身不可用，整个依赖于此的应用服务也就不可用。用现代运维技术的观点来看，即使设置了Primary和Replica，当Primary出现问题，如果缺少同步Replica到Primary的机制，就还是缺少可用性。</p>

<h4>扩展读库</h4>

<p>针对读操作密集的应用，其扩展复杂度要比针对写操作低得多。因为目前存在各种各样的缓存技术（下篇文章将讨论），这里给出另外一种数据库架构：读备库。</p>

<p>在关系型数据中，例如MySQL和Postgres，数据可以从主节点被复制到多个备库节点，这样一来确保了数据的安全性，同样也可被用于分担读操作。一般来说，写操作由一个主节点负责，读操作则由更多备库节点负责。复制操作发生在写操作的之后，从而在读库上存在一定的脏数据时间，当然最终能够保证一致性。这种系统其实是所谓的“最终一致”，通过牺牲严格的一致性来达到系统的扩展（参见分布式系统的CAP定律）。</p>

<p>然而随着缓存技术的发展，实现缓存要比搭建读库方便得多，成本也更低。一般也应该根据业务需求选择不同的扩展方式。</p>

<h4>扩展写库</h4>

<p>前面提到写操作扩展相对不那么容易，但也存在一般方案：分片。分片可以在写数据时，将数据按照某种哈希规则分布至多个节点上，从而实现横向扩展。在现代数据库技术中，分片几乎成为标配。</p>

<p>分片的难点在于处理查询问题。在单片数据库中，查询基本上是通过内存和索引一次完成的，但是对于分片数据库，查询可能分别发生在不同的节点中，而这种多次查询的方式最终用异步机制组织起来，并配置缓存。例如MongoDB，就采用Map/Reduce架构解决查询问题。</p>

<p>分片系统的另一个问题是，当要增加新的数据库节点时，系统需要整体停机，特别是针对大规模的分片＋Replica系统。目前最新的数据库技术已经支持在线扩充节点，并且能在后台保证数据的重新平衡，例如Cassandra。</p>

<p>分片能够实现读操作的横向扩展，但无法满足扩展的另一个需求：可靠性。Cassandra先行一步，实现了环上多节点备份以提高可靠性。</p>

<h4>CQRS模式</h4>

<p>CQRS指命令查询职责隔离，这是一种极端的数据解耦模式。在标准数据库应用中，通常采用同一系统执行数据操作和查询。但是在CQRS模式下，负责处理命令并操作数据的模块和查询模块应当是分离的。</p>

<p>CQRS带来的隔离性使得扩展方式更加多样化，同时一些业务需求也确实符合操作/查询分离的情况。在CQRS中，一种更加极端的形式甚至是同一个数据仓库，分别存在基于图的表示和基于键值对的表示等各种读方案。然而需要提醒的是，CQRS的应用场景目前还比较狭窄，因为传统CRUD的方式与次存在很大程度的不同，对团队来说将是一个巨大的挑战。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby Web服务器：这十五年]]></title>
    <link href="http://www.hanyi.name/blog/2015/06/28/ruby-webserver-last-10-years/"/>
    <updated>2015-06-28T16:39:54+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/06/28/ruby-webserver-last-10-years</id>
    <content type="html"><![CDATA[<p>坦率的说，作为一门年轻的计算机语言，Ruby在最近二十年里的发展并不算慢。但如果与坐拥豪门的明星语言们相比，Ruby就颇显平民范儿，表现始终不温不火，批评胜于褒奖，下行多过上扬。但总有一些至少曾经自称过Rubyist的程序员们，愉快地实践了这门语言，他们没有丝毫的歧视习惯，总是努力尝试各家之长，以语言表达思想，用基准评判高下，一不小心就影响了整个技术发展的进程。</p>

<p>本文谨以Ruby Web服务器技术的发展为线索，回顾Ruby截至目前最为人所知的Web领域中，重要性数一数二的服务器技术的发展历程，试图帮助我们了解过去，预见未来。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/timeline.png" alt="Ruby Web服务器发展时间轴" /></p>

<h3>一、随波逐流</h3>

<p>长久以来，任何Web服务器都具备的两项最重要的功能：一是根据RFC2616解析HTTP/1.1协议，二是接收、处理并响应客户端的HTTP请求。幸运的是Web技术的发展并不算太早，使得Ruby恰好能赶上这趟顺风车，但在前期也基本上受限于整个业界的进展。像Apache HTTP Server、Lighttpd和Nginx这些通用型Web服务器＋合适的Web服务器接口即可完成大部分工作，而当时开发者的重心则是放在接口实现上。</p>

<h4><a href="http://ruby-doc.org/stdlib-2.2.2/libdoc/cgi/rdoc/CGI.html">cgi.rb</a></h4>

<p>作为Web服务器接口的早期标准，CGI程序在调用过程中，通过环境变量（GET）或$stdin（POST）传递参数，然后将结果返回至$stdout，从而完成Web服务器和应用程序之间的通信。cgi.rb是Ruby官方的CGI协议标准库，发布于2000年的cgi.rb包含HTTP参数获取、Cookie/Session管理、以及生成HTML内容等基本功能。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/cgi.png" alt="Web服务器和CGI" /></p>

<p>当支持CGI应用的Web服务器接到HTTP请求时，需要先创建一个CGI应用进程，并传入相应的参数，当该请求被返回时再销毁该进程。因此CGI原生是单一进程/请求的，特别是每次请求时产生的进程创建/销毁操作消耗了大量系统资源，根本无法满足较高负载的HTTP请求。此外，CGI进程模型还限制了数据库连接池、内存缓存等资源的复用。</p>

<p>对于标准CGI应用存在的单一进程问题，各大厂商分别提出了兼容CGI协议的解决方案，包括网景的NSAPI、微软的ISAPI和后来的Apache API（ASAPI）。上述服务器API的特点是既支持在服务器进程内运行CGI程序，也支持在独立进程中运行CGI程序，但通常需要在服务器进程中嵌入一个插件以支持该API。</p>

<h4><a href="http://ruby-doc.org/stdlib-2.2.2/libdoc/webrick/rdoc/WEBrick.html">Webrick</a></h4>

<p>作为最古老的Ruby Web服务器而不仅仅是一个接口，诞生于2000年的Webrick从Ruby 1.9.3（2011年10月正式发布）起被正式纳入标准库，成为Ruby的默认Web服务器API。Webrick支持HTTP/HTTPS、代理服务器、虚拟主机服务器，以及HTTP基础认证等RFC2617及以外的其它认证算法。同时，一个Webrick服务器还能由多个Webrick服务器或服务器小程序组合，提供类似虚拟主机或路由等功能：例如处理CGI脚本、ERb页面、Ruby块以及目录服务等。</p>

<p>Webrick曾被用于Rails核心团队的开发和测试中。但是，Webrick内置的HTTP Parser非常古老，文档缺失，性能低下且不易维护，功能单一且默认只支持单进程模式（但支持多线程，不过在Rails中默认关闭了对Webrick的多线程支持），根本无法满足产品环境中的并发和日常维护需求。目前一般只用于Web应用的本地开发和基准测试。</p>

<h4><a href="https://rubygems.org/gems/fcgi">fcgi.rb</a></h4>

<p>fcgi.rb是FastCGI协议的Ruby封装（latest版底层依赖libfcgi）。为了与当时的NSAPI竞争，FastCGI协议最初由Open Market提出和开发、并应用于自家Web服务器，延续了前者采用独立进程处理请求的做法：即维持一个FastCGI服务器。当Web服务器接收到HTTP请求时，请求内容和环境信息被通过Socket（本地）或TCP连接（远程）的方式传递至FastCGI服务器进行处理，再通过相反路径返回响应信息。分离进程的好处是Web服务器进程和FastCGI进程是永远保持的，只有相互之间的连接会被断开，避免了进程管理的开销。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/fastcgi.png" alt="Web服务器和FastCGI/SCGI服务器" /></p>

<p>进一步，FastCGI还支持同时响应多个请求。为了尽量减少资源浪费，若干请求可以复用同一个与Web服务器之间的连接，且支持扩展至多个FastCGI服务器进程。FastCGI降低了Web服务器和应用程序之间的耦合度，进而为解决安全、性能、管理等各方面问题提供新的思路，相比一些嵌入式方案如mod_perl和mod_php更具灵活性。</p>

<p>由于FastCGI协议的开放性，主流Web服务器产品基本都实现了各自的FastCGI插件，从而导致FastCGI方案被广泛使用。fcgi.rb最早开发于1998年，底层包含C和Ruby两种实现方式，早期曾被广泛应用于Rails应用的产品环境。</p>

<h4><a href="https://github.com/shugo/mod_ruby">mod_ruby</a></h4>

<p>mod_ruby是专门针对Apache HTTP Server的Ruby扩展插件，支持在Web服务器中直接运行Ruby CGI代码。由于mod_ruby在多个Apache进程中只能共享同一个Ruby解释器，意味着当同时运行多个Web应用（如Rails）时会发生冲突，存在安全隐患。因此只在一些简单部署环境下被采用，实际上并没有普及。</p>

<h4><a href="https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:other_ext_app">LiteSpeed API/RubyRunner</a></h4>

<p>LiteSpeed是由LiteSpeed Tech公司最初于2002年发布的商用Web服务器，特点是与被广泛采用的Apache Web服务器的配置文件兼容，但因为采用了事件驱动架构而具有更好的性能。</p>

<p>LiteSpeed API（LSAPI）是LiteSpeed专有的服务器API，LSAPI具备深度优化的IPC协议以提升通信性能。类似其它Web服务器，LiteSpeed支持运行CGI、FastCGI、以及后来的Mongrel。同时在LSAPI的基础上开发了Ruby接口模块，支持运行基于Ruby的Web应用。此外，LiteSpeed还提供RubyRunner插件，允许采用第三方Ruby解释器运行Ruby应用，但综合性能不如直接基于LSAPI Ruby。</p>

<p>由于LiteSpeed是收费产品，其普及率并不高，一般会考虑采用LiteSpeed作为Web服务器的业务场景包括虚拟主机/VPS提供商、以及相关业务的cPanel产品。同时，LiteSpeed也会被用于一些业务需求比较特殊的场合，例如对Web服务器性能要求高，且应用程序及其部署需要兼容Apache服务器。LiteSpeed于2013年发布了开源的轻量Web服务器——OpenLiteSpeed（GPL v3），移除了商业版本中偏具体业务的功能如cPanel等，更倾向于成为通用Web服务器。</p>

<h4><a href="https://rubygems.org/gems/scgi">scgi.rb</a></h4>

<p>scgi.rb是对SCGI协议的纯Ruby实现。从原理上来看，SCGI和FastCGI类似，二者的性能并无多大差别。但比起后者复杂的协议内容来说，SCGI移除了许多非必要的功能，看起来十分简洁，且实现复杂度更低。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/fastcgi_advance.png" alt="Web服务器和多FastCGI/SCGI服务器" /></p>

<p>与FastCGI类似，一个SCGI服务器可以动态创建服务器子进程用于处理更多请求（处理完毕将转入睡眠），直至达到配置的子进程上限。当获得Web服务器请求时，SCGI服务器进程会将其转发至子进程，并由子进程运行CGI程序处理该请求。此外，SCGI还能自动销毁退出和崩溃的子进程，具有良好的稳定性。</p>

<h3>二、闻名天下</h3>

<p>2005年，David Heinemeier Hansson（DHH）发布了基于Ruby的开发框架Ruby on Rails（Rails），聚光灯第一次聚焦在Ruby身上。但是业内普遍对Web服务器的方案感到棘手，本地环境Webrick/产品环境FastCGI＋通用Web服务器几乎成了标配，无论是开发、部署或维护都遇到不少困难，一些吃螃蟹的人遂把此视为Rails不如J2EE、PHP方案的证据。</p>

<h4><a href="https://rubygems.org/gems/mongrel">Mongrel</a></h4>

<p>2006年，Zed Shaw发布了划时代的Mongrel。Mongrel把自己定位成一个“应用服务器”，因为其不仅可以运行Ruby Web应用，也提供标准的HTTP接口，从而使Mongrel可以被放置在Web代理、Load Balancer等任意类型的转发器后面，而非像FastCGI、SCGI一样通过调用脚本实现Web服务器和CGI程序的通信。</p>

<p>Mongrel采用Ragel开发HTTP/1.1协议的Ruby parser，而后者是一个高性能有限自动机编译器，支持开发协议/数据parser、词法分析器和用户输入验证，支持编译成多种主流语言（包括Ruby）。采用Regel也使parser具有更好的可移植性。但是，Mongrel本身不支持任何应用程序框架，而需要由框架自身提供这种支持。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/mongrel.png" alt="Mongrel Web服务器" /></p>

<p>Mongrel支持多线程运行（但对于当时非线程安全的Rails来说，仍然只能采用多进程的方式提高一部分并发能力），曾被Twitter作为其第一代Web服务器，还启发了Ryan Dahl发布于2009年的Node.JS。</p>

<p>但是当Mongrel发布后没过多久，Shaw就与Rails社区的核心成员不和（实际上Shaw对业界的许多技术和公司都表达过不满），随后就终止了Mongrel的开发。进而在其Parser的基础上开发了其后续——语言无关的Web服务器Mongrel2（与前续毫无关系）。</p>

<p>尽管Mongrel迅速衰落，却成功启发了随后更多优秀Ruby应用服务器的诞生，例如后文将介绍的Thin、Unicorn和Puma。</p>

<h4><a href="https://rubygems.org/gems/rack">Rack</a></h4>

<p>随着Web服务器接口技术的发展，从开始时作为一个module嵌入Web服务器，到维护独立的应用服务器进程，越来越多的应用服务器产品开始涌现，同时相互之间还产生了差异化以便适应不同的应用场景。但是，由于底层协议和API的差别，基于不同的应用服务器开发Web产品时，意味着要实现各自的通信接口，从而为Web应用开发带来更多工作量。特别是对于类似Django、Rails这些被广泛使用的Web框架来说，兼容主流应用服务器几乎是必须的。</p>

<p>2003年，Python界权威Phillip J. Eby发表了PEP 0333（Python Web Server Gateway Interface v1.0，即WSGI），提出一种Web服务器和应用程序之间的统一接口，该接口封装了包括CGI、FastCGI、mod_python等主流方案的API，使遵循WSGI的Python Web应用能够直接部署在各类Web服务器上。与Python的发展轨迹相似，Ruby界也遇到了类似的挑战，并最终在2007年出现了与WSGI类似的Rack。</p>

<p>与WSGI最初只作为一份建议不同，Rack直接提供了模块化的框架实现，并由于良好的设计架构迅速统一了Ruby Web服务器和应用程序框架接口。</p>

<p>Rack被设计成一种中间件“框架”，接收到的HTTP请求会被rack放入不同的管线（中间件）进行处理，直到从应用程序获取响应。这种设计通过统一接口，把一般Web应用所需的底层依赖，包括Session处理、数据库操作、请求处理、渲染视图、路由/调度、以及表单处理等组件以中间件的形式“放入”rack的中间件管线中，并在HTTP请求/响应发生时依次通过上述管线传递至应用程序，从而实现Web应用程序对底层通信依赖的解绑。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/rack.png" alt="Rack中间件" /></p>

<p>Rack接口部分包含两类组件：Handler，用于和Web服务器通信；Adapter，用于和应用程序通信。截至Rack 1.6，Rack内置的handlers包括WEBrick、FCGI、CGI、SCGI、LiteSpeed以及Thin，上述handlers用以兼容已有的常见应用服务器。而2008年后，随着rack逐渐成为事实标准，更新的Ruby Web服务器几乎都包含Rack提供的handler。包括Rails、Sinatra、Merb等等几乎所有主流框架都引入了Rack Adapters的支持。</p>

<h3>三、百花齐放</h3>

<p>Mongrel和Rack的相继诞生，使Ruby Web服务器、乃至应用程序框架的发展有了一定意义上可以遵循的标准。Mongrel后相继派生出Thin、Unicorn和Puma；而Rack统一了Ruby Web服务器和应用程序框架接口，使应用开发不再需要考虑特定的部署平台。Ruby Web服务器开始依据特定需求深入发展。</p>

<h4><a href="https://rubygems.org/gems/thin">Thin</a>/<a href="https://rubygems.org/gems/goliath">Goliath</a></h4>

<p>发布于2009年的Thin沿用了Mongrel的Parser，基于Rack和EventMachine开发，前者上文已有介绍，EventMachine是一个Ruby编写的、基于Reactor模式的轻量级事件驱动I/O（类似JBoss Netty、Apache MINA、Python Twisted、Node.js、libevent和libev等）和并发库，使Thin能够在面对慢客户端的同时支持高并发请求。</p>

<p>发表自1995年的Reactor模型的基本原理是采用一个单线程事件循环缓存所有系统事件，当事件发生时，以同步方式将该事件发送至处理模块，处理完成后返回结果。基于Reactor模型的EventMachine具备异步（非阻塞）I/O的能力，被广泛用于大部分基于Ruby的事件驱动服务器、异步客户端、网络代理以及监控工具中。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/reactor.png" alt="Reactor模型" /></p>

<p>2011年，社交网络分析商PostRank开源了其Web服务器Goliath，与Thin相似（都采用了EventMachine）但又有很大不同，采用新的HTTP Parser，同时针对异步事件编程中的高复杂度回调函数问题，借助Ruby1.9+的纤程技术实现了线性编码，使程序具备更好的可维护性。Goliath支持MRI、JRuby和Rubinius等多平台。在附加功能方面，Goliath的目标不仅是作为Web服务器，更是一个快速构建WebServices/APIs的开发框架，但是随着之后PostRank被Google收购，Goliath项目也就不再活跃在开源界了。</p>

<h4><a href="https://rubygems.org/gems/unicorn">Unicorn</a></h4>

<p>2009年，Eric Wong在Mongrel 1.1.5版本的基础上开发了Unicorn。Unicorn是一个基于Unix/类Unix操作系统的、面向快客户端、低延迟和高带宽场景的Rack服务器，基于上述限制，任何情况下几乎都需要在Unicorn和客户端之间设置一个反向代理缓存请求和响应数据，这是Unicorn的设计特点所决定的，但也使得Unicorn的内部实现相对简洁、可靠。</p>

<p>尽管来源于Mongrel，但Unicorn只在进程级运行，且吸收和利用了一些Unix/类Unix系统内核的特性，如Prefork模型。</p>

<p>Unicorn由1个master进程和n个fork(2)子进程组成，子进程分别调用select(2)阻塞自己，直到出错或者超时时，才做一些写日志、处理信号以及维护与master的心跳链接等内置任务。子进程和master间通过一个共享socket实现通信，而由Unix/类Unix系统内核自身处理资源调度。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/unicorn.png" alt="Unicorn的多进程模型" /></p>

<p>Unicorn的设计理念是“只专注一件事”：多进程阻塞I/O的方式令其无从接受慢客户端——但前置反向代理能解决这一问题；workers的负载均衡就直接交给操作系统处理。这种理念大大降低了实现复杂度，从而提高了自身可靠性。此外，类似Nginx的重加载机制，Unicorn也支持零宕机重新加载配置文件，使其允许在线部署Web应用而不用产生离线成本。</p>

<h4><a href="https://rubygems.org/gems/passenger">Phusion Passenger（mod_rails/mod_rack）</a></h4>

<p>2008年初，一位叫赖洪礼的Ruby神童发布了mod_rails。尽管Mongrel在当时已经席卷Rails的Web服务器市场，但是面对部署共享主机或是集群的情况时还是缺少统一有效的解决方案，引起业内一些抱怨，包括DHH（也许Shaw就不认为这是个事儿）。</p>

<p>mod_rails最初被设计成一个Apache的module，与FastCGI的原理类似，但设置起来异常简单——只需要设置一个RailsBaseURI匹配转发至Rails服务器的URI串。mod_rails服务器会在启动时自动加载Web应用程序，然后按需创建子进程，并协调Web服务器和Rails服务器的通信，从而支持单一服务器同时部署多个应用，还允许按需自动重启应用服务器。</p>

<p>mod_rails遵循了Rails的设计原则，包括Convention over Configuration、Don&rsquo;t Repeat Yourself，使其面向部署非常友好，很快得到了业界青睐，并在正式release时改名Passenger。</p>

<p>在随后的发展中，Passenger逐渐成为独立的Ruby应用服务器、支持多平台的Web服务器。截至2015年6月，Phusion Passenger的版本号已经达到5.0.10（Raptor），核心采用C++编写，同时支持Ruby、Python和Node.js应用。支持Apache、Nginx和独立HTTP模式（推荐采用独立模式），支持Unix/类Unix系统，在统计网站Builtwith上排名Ruby Web服务器使用率第一。</p>

<p>值得一提的是，Phusion Passenger的开源版本支持多进程模式，但是其企业版同样支持多线程运行。本文撰写时，Phusion Passenger是最近一个号称“史上最快”的Ruby Web服务器（本文最后将进一步介绍Raptor）。</p>

<h4><a href="https://rubygems.org/gems/trinidad">Trinidad</a>/<a href="https://rubygems.org/gems/torquebox">TorqueBox</a></h4>

<p>Trinidad发布于2009年，基于JRuby::Rack和Apache Tomcat，使Rails的部署和世界上最流行的Web服务器之一Tomcat结合，支持集成Java代码、支持多线程的Resque和Delayed::Job等Worker，也支持除Tomcat以外的其它Servlet容器。</p>

<p>与Trinidad相比，同样发布于2009年的TorqueBox不仅仅是一个Web服务器，而且被设计成一个可移植的Ruby平台。基于JRuby::Rack和WildFly（JBoss AS），支持多线程阻塞I/O，内置对消息、调度、缓存和后台进程的支持。同时具有集群、负载均衡、高可用等多种附加功能。</p>

<h4><a href="https://rubygems.org/gems/puma">Puma</a></h4>

<p>Puma——Mongrel最年轻的后代于2011年发布，作者是Evan Phoenix。</p>

<p>由于Mongrel诞生于前Rack时期，而随着Rack统一了Web服务器接口，任何基于Rack的应用再与Mongrel配合就有许多不便。Puma继承了前者的Parser，并且基于Rack重写了底层通信部分。更重要的是，Puma部分依赖Ruby的其它两个流行实现：Rubinius和JRuby，与TorqueBox类似拥有多线程阻塞I/O的能力（MRI平台不支持真正意义上的多线程，但Puma依然具有良好并发能力），支持高并发。同时Puma还包含了一个事件I/O模块以缓冲HTTP请求，以降低慢客户端的影响。但是，从获得更高吞吐量的角度来说，Puma目前仍然需要采用Rubinius和JRuby这两个平台。</p>

<h4><a href="https://rubygems.org/gems/reel">Reel</a></h4>

<p>Reel是最初由Tony Arcieri发布于2012年的采用事件I/O的Web服务器。采用了不同于Eventmachine的Celluloid::IO，后者基于Celluloid——Actor并发模型的Ruby实现库，解决了EM只能在单一线程中运行事件循环程序的问题，从而同时支持多线程＋事件I/O，在非阻塞I/O和多线程方案间实现了良好的融合。</p>

<p>与其它现代Ruby Web服务器不同的是，Reel并不是基于Rack创建，但通过Reel::Rack提供支持Rack的Adapter。尽管支持Rails，与Puma也有一定的相似性，但与Unicorn、Puma和Raptor相比，Reel在部署Rails/Rack应用方面缺少易用性。实际上基于Celluloid本身的普及程度和擅长领域，相比其它Web服务器而言，Reel更适合部署WebSocket/Stream应用。</p>

<h4><a href="https://rubygems.org/gems/yahns">Yahns</a></h4>

<p>2013年，Eric Wong等人受Kqueue（源自FreeBSD，同时被Node.js作为基础事件I/O库）的启发启动了Yahns项目。其目标与Reel类似，同样是在非阻塞I/O设计中引入多线程。与Reel不同的是，Yahns原生支持Rack/HTTP应用。</p>

<p>Yahns被设计成具有良好的伸缩性和轻量化特性，当系统应用访问量较低或为零时，Yahns本身的资源消耗也会保持在较低水平。此外，yahns只支持GNU/Linux（并通过kqueue支持FreeBSD），并声称永远不会支持类似Unicorn或Passenger里的Watchdog技术，不会因为应用崩溃而自动销毁和创建进程/线程，因此对应用程序本身的可靠性有一定要求。</p>

<h3>四、迈向未来</h3>

<p>回顾过去，Ruby Web服务器在发展中先后解决了缺少部署方案、与Web应用程序不兼容、运维管理困难等问题，基础架构趋于成熟且稳定。而随着更多基准测试结果的出现，业界逐渐开始朝着更高性能和并发量发展，同时针对HTTP协议本身的优化和扩展引入的HTTP/2，以及HTML5的WebSocket/Stream等需求均成为未来Ruby Web服务器发展的方向。</p>

<h4>高吞吐量</h4>

<p>以最新的Raptor（上文提到的Phusion Passenger 5）为例，其在网络I/O模型的选择上融合了现有其它优秀产品的方案，包括Unicorn的多进程模型、内置基于多线程和事件I/O模型的反向代理缓冲（类似Nginx的功能，但对Raptor自身做了大量裁减和优化）、以及企业版具有的多线程模型（类似Puma和TorqueBox）；此外，Raptor采用的Node.js HTTP Parser（基于Nginx的Parser）的性能超过了Mongrel；更进一步，Raptor甚至实现了Zero-copy和一般在大型游戏中使用的区域内存管理技术，使其对CPU和内存访问的优化达到了极致（感兴趣的话可以进一步查阅<a href="http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/">这篇文章</a>）。</p>

<p><img src="http://7xk84n.com1.z0.glb.clouddn.com/ruby_web_servers/raptor.png" alt="Raptor的优化模型" /></p>

<p>另外也需要看到，当引入多线程运行方式，现有Web应用将不得不仔细检查自身及其依赖，是否是线程安全的，同时这也给构建Ruby Web应用带来更多新的挑战。这也是为什么更多人宁愿选择进程级应用服务器的方式——毕竟对大多数应用来说不需要用到太多横向扩展，引入反向代理即可解决慢客户端的问题，而采用Raptor甚至在独立模式能工作的更好（这样就不用花时间去学习Nginx）。</p>

<p>除非你已经开始考虑向支持大规模并发的架构迁移，并希望节省接下来的一大笔花费了。</p>

<h4>HTTP/2</h4>

<p>2015年5月，HTTP/2随着RFC7540正式发布。如今各主流服务器/浏览器厂商正在逐渐完成从HTTP/2测试模块到正式版本的过渡。而截至目前，主流Ruby Web服务器都还没有公开HTTP/2的开发信息。<a href="https://rubygems.org/gems/http-2">HTTP-2</a>是在2013年由Ilya Grigorik发布的纯Ruby的HTTP/2协议实现，包括二进制帧的解析与编码、流传输的多路复用和优先级制定、连接和流传输的流量控制、报头压缩与服务器推送、连接和流传输管理等功能。随着HTTP/2的发布和普及，主流Ruby Web服务器将不可避免的引入对HTTP/2的支持。</p>

<h4>WebSocket/流（Stream）/服务器推送事件（Server Sent Events，SSE）</h4>

<p>2011年，RFC6455正式公布了WebSocket协议。WebSocket用于在一个TCP链接上实现全双工通信，其目的是实现客户端与服务器之间更高频次的交互，以完成实时互动业务。鉴于该特点，仅支持慢客户端的Web服务器就无法有效支撑WebSocket的并发需求，更何况后者对并发量更加严苛的要求了。而对于同样需要长连接的流服务器和服务器推送事件服务（SSE），都避免不了对长连接和高并发量的需求。尽管高性能的Ruby Web服务器都有足够的潜力完成这些任务，但是从原生设计的角度来看，更加年轻的Reel和Yahns无疑具有优势。</p>

<p>最近Planet ruby在Ruby邮件组发布了<a href="https://github.com/planetruby/awesome-webservers">Awesome Webservers</a>，该Github Repo旨在对主流Ruby Web服务器进行总结和对比，并且保持持续更新，推荐开发者关注。</p>

<p>本文灵感来源于RailsConf 2015的Session：&#8221;Riding Rails for 10 Years - John Duff&#8221;。</p>

<h3>参考资源</h3>

<p><a href="http://www.fastcgi.com/devkit/doc/fcgi-spec.html">http://www.fastcgi.com/devkit/doc/fcgi-spec.html</a></p>

<p><a href="https://github.com/rails/rails/blob/master/railties/lib/rails/commands/server.rb#L82">https://github.com/rails/rails/blob/master/railties/lib/rails/commands/server.rb#L82</a></p>

<p><a href="https://github.com/shugo/mod_ruby">https://github.com/shugo/mod_ruby</a></p>

<p><a href="http://www.rubyinside.com/no-true-mod_ruby-is-damaging-rubys-viability-on-the-web-693.html">http://www.rubyinside.com/no-true-mod_ruby-is-damaging-rubys-viability-on-the-web-693.html</a></p>

<p><a href="http://confreaks.tv/videos/railsconf2015-riding-rails-for-10-years">http://confreaks.tv/videos/railsconf2015-riding-rails-for-10-years</a></p>

<p><a href="http://kovyrin.net/2006/08/28/ruby-performance-results/lang/en/">http://kovyrin.net/2006/08/28/ruby-performance-results/lang/en/</a></p>

<p><a href="http://chneukirchen.org/blog/archive/2007/02/introducing-rack.html">http://chneukirchen.org/blog/archive/2007/02/introducing-rack.html</a></p>

<p><a href="https://www.wiredtree.com/blog/litespeed-replacement-for-apache/">https://www.wiredtree.com/blog/litespeed-replacement-for-apache/</a></p>

<p><a href="https://utcc.utoronto.ca/~cks/space/blog/programming/SCGIvsFastCGI">https://utcc.utoronto.ca/~cks/space/blog/programming/SCGIvsFastCGI</a></p>

<p><a href="https://www.litespeedtech.com/support/wiki/doku.php?id=litespeed_wiki:ruby_rails">https://www.litespeedtech.com/support/wiki/doku.php?id=litespeed_wiki:ruby_rails</a></p>

<p><a href="https://www.colm.net/open-source/ragel/">https://www.colm.net/open-source/ragel/</a></p>

<p><a href="http://zedshaw.com/archive/ragel-state-charts/">http://zedshaw.com/archive/ragel-state-charts/</a></p>

<p><a href="https://en.wikipedia.org/wiki/Comparison_of_parser_generators">https://en.wikipedia.org/wiki/Comparison_of_parser_generators</a></p>

<p><a href="https://blog.twitter.com/2010/unicorn-power">https://blog.twitter.com/2010/unicorn-power</a></p>

<p><a href="http://david.heinemeierhansson.com/posts/31-myth-2-rails-is-expected-to-crash-400-timesday">http://david.heinemeierhansson.com/posts/31-myth-2-rails-is-expected-to-crash-400-timesday</a></p>

<p><a href="http://techcrunch.com/2008/01/01/zed-shaw-puts-the-smack-down-on-the-rails-community/">http://techcrunch.com/2008/01/01/zed-shaw-puts-the-smack-down-on-the-rails-community/</a></p>

<p><a href="http://code.macournoyer.com/thin/">http://code.macournoyer.com/thin/</a></p>

<p><a href="http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf">http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf</a></p>

<p><a href="http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf">http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf</a></p>

<p><a href="http://www.infoq.com/articles/meet-goliath">http://www.infoq.com/articles/meet-goliath</a></p>

<p><a href="http://2ndscale.com/rtomayko/2009/unicorn-is-unix">http://2ndscale.com/rtomayko/2009/unicorn-is-unix</a></p>

<p><a href="http://unicorn.bogomips.org/SIGNALS.html">http://unicorn.bogomips.org/SIGNALS.html</a></p>

<p><a href="http://nginx.org/en/docs/control.html">http://nginx.org/en/docs/control.html</a></p>

<p><a href="http://izumi.plan99.net/blog/index.php/2008/01/14/what-is-so-hard-about-rails-deployment/">http://izumi.plan99.net/blog/index.php/2008/01/14/what-is-so-hard-about-rails-deployment/</a></p>

<p><a href="https://blog.phusion.nl/">https://blog.phusion.nl/</a></p>

<p><a href="https://trends.builtwith.com/web-server">https://trends.builtwith.com/web-server</a></p>

<p><a href="https://github.com/puma/puma">https://github.com/puma/puma</a></p>

<p><a href="http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/">http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/</a></p>

<p><a href="http://ohcoder.com/blog/2014/11/11/raptor-part-1/">http://ohcoder.com/blog/2014/11/11/raptor-part-1/</a></p>

<p><a href="https://github.com/celluloid/reel">https://github.com/celluloid/reel</a></p>

<p><a href="http://yahns.yhbt.net/README">http://yahns.yhbt.net/README</a></p>

<p><a href="https://en.wikipedia.org/wiki/HTTP/2">https://en.wikipedia.org/wiki/HTTP/2</a></p>

<p><a href="https://github.com/igrigorik/http-2">https://github.com/igrigorik/http-2</a></p>

<p><a href="https://github.com/celluloid/reel/wiki/Frequently-Asked-Questions">https://github.com/celluloid/reel/wiki/Frequently-Asked-Questions</a></p>

<p><a href="https://en.wikipedia.org/wiki/WebSocket">https://en.wikipedia.org/wiki/WebSocket</a></p>

<p><a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming</a></p>
]]></content>
  </entry>
  
</feed>
