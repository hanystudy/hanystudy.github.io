<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 天选之作 | Wing of Dream 梦境之翼]]></title>
  <link href="http://www.hanyi.name/blog/categories/tian-xuan-zhi-zuo/atom.xml" rel="self"/>
  <link href="http://www.hanyi.name/"/>
  <updated>2015-07-07T02:45:44+08:00</updated>
  <id>http://www.hanyi.name/</id>
  <author>
    <name><![CDATA[Han Yi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ruby Web服务器：这十五年]]></title>
    <link href="http://www.hanyi.name/blog/2015/06/28/ruby-webserver-last-10-years/"/>
    <updated>2015-06-28T16:39:54+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/06/28/ruby-webserver-last-10-years</id>
    <content type="html"><![CDATA[<p>坦率的说，作为一门年轻的计算机语言，Ruby在最近二十年里的发展并不算慢。但如果与坐拥豪门的明星语言们相比，Ruby就颇显平民范儿，表现始终不温不火，批评胜于褒奖，下行多过上扬。但总有一些至少曾经自称过Rubyist的程序员们，愉快地实践了这门语言，他们没有丝毫的歧视习惯，总是努力尝试各家之长，以语言表达思想，用基准评判高下，一不小心就影响了整个技术发展的进程。</p>

<p>本文谨以Ruby Web服务器技术的发展为线索，回顾Ruby截至目前最为人所知的Web领域中，重要性数一数二的服务器技术的发展历程，试图帮助我们了解过去，预见未来。</p>

<h3>一、随波逐流</h3>

<p>长久以来，任何Web服务器都具备的两项最重要的功能：一是根据RFC2616解析HTTP/1.1协议，二是接收、处理并响应客户端的HTTP请求。幸运的是Web技术的发展并不算太早，使得Ruby恰好能赶上这趟顺风车，但在前期也基本上受限于整个业界的进展。像Apache HTTP Server、Lighttpd和Nginx这些通用型Web服务器＋合适的Web服务器接口即可完成大部分工作，而当时开发者的重心则是放在接口实现上。</p>

<h4>cgi.rb</h4>

<p>作为Web服务器接口的早期标准，CGI程序在调用过程中，通过环境变量（GET）或$stdin（POST）传递参数，然后将结果返回至$stdout，从而完成Web服务器和应用程序之间的通信。cgi.rb是Ruby官方的CGI协议标准库，发布于2000年的cgi.rb包含HTTP参数获取、Cookie/Session管理、以及生成HTML内容等基本功能。</p>

<p>当支持CGI应用的Web服务器接到HTTP请求时，需要先创建一个CGI应用进程，并传入相应的参数，当该请求被返回时再销毁该进程。因此CGI原生是单一进程/请求的，特别是每次请求时产生的进程创建/销毁操作消耗了大量系统资源，根本无法满足较高负载的HTTP请求。此外，CGI进程模型还限制了数据库连接池、内存缓存等资源的复用。</p>

<p>对于标准CGI应用存在的单一进程问题，各大厂商分别提出了兼容CGI协议的解决方案，包括网景的NSAPI、微软的ISAPI和后来的Apache API（ASAPI）。上述服务器API的特点是既支持在服务器进程内运行CGI程序，也支持在独立进程中运行CGI程序，但通常需要在服务器进程中嵌入一个插件以支持该API。</p>

<h4>Webrick</h4>

<p>作为最古老的Ruby Web服务器而不仅仅是一个接口，诞生于2000年的Webrick从Ruby 1.9.3（2011年10月正式发布）起被正式纳入标准库，成为Ruby的默认Web服务器API。Webrick支持HTTP/HTTPS、代理服务器、虚拟主机服务器，以及HTTP基础认证等RFC2617及以外的其它认证算法。同时，一个Webrick服务器还能由多个Webrick服务器或服务器小程序组合，提供类似虚拟主机或路由等功能：例如处理CGI脚本、ERb页面、Ruby块以及目录服务等。</p>

<p>Webrick曾被用于Rails核心团队的开发和测试中。但是，Webrick内置的HTTP Parser非常古老，文档缺失，性能低下且不易维护，功能单一且默认只支持单进程模式（但支持多线程，不过在Rails中默认关闭了对Webrick的多线程支持），根本无法满足产品环境中的并发和日常维护需求。目前一般只用于Web应用的本地开发和基准测试。</p>

<h4>fcgi.rb</h4>

<p>fcgi.rb是FastCGI协议的Ruby封装（latest版底层依赖libfcgi）。为了与当时的NSAPI竞争，FastCGI协议最初由Open Market提出和开发、并应用于自家Web服务器，延续了前者采用独立进程处理请求的做法：即维持一个FastCGI服务器。当Web服务器接收到HTTP请求时，请求内容和环境信息被通过Socket（本地）或TCP连接（远程）的方式传递至FastCGI服务器进行处理，再通过相反路径返回响应信息。分离进程的好处是Web服务器进程和FastCGI进程是永远保持的，只有相互之间的连接会被断开，避免了进程管理的开销。</p>

<p>进一步，FastCGI还支持同时响应多个请求。为了尽量减少资源浪费，若干请求可以复用同一个与Web服务器之间的连接，且支持扩展至多个FastCGI服务器进程。FastCGI降低了Web服务器和应用程序之间的耦合度，进而为解决安全、性能、管理等各方面问题提供新的思路，相比一些嵌入式方案如mod_perl和mod_php更具灵活性。</p>

<p>由于FastCGI协议的开放性，主流Web服务器产品基本都实现了各自的FastCGI插件，从而导致FastCGI方案被广泛使用。fcgi.rb最早开发于1998年，底层包含C和Ruby两种实现方式，早期曾被广泛应用于Rails应用的产品环境。</p>

<h4>mod_ruby</h4>

<p>mod_ruby是专门针对Apache HTTP Server的Ruby扩展插件，支持在Web服务器中直接运行Ruby CGI代码。由于mod_ruby在多个Apache进程中只能共享同一个Ruby解释器，意味着当同时运行多个Web应用（如Rails）时会发生冲突，存在安全隐患。因此只在一些简单部署环境下被采用，实际上并没有普及。</p>

<h4>LiteSpeed API/RubyRunner</h4>

<p>LiteSpeed是由LiteSpeed Tech公司最初于2002年发布的商用Web服务器，特点是与被广泛采用的Apache Web服务器的配置文件兼容，但因为采用了事件驱动架构而具有更好的性能。</p>

<p>LiteSpeed API（LSAPI）是LiteSpeed专有的服务器API，LSAPI具备深度优化的IPC协议以提升通信性能。类似其它Web服务器，LiteSpeed支持运行CGI、FastCGI、以及后来的Mongrel。同时在LSAPI的基础上开发了Ruby接口模块，支持运行基于Ruby的Web应用。此外，LiteSpeed还提供RubyRunner插件，允许采用第三方Ruby解释器运行Ruby应用，但综合性能不如直接基于LSAPI Ruby。</p>

<p>由于LiteSpeed是收费产品，其普及率并不高，一般会考虑采用LiteSpeed作为Web服务器的业务场景包括虚拟主机/VPS提供商、以及相关业务的cPanel产品。同时，LiteSpeed也会被用于一些业务需求比较特殊的场合，例如对Web服务器性能要求高，且应用程序及其部署需要兼容Apache服务器。LiteSpeed于2013年发布了开源的轻量Web服务器——OpenLiteSpeed（GPL v3），移除了商业版本中偏具体业务的功能如cPanel等，更倾向于成为通用Web服务器。</p>

<h4>scgi.rb</h4>

<p>scgi.rb是对SCGI协议的纯Ruby实现。从原理上来看，SCGI和FastCGI类似，二者的性能并无多大差别。但比起后者复杂的协议内容来说，SCGI移除了许多非必要的功能，看起来十分简洁，且实现复杂度更低。</p>

<p>在SCGI协议中，一个SCGI服务器可以动态创建服务器子进程用于处理更多请求（处理完毕将转入睡眠），直至达到配置的子进程上限。当获得Web服务器请求时，SCGI服务器进程会将其转发至子进程，并由子进程运行CGI程序处理该请求，从而使SCGI与FastCGI相比拥有更好的并发处理能力。此外，SCGI还能自动销毁退出和崩溃的子进程，具有良好的稳定性。</p>

<h3>二、闻名天下</h3>

<p>2005年，David Heinemeier Hansson（DHH）发布了基于Ruby的开发框架Ruby on Rails（Rails），聚光灯第一次聚焦在Ruby身上。但是业内普遍对Web服务器的方案感到棘手，本地环境Webrick/产品环境FastCGI＋通用Web服务器几乎成了标配，无论是开发、部署或维护都遇到不少困难，一些吃螃蟹的人遂把此视为Rails不如J2EE、PHP方案的证据。</p>

<h4>Mongrel</h4>

<p>2006年，Zed Shaw发布了划时代的Mongrel。Mongrel把自己定位成一个“应用服务器”，因为其不仅可以运行Ruby Web应用，也提供标准的HTTP接口，从而使Mongrel可以被放置在Web代理、Load Balancer等任意类型的转发器后面，而非像FastCGI、SCGI一样通过调用脚本实现Web服务器和CGI程序的通信。</p>

<p>Mongrel采用Ragel开发HTTP/1.1协议的Ruby parser，而后者是一个高性能有限自动机编译器，支持开发协议/数据parser、词法分析器和用户输入验证，支持编译成多种主流语言（包括Ruby）。采用Regel也使parser具有更好的可移植性。但是，Mongrel本身不支持任何应用程序框架，而需要由框架自身提供这种支持。</p>

<p>Mongrel支持多线程运行（但对于当时非线程安全的Rails来说，仍然只能采用多进程的方式提高一部分并发能力），曾被Twitter作为其第一代Web服务器，还启发了Ryan Dahl发布于2009年的Node.JS。</p>

<p>但是当Mongrel发布后没过多久，Shaw就与Rails社区的核心成员不和（实际上Shaw对业界的许多技术和公司都表达过不满），随后就终止了Mongrel的开发。进而在其Parser的基础上开发了其后续——语言无关的Web服务器Mongrel2（与前续毫无关系）。</p>

<p>尽管Mongrel迅速衰落，却成功启发了随后更多优秀Ruby应用服务器的诞生，例如后文将介绍的Thin、Unicorn和Puma。</p>

<h4>Rack</h4>

<p>随着Web服务器接口技术的发展，从开始时作为一个module嵌入Web服务器，到维护独立的应用服务器进程，越来越多的应用服务器产品开始涌现，同时相互之间还产生了差异化以便适应不同的应用场景。但是，由于底层协议和API的差别，基于不同的应用服务器开发Web产品时，意味着要实现各自的通信接口，从而为Web应用开发带来更多工作量。特别是对于类似Django、Rails这些被广泛使用的Web框架来说，兼容主流应用服务器几乎是必须的。</p>

<p>2003年，Python界权威Phillip J. Eby发表了PEP 0333（Python Web Server Gateway Interface v1.0，即WSGI），提出一种Web服务器和应用程序之间的统一接口，该接口封装了包括CGI、FastCGI、mod_python等主流方案的API，使遵循WSGI的Python Web应用能够直接部署在各类Web服务器上。与Python的发展轨迹相似，Ruby界也遇到了类似的挑战，并最终在2007年出现了与WSGI类似的Rack。</p>

<p>与WSGI最初只作为一份建议不同，Rack直接提供了模块化的框架实现，并由于良好的设计架构迅速统一了Ruby Web服务器和应用程序框架接口。</p>

<p>Rack被设计成一种中间件“框架”，接收到的HTTP请求会被rack放入不同的管线（中间件）进行处理，直到从应用程序获取响应。这种设计通过统一接口，把一般Web应用所需的底层依赖，包括Session处理、数据库操作、请求处理、渲染视图、路由/调度、以及表单处理等组件以中间件的形式“放入”rack的中间件管线中，并在HTTP请求/响应发生时依次通过上述管线传递至应用程序，从而实现Web应用程序对底层通信依赖的解绑。</p>

<p>Rack接口部分包含两类组件：Handler，用于和Web服务器通信；Adapter，用于和应用程序通信。截至Rack 1.6，Rack内置的handlers包括WEBrick、FCGI、CGI、SCGI、LiteSpeed以及Thin，上述handlers用以兼容已有的常见应用服务器。而2008年后，随着rack逐渐成为事实标准，更新的Ruby Web服务器几乎都包含Rack提供的handler。包括Rails、Sinatra、Merb等等几乎所有主流框架都引入了Rack Adapters的支持。</p>

<h3>三、百花齐放</h3>

<p>Mongrel和Rack的相继诞生，使Ruby Web服务器、乃至应用程序框架的发展有了一定意义上可以遵循的标准。Mongrel后相继派生出Thin、Unicorn和Puma；而Rack统一了Ruby Web服务器和应用程序框架接口，使应用开发不再需要考虑特定的部署平台。Ruby Web服务器开始依据特定需求深入发展。</p>

<h4>Thin/Goliath</h4>

<p>发布于2009年的Thin沿用了Mongrel的Parser，基于Rack和EventMachine开发，前者上文已有介绍，EventMachine是一个Ruby编写的、基于Reactor模式的轻量级事件驱动I/O（类似JBoss Netty、Apache MINA、Python Twisted、Node.js、libevent和libev等）和并发库，使Thin能够在面对慢客户端的同时支持高并发请求。</p>

<p>发表自1995年的Reactor模型的基本原理是采用一个单线程事件循环缓存所有系统事件，当事件发生时，以同步方式将该事件发送至处理模块，处理完成后返回结果。基于Reactor模型的EventMachine具备异步（非阻塞）I/O的能力，被广泛用于大部分基于Ruby的事件驱动服务器、异步客户端、网络代理以及监控工具中。</p>

<p>2011年，社交网络分析商PostRank开源了其Web服务器Goliath，与Thin相似（都采用了EventMachine）但又有很大不同，采用新的HTTP Parser，同时针对异步事件编程中的高复杂度回调函数问题，借助Ruby1.9+的纤程技术实现了线性编码，使程序具备更好的可维护性。Goliath支持MRI、JRuby和Rubinius等多平台。在附加功能方面，Goliath的目标不仅是作为Web服务器，更是一个快速构建WebServices/APIs的开发框架，但是随着之后PostRank被Google收购，Goliath项目也就不再活跃在开源界了。</p>

<h4>Unicorn</h4>

<p>2009年，Eric Wong在Mongrel 1.1.5版本的基础上开发了Unicorn。Unicorn是一个基于Unix/类Unix操作系统的、面向快客户端、低延迟和高带宽场景的Rack服务器，基于上述限制，任何情况下几乎都需要在Unicorn和客户端之间设置一个反向代理缓存请求和响应数据，这是Unicorn的设计特点所决定的，但也给使得Unicorn具有简洁，可靠的特性。</p>

<p>尽管来源于Mongrel，但Unicorn只在进程级运行，且吸收和利用了一些Unix/类Unix系统内核的特性，如Prefork模型。</p>

<p>Unicorn由1个master进程和n个fork(2)子进程组成，子进程分别调用select(2)阻塞自己，直到出错或者超时时，才做一些写日志、处理信号以及维护与master的心跳链接等内置任务。子进程和master间通过一个共享socket实现通信，而由Unix/类Unix系统内核自身处理资源调度。</p>

<p>Unicorn的设计理念是“只专注一件事”：多进程阻塞I/O的方式令其无从接受慢客户端——但前置反向代理能解决这一问题；workers的负载均衡就直接交给操作系统处理。这种理念大大降低了实现复杂度，从而提高了自身可靠性。此外，类似Nginx的重加载机制，Unicorn也支持零宕机重新加载配置文件，使其允许在线部署Web应用等特性。</p>

<h4>Phusion Passenger（mod_rails/mod_rack）</h4>

<p>2008年初，一位叫赖洪礼的Ruby神童发布了mod_rails。尽管Mongrel在当时已经席卷Rails的Web服务器市场，但是面对部署共享主机或是集群的情况时还是缺少统一有效的解决方案，引起业内一些抱怨，包括DHH（也许Shaw就不认为这是个事儿）。</p>

<p>mod_rails最初被设计成一个Apache的module，与FastCGI的原理类似，但设置起来异常简单——只需要设置一个RailsBaseURI匹配转发至Rails服务器的URI串。mod_rails服务器会在启动时自动加载Web应用程序，然后按需创建子进程，并协调Web服务器和Rails服务器的通信，从而支持单一服务器同时部署多个应用，还允许按需自动重启应用服务器。</p>

<p>mod_rails遵循了Rails的设计原则，包括Convention over Configuration、Don&rsquo;t Repeat Yourself，使其面向部署非常友好，很快得到了业界青睐，并在正式release时改名Passenger。</p>

<p>在随后的发展中，Passenger逐渐成为独立的Ruby应用服务器、支持多平台的Web服务器。截至2015年6月，Phusion Passenger的版本号已经达到5.0.10（Raptor），核心采用C++编写，同时支持Ruby、Python和Node.js应用。支持Apache、Nginx和独立HTTP模式（建议采用独立模式），支持Unix/类Unix系统，在统计网站Builtwith上排名Ruby Web服务器使用率第一。</p>

<p>值得一提的是，Phusion Passenger的开源版本支持多进程模式，但是其企业版同样支持多线程运行。本文撰写时，Phusion Passenger是最近一个号称“史上最快”的Ruby Web服务器（本文最后将进一步介绍Raptor）。</p>

<h4>Trinidad/TorqueBox</h4>

<p>Trinidad发布于2009年，基于JRuby::Rack和Apache Tomcat，使Rails的部署和世界上最流行的Web服务器之一Tomcat结合，支持集成Java代码、支持多线程的Resque和Delayed::Job等Worker，也支持除Tomcat以外的其它Servlet容器。</p>

<p>与Trinidad相比，同样发布于2009年的TorqueBox不仅仅是一个Web服务器，而且被设计成一个可移植的Ruby平台。基于JRuby::Rack和WildFly（JBoss AS），支持多线程阻塞I/O，内置对消息、调度、缓存和后台进程的支持。同时具有集群、负载均衡、高可用等多种附加功能。</p>

<h4>Puma</h4>

<p>Puma——Mongrel最年轻的后代于2011年发布，作者是Evan Phoenix。</p>

<p>由于Mongrel诞生于前Rack时期，而随着Rack统一了Web服务器接口，任何基于Rack的应用再与Mongrel配合就有许多不便。Puma继承了前者的Parser，并且基于Rack重写了底层通信部分。更重要的是，Puma部分依赖Ruby的其它两个流行实现：Rubinius和JRuby，与TorqueBox类似拥有多线程阻塞I/O的能力（MRI平台不支持真正意义上的多线程，但Puma依然具有良好并发能力），支持高并发。同时Puma还包含了一个事件I/O模块以缓冲HTTP请求，以降低慢客户端的影响。但是，从获得更高吞吐量的角度来说，Puma目前仍然需要采用Rubinius和JRuby这两个平台。</p>

<h4>Reel</h4>

<p>Reel是最初由Tony Arcieri发布于2012年的采用事件I/O的Web服务器。采用了不同于Eventmachine的Celluloid::IO，后者基于Celluloid——Actor并发模型的Ruby实现库，解决了EM只能在单一线程中运行事件循环程序的问题，从而同时支持多线程＋事件I/O，在非阻塞I/O和高并发方案间实现了良好的融合。</p>

<p>与其它现代Ruby Web服务器不同的是，Reel并不是基于Rack创建，但通过Reel::Rack提供支持Rack的Adapter。尽管支持Rails，与Puma也有一定的相似性，但与Unicorn、Puma和Raptor相比，Reel在部署Rails/Rack应用方面缺少易用性。但由于Celluloid本身的普及程度和擅长领域，相比其它Web服务器而言，Reel更适合部署Stream/WebSocket应用。</p>

<h3>四、迈向未来</h3>

<p>回顾过去，Ruby Web服务器在发展中先后解决了缺少部署方案、与Web应用程序不兼容、运维管理困难等问题，基础架构趋于成熟且稳定。而随着更多基准测试结果的出现，业界逐渐开始朝着更高性能和并发量发展，同时针对HTTP协议本身的优化和扩展引入的HTTP/2，以及在细分领域中采用的WebSocket/Stream等需求均成为未来Ruby Web服务器发展的方向。</p>

<h4>高吞吐量</h4>

<p>以最新的Raptor（上文提到的Phusion Passenger 5）为例，其在网络I/O模型的选择上融合了现有其它优秀产品的方案，包括Unicorn的多进程模型、内置基于多线程和事件I/O模型的反向代理缓冲（类似Nginx的功能，但对Raptor自身做了大量裁减和优化）、以及企业版具有的多线程模型（类似Puma和TorqueBox）；此外，Raptor采用的Node.js HTTP Parser（基于Nginx的Parser）的性能超过了Mongrel；更进一步，Raptor甚至实现了Zero-copy和一般在大型游戏中使用的区域内存管理技术，使其对CPU和内存访问的优化达到了极致（感兴趣的话可以进一步查阅<a href="http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/">这篇文章</a>）。</p>

<p>另外也需要看到，当引入多线程运行方式，现有Web应用将不得不仔细检查自身及其依赖，是否是线程安全的，同时这也给构建Ruby Web应用带来更多新的挑战。这也是为什么更多人宁愿选择进程级应用服务器的方式——毕竟对大多数应用来说不需要用到太多横向扩展，引入反向代理即可解决慢客户端拒绝服务的问题，而采用Raptor甚至在独立模式能工作的更好（这样就不用花时间去学习Nginx）。</p>

<p>除非你已经开始考虑向支持大规模并发的架构迁移，并希望节省接下来的一大笔花费了。</p>

<h4>HTTP/2</h4>

<p>2015年5月，HTTP/2随着RFC7540正式发布。如今各主流服务器/浏览器厂商正在逐渐完成从测试模块到正式版本的过渡。而截至目前，主流Ruby Web服务器都还没有公开HTTP/2的开发信息。HTTP-2是在2013年由Ilya Grigorik发布的纯Ruby的HTTP/2协议实现，包括二进制帧的解析与编码、流传输的多路复用和优先级、连接和流传输的流量控制、报头压缩与服务器推送、连接和流传输管理等功能。随着HTTP/2的发布和普及，相信主流Ruby Web服务器引入对HTTP/2支持是不可避免的。</p>

<h4>WebSocket/流（Stream）/服务器推送事件（Server Sent Events，SSE）</h4>

<p>2011年，RFC6455正式公布了WebSocket协议。WebSocket用于在一个TCP链接上实现全双工通信，其目的是实现客户端与服务器之间更高频次的交互，以完成实时互动业务。鉴于该特点，仅支持慢客户端的Web服务器就无法有效支撑WebSocket的并发需求，更何况后者对并发量更加严格的要求了。而对于同样需要长连接的流服务器和服务器推送事件服务（SSE），都避免不了对长连接和高并发量的需求。尽管高性能的Ruby Web服务器都有足够的潜力完成这些任务，但是从原生设计的角度来看，Reel无疑更具有优势。</p>

<p>本文灵感来源于RailsConf 2015的Session：&#8221;Riding Rails for 10 Years - John Duff&#8221;。</p>

<h3>参考资源</h3>

<p><a href="http://www.fastcgi.com/devkit/doc/fcgi-spec.html">http://www.fastcgi.com/devkit/doc/fcgi-spec.html</a></p>

<p><a href="https://github.com/rails/rails/blob/master/railties/lib/rails/commands/server.rb#L82">https://github.com/rails/rails/blob/master/railties/lib/rails/commands/server.rb#L82</a></p>

<p><a href="https://github.com/shugo/mod_ruby">https://github.com/shugo/mod_ruby</a></p>

<p><a href="http://www.rubyinside.com/no-true-mod_ruby-is-damaging-rubys-viability-on-the-web-693.html">http://www.rubyinside.com/no-true-mod_ruby-is-damaging-rubys-viability-on-the-web-693.html</a></p>

<p><a href="http://confreaks.tv/videos/railsconf2015-riding-rails-for-10-years">http://confreaks.tv/videos/railsconf2015-riding-rails-for-10-years</a></p>

<p><a href="http://kovyrin.net/2006/08/28/ruby-performance-results/lang/en/">http://kovyrin.net/2006/08/28/ruby-performance-results/lang/en/</a></p>

<p><a href="http://chneukirchen.org/blog/archive/2007/02/introducing-rack.html">http://chneukirchen.org/blog/archive/2007/02/introducing-rack.html</a></p>

<p><a href="https://www.wiredtree.com/blog/litespeed-replacement-for-apache/">https://www.wiredtree.com/blog/litespeed-replacement-for-apache/</a></p>

<p><a href="https://utcc.utoronto.ca/~cks/space/blog/programming/SCGIvsFastCGI">https://utcc.utoronto.ca/~cks/space/blog/programming/SCGIvsFastCGI</a></p>

<p><a href="https://www.litespeedtech.com/support/wiki/doku.php?id=litespeed_wiki:ruby_rails">https://www.litespeedtech.com/support/wiki/doku.php?id=litespeed_wiki:ruby_rails</a></p>

<p><a href="https://www.colm.net/open-source/ragel/">https://www.colm.net/open-source/ragel/</a></p>

<p><a href="http://zedshaw.com/archive/ragel-state-charts/">http://zedshaw.com/archive/ragel-state-charts/</a></p>

<p><a href="https://en.wikipedia.org/wiki/Comparison_of_parser_generators">https://en.wikipedia.org/wiki/Comparison_of_parser_generators</a></p>

<p><a href="https://blog.twitter.com/2010/unicorn-power">https://blog.twitter.com/2010/unicorn-power</a></p>

<p><a href="http://david.heinemeierhansson.com/posts/31-myth-2-rails-is-expected-to-crash-400-timesday">http://david.heinemeierhansson.com/posts/31-myth-2-rails-is-expected-to-crash-400-timesday</a></p>

<p><a href="http://techcrunch.com/2008/01/01/zed-shaw-puts-the-smack-down-on-the-rails-community/">http://techcrunch.com/2008/01/01/zed-shaw-puts-the-smack-down-on-the-rails-community/</a></p>

<p><a href="http://code.macournoyer.com/thin/">http://code.macournoyer.com/thin/</a></p>

<p><a href="http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf">http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf</a></p>

<p><a href="http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf">http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf</a></p>

<p><a href="http://www.infoq.com/articles/meet-goliath">http://www.infoq.com/articles/meet-goliath</a></p>

<p><a href="http://2ndscale.com/rtomayko/2009/unicorn-is-unix">http://2ndscale.com/rtomayko/2009/unicorn-is-unix</a></p>

<p><a href="http://unicorn.bogomips.org/SIGNALS.html">http://unicorn.bogomips.org/SIGNALS.html</a></p>

<p><a href="http://nginx.org/en/docs/control.html">http://nginx.org/en/docs/control.html</a></p>

<p><a href="http://izumi.plan99.net/blog/index.php/2008/01/14/what-is-so-hard-about-rails-deployment/">http://izumi.plan99.net/blog/index.php/2008/01/14/what-is-so-hard-about-rails-deployment/</a></p>

<p><a href="https://blog.phusion.nl/">https://blog.phusion.nl/</a></p>

<p><a href="https://trends.builtwith.com/web-server">https://trends.builtwith.com/web-server</a></p>

<p><a href="https://github.com/puma/puma">https://github.com/puma/puma</a></p>

<p><a href="http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/">http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/</a></p>

<p><a href="http://ohcoder.com/blog/2014/11/11/raptor-part-1/">http://ohcoder.com/blog/2014/11/11/raptor-part-1/</a></p>

<p><a href="https://github.com/celluloid/reel">https://github.com/celluloid/reel</a></p>

<p><a href="https://en.wikipedia.org/wiki/HTTP/2">https://en.wikipedia.org/wiki/HTTP/2</a></p>

<p><a href="https://github.com/igrigorik/http-2">https://github.com/igrigorik/http-2</a></p>

<p><a href="https://github.com/celluloid/reel/wiki/Frequently-Asked-Questions">https://github.com/celluloid/reel/wiki/Frequently-Asked-Questions</a></p>

<p><a href="https://en.wikipedia.org/wiki/WebSocket">https://en.wikipedia.org/wiki/WebSocket</a></p>

<p><a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 安全]]></title>
    <link href="http://www.hanyi.name/blog/2015/06/10/microservices-security/"/>
    <updated>2015-06-10T08:02:55+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/06/10/microservices-security</id>
    <content type="html"><![CDATA[<p>自计算机普及开始，安全问题就成为困扰产业发展的阿喀琉斯之踵。如今，安全问题爆发所造成严重后果的并不鲜见，事实上每一个产品参与者都承担了安全缺陷所带来的风险和损失。因此业界必须重视安全，理解并持续加固IT设施。当采用微服务架构后，安全问题的解决将面临新一轮挑战，更高的成本投入也是显而易见。</p>

<h3>1. 认证（Authentication）和授权（Authorization）</h3>

<p>在应用级，安全的第一道锁就是认证和授权。认证的目的是确认用户提交的身份信息是否属实，例如系统登录验证，而能够唯一识别用户的信息通常被称作Principal。授权是指依据用户的Principal允许其从事某些操作的机制。一旦Principal被验证，系统就能根据相关信息（例如Hierarchy）对用户进行授权。但在微服务架构下，由于服务的相互隔离性，Principal的传递面临挑战，毕竟用户并不希望访问任何独立服务都输一遍密码——这是一个很直观的用户体验问题。</p>

<h4>单点登录（SSO）</h4>

<p>SSO是最常见的一种认证和授权方案。SAML、OpenID Connect是当前业界比较流行的SSO实现，但基本原理相差无几，本文将以SAML为例简单介绍SSO技术。</p>

<p>当Principal试图访问系统资源时，首先由身份提供者验证其密钥信息，如果验证通过，Principal将被引导至服务提供者，并且由后者决定是否向Principal授权相关资源。</p>

<p>在上述过程中，身份提供者可以是第三方系统。例如，Google提供的就是基于OpenID Connect的身份提供服务，对于其它企业应用来说，这种身份提供者有权链接至组织内部的目录服务（可以是LDAP或Active Directory，此类系统能够存放Principles），也可以由自身直接提供数据服务。基于SAML技术的有Okta，也提供链接至企业目录的服务。</p>

<p>SAML本身基于SOAP标准，其背后的工程复杂度并不低；而OpenID Connect实际上是一种OAuth 2.0的特定实现，来源于采用SSO的Google和其它一些公司，后者使用简单REST调用。OpenID Connect的缺点是本身缺少支持它的身份提供服务，因此更广泛的用于互联网第三方登录机制，特别是在如今越来越多的公众互联网服务应用中。然而如果要采用组织内的身份提供服务，目前较好的方案是OpenAM和Gluu，但并不完善。这也成为OpenID Connect在统治SSO领域之路上的绊脚石，尽管它看起来确实是最终的选择。</p>

<h4>单点登录网关</h4>

<p>在微服务架构中，每个独立的服务都有权决定对应的身份提供者，显然这将造成大量的资源浪费。如果考虑采用共享代码库，还要规避异构技术栈的问题。因此这里给出一个基于SSO网关的有效解决方案。</p>

<p>该方法的核心是只在一处处理全部用户重定向请求和握手信息，并且要求下游服务都接收一个Principle，如果基于HTTP构建微服务通信，那么很自然地就可以利用HEADER解决信息承载的问题，Shibboleth就是其中一个实践方案。此外，如果认证服务被置于网关，就更难对隔离状态下的微服务进行调试，同时也不利于日常开发。</p>

<p>上述方法最大的问题是它可能给人一种绝对安全的错觉——导致越来越多的功能依赖网关进行传递。这么做的结果就是网关服务变得越来越臃肿，成为系统中一个巨大的耦合点，同时也提高了单点失败的风险，特别是它也是反安全模式的——功能扩充意味着增加攻击面，就更容易遭受攻击。</p>

<h4>细粒度授权</h4>

<p>网关能够提供一个有效但较粗粒度的授权。但是由于它只有Principle而无法做到深度解析，就缺少更细的授权功能，而后者更多需要微服务自身去负责。而对于用户Role这种Principle信息来说，围绕组织功能实现粗粒度授权——然后进一步在微服务中实现细粒度授权。</p>

<h3>2. 服务间认证和授权</h3>

<p>Principle通常用来指代人机交互过程中的认证和授权对象。随着微服务架构日益复杂，服务间也必然会出现类似的交互过程。下面列举了若干种当前常用的解决方案：</p>

<h4>边界内畅通</h4>

<p>最简单的可能是，当服务请求来自组织内部——那么自然被当作是可信的。或者更远一些，建立HTTPS通信以防止中间人攻击。缺点是一旦内网被攻破，那么内部系统几乎没有任何防御，这就是安全领域的单点失败，然而事实上这也是目前多数组织的选择。</p>

<h4>HTTP(S)基础认证</h4>

<p>HTTP(S)协议具有基本的认证机制，其在Header中携带一个用户名和密码，并由服务端进行认证，实现起来也十分方便。缺点是采用HTTP传输用户名和密码十分危险，你几乎始终需要HTTPS——但需要额外的证书管理成本。此外，基于SSL的通信无法被类似Varnish、Squid等反向代理缓存，也就是说，在这种情况下，缓存策略只能在应用服务端、或是客户端实施。一种解决方法是在外层搭建一个LBS用于解析SSL通信，并在LBS后端存储缓存。</p>

<p>另一方面，如果现有架构已经采用SSO，那么如何结合SSO也是一个难题。如果允许基本服务访问与SSO一致的目录服务，也可以另外搭建一份目录服务——这种重复功能的存在会导致更多的潜在风险。还需注意：如果采用身份认证方案，那就意味着拥有Principle就能够访问资源，而无论它身在何处。</p>

<h4>采用SAML和OpenID Connect</h4>

<p>直接应用SSO架构能够减轻一些开发成本，如果基于SSO网关，那就意味着所有的通信将路由至网关，否则就又相当于重复功能。此外，客户端服务需要妥善保存自身的证书，以便用于各类服务间通信的认证——这就需要第三方存储服务。此外，无论是SAML和OpenID Connect，其组织内应用的支持都还远未成熟。</p>

<h4>客户端证书</h4>

<p>另一种认证方法是采用TLS（相当于SSL的继任）的特性，由于TLS要求每个客户端都具备X.509证书，那么基于证书认证的通信可以保证安全性。问题是需要一个完整的证书管理机制，因为这不仅仅意味着创建和管理证书，同时还要验证证书正确工作。采用通用证书也许是一种方法，但会引起一定的安全风险。因此当通信安全要求较高时，才应考虑该方案。</p>

<h4>基于HTTP的HMAC</h4>

<p>HMAC指一种基于哈希值的消息码技术，它克服了HTTP基础认证的安全缺陷，同时能够在HTTP上实现类似HTTPS通信。该技术最初由Amazon的AWS S3 API实现，并且属于OAuth规范的一部分。HMAC通过计算消息体和私钥的哈希值，将结果封装进消息体本身。服务端同样保存了一份私钥，然后重算并比较消息体携带的值，如果二者结果一致，则被认为是合法的请求。HMAC能够有效防止中间人攻击，同时由于私钥本身不会被明文传输，因此能保证一定的安全性。同时比起HTTPS还拥有更好的计算性能。</p>

<p>HMAC的主要缺点在于，首先服务间需要共享相同的私钥，这种私钥可以是硬编码的（缺少灵活性），也可以通过第三方获取（需要额外设计私钥交换机制）。其次，当前HMAC并没有一种统一的实现，需要开发者自己决定实现细节，比如采用SHA-256。JSON Web Tokens（JWT）也是一种可行的方案，但依然缺少标准实现。最后，需要知道HMAC只能保证通信不被第三方篡改，由于消息体本身使用HTTP传输，依然会被网络程序嗅探。</p>

<h4>API密钥</h4>

<p>目前绝大多数的互联网服务都采用API密钥解决认证和授权问题。然而如果要直接用于微服务架构，还存在一些困难。首先，一些系统使用共享的API密钥，同时基于类似HMAC的方法进行通信，而也有部分系统采用公钥＋私钥的形式。此外，密钥管理一般也是集中式的，类似前文提到的网关方案。</p>

<p>API密钥真正风靡的原因是其易用性，与SAML相比，基于API密钥的认证与授权几乎就是零成本。而且API密钥还能够用于频率控制、转化率分析、API目录、以及服务发现系统，具有相当的灵活性。一些API系统还允许将API密钥链接至现有目录服务，这样就能真正实现同步管理Principle和密钥，达到高可配置化。一种随之而来的复杂结构是：用户认证统一采用SAML实施SSO，然后取得API密钥用于服务间通信，二者共用一套目录服务。</p>

<h4>代理问题</h4>

<p>随着服务数量和调用层级增加，代理问题可能影响系统安全。如果采用传统单一系统的形式，服务调用和用户界面直接通信，因此SSO就能直接解决所有问题。但对于微服务而言，调用层级使得SSO不再有效。例如，当用户访问A服务，并且需要通过A服务调用B服务的借口时，对B来说现有SSO方案就无能为力，此时为了确保用户合法性，就只能在发生调用时携带原始Principle，并在B端进行重新认证。随着微服务架构的普及，此类应用场景会越来越多，代码和功能的重复性会显著提升。</p>

<p>一般而言，解决上述问题存在三种基本方法：1.忽略安全性，即隐式可信，一些安全性要求低的应用就无所谓了。2.前面提到的传递Principle。3.服务B向A请求Principle。但无论是哪一种，目前都缺少一个成熟的解决方案。</p>

<h3>3.静态数据安全</h3>

<p>静态数据Data at Rest，与使用中数据Data in Use，以及动态数据Data in Motion，分别描述了计算领域中的三种数据形态。使用中数据，一般指存在于内存、寄存器或逻辑计算单元中的数据。动态数据，主要指网络中传输的数据。而静态数据，主要指存放在物理介质中的数据。通常所说的安全一般都是针对使用中的动态数据，例如网络安全、系统安全和应用安全。然而如果上述安全措施不再有效，静态数据被窃取就会显得易如反掌——从而为业界引入了深度安全的概念。</p>

<p>无论如何，数据窃取事件的发生不外乎未加密存储、或是保护系统失效，在任何安全方案中，此类隐患是必须得到重视的。</p>

<h4>尽量采用稳固的加密算法</h4>

<p>如果要自己实现加密算法，安全性就很难保证。即使采用第三方加密算法，也需要时刻保证该算法是否会随时被发现漏洞并攻破。AES-128和AES-256是一种有效的静态数据加密算法，许多语言都内置了算法实现，Java和C#也可以采用Bouncy Castle Libraries。密码也应至少实现带盐哈希加密。</p>

<h4>密钥存储</h4>

<p>许多加密算法都引入了密钥环节，因此对密钥本身的保护也不容忽视，否则再强大的加密算法也是十分脆弱的。采用第三方系统管理密钥是必须的，或者直接采用类似SQL Server的透明数据加密。无论采用何种方案，都需要仔细分析相关的安全风险。</p>

<h4>可选和必选</h4>

<p>应有选择的加密静态数据——这不仅关系到应用性能问题。一方面，前文介绍的日志和监控需要明文数据，此外，数据移植也会因为引入解密、加密过程而变得繁琐和低效。因此，对数据进行安全性分级是必要的。此外，对高安全要求的数据，当数据获取后即加密，只在请求数据时解密——除此之外不要在任何形式下存储该数据。对于备份数据，应实现整体加密并妥善保存和管理。</p>

<h3>4.深度防御</h3>

<p>安全如今已经不仅仅是一个单一的概念，要实现高可靠的安全，必须采用综合、深度防御，摒弃单点失败带来的潜在风险。当防御因素增加，攻击者成本也就越高，系统安全性才能得到保证。</p>

<h4>防火墙</h4>

<p>防火墙依然在进化，相比过去的端口限制和包识别，像ModSecurity已经实现了限制IP段连接次数、主动监测某些恶意攻击等功能。同时，采用多层防火墙也是必要的，例如系统级可以采用Iptables，而在应用级，服务内部也可以设置防火墙进行自身防御。</p>

<h4>日志</h4>

<p>日志是把双刃剑，一方面，良好的日志结构能方便发现各种风险，包括安全问题。但是日志中的敏感数据本身也会造成风险，适当遮蔽这部分数据是有必要的。</p>

<h4>入侵检测和防御系统（IDS/IPS）</h4>

<p>与防火墙不同的是，入侵检测主要监控系统内部行为，并发出警告或选择阻止危险行为。但是IDS（IPS）在实施上需要投入长期的人力成本，现有的IDS基本都是基于启发式防御，其基本形式就是通过设置一些规则，当某些系统行为与该规则相匹配，则认为该行为有风险。但在实施过程中，特别是系统初期建设时，入侵规则的建立是和系统和架构特点息息相关的。因此通常应从一个被动式IDS/IPS开始，逐步完善入侵规则，再逐渐过渡到主动防御——才是有效且可靠的。</p>

<h4>网络隔离</h4>

<p>在单一系统中，由于应用设施都部署在同一环境，从而导致安全性的单点失败风险。而对于微服务架构，由于服务隔离性，本身就可以通过现有的网络管理机制增加安全性。例如AWS就提供一种自定义虚拟私有云VPC的服务，该服务允许主机处于相互隔离的子网中，从而通过定义网络规则指定服务间的可见性，或者指定网络通信的路由方式。</p>

<h4>操作系统</h4>

<p>操作系统级别的安全策略，主要集中在运行服务的用户最小权限、以及基础软件的漏洞修复速度。目前大型软件的更新都支持自动化，同时提供警告机制，例如微软的SCCM和RedHat的Spacewalk。</p>

<p>另一方面，操作系统的第三方安全模块也值得考虑。例如RedHat采用的SELinux，Ubuntu和SuSE支持的AppArmour，还有GrSSecurity，上述模块允许用户定义系统安全行为，并且直接监视内核行为，及时制止相关操作。</p>

<h3>5.小结</h3>

<p>在德语中有一个短语Datensparsamkeit，意思是当你需要存储数据时，应尽量保证只保存有效且合法的最小数据集，该定义来源于德国的隐私保护法案。实际上，当你不携带“有价值”的数据，那么自然就不会引起攻击者觊觎了。许多系统发生隐私泄漏事件，其本身就无权存储相关信息，这才是风险的源头。</p>

<p>另外，组织内部人员也是一大风险因素，如何处理权限的创建和删除、避免社会工程攻击、或者其他内部人员的恶意攻击，都是组织管理需要考虑的问题。</p>

<p>安全领域的另一大忌就是避免重复造轮子，因为很难得到充分的审议和验证，此类代码将成为极高的风险源。</p>

<p>最后，随着OWASP等的流行和安全测试框架的日益完善，可以考虑把安全测试引入到现有CI/CD流程，例如Zed Attack Proxy（ZAP），Ruby支持的Brakeman，Nesssus等等。微软提出的安全开发生命周期，也是一个有效的开发团队安全实施模型。然后就是定期邀请组织内外的安全专家逐轮审议、修订安全架构，以确保安全设施的持续更新。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microsevices陷阱: 监控]]></title>
    <link href="http://www.hanyi.name/blog/2015/06/04/microsevices-trap-monitoring/"/>
    <updated>2015-06-04T13:57:50+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/06/04/microsevices-trap-monitoring</id>
    <content type="html"><![CDATA[<p>微服务带来的是更高的架构复杂度，即使你有能力驾驭整体设计，也很难处理生产环境中的各种failures。监控在运维中是一项专门的技术，而对微服务而言则更具有必要性。原因在于，对于单一系统、或者是单点失败型系统而言，任何的错误信息都是有的可循的，然而对于微服务类似的自治系统来说，除非拥有良好的日志和监控系统，否则连发现问题都成问题。一个好的解决办法就是分布式日志数据抽取和聚合查询。</p>

<h3>1. 系统架构对监控模式的影响</h3>

<p>由于系统架构的不同，即使利用前述的思想，实施监控模式也有一定区别。</p>

<h4>单服务，单节点</h4>

<p>一切都变得如此简单，CPU、内存&hellip;日志和查询系统、平均响应时间、负载增长&hellip;甚至当你知道Nagios或New Relic，搭建此类系统就几乎等于零成本。当然，这并不意味着你就此精通监控技术&hellip;&hellip;</p>

<h4>单服务，多节点</h4>

<p>由于各个节点的权重由LBS决定，因此监控时采用的系数可能会有差别。不过，通过把多个节点的运维数据聚合在一起，实现一次查询也是理想的方案。直观的，可以利用ssh-multiplexers类的工具同时在多节点上获取数据，并存储在第三方节点上向运维提供分析平台。</p>

<h4>多服务，多节点</h4>

<p>在遇到真正的微服务大杀器时，运维会遇到很多从未谋面的问题。例如在复杂架构下，如何判断某个服务的错误和功能错误有关（可调试性），如何在海量运维数据中找到具体的错误（可检索性），这些都成为眼前难题。而解决方案还是采集＋聚合，无论数据属于日志或系统应用度量。</p>

<h3>2. 日志系统架构</h3>

<p>现在，开源界流行专门的日志采集工具logstash，和聚合查询系统Kibana，实际上就体现了一种良好的日志监控系统架构。logstash负责分布式日志数据采集和过滤，并将其存储至任何介质。Kibana是一个基于ElasticSearch的聚合查询工具，能够方便管理其中的日志数据，并提供可视化。但是，针对不同类型的日志数据也要采取不同的采集和管理方案。</p>

<h4>跨多服务跟踪系统应用度量数据</h4>

<p>系统应用度量，包括CPU、内存、网络请求/响应等基础数据，在复杂架构中，运维可能需要全局性数据，也可能需要针对单个服务的全部节点数据，也就是说，在元数据中必须加入相关的关联性，以保证日志数据的可用性。当数据准备就绪，Graphite这种实时可视化工具就能派上用场了。</p>

<h4>服务度量</h4>

<p>系统度量采集了除应用服务之外的几乎全部运维数据，而对应用本身来说，利用其自身的日志工具在多数情况下就足够了。但是，我们可能有时会遇到如下若干需求：希望查看所有用户检查其购物车的次数、希望查看用户操作的平均间隔时间&hellip;对于此类统计需求，一方面可以作为功能进行开发，但由于引入发布流程，本身缺乏灵活性，在多数场合下并不适用。另一方面，可以直接从日志数据中攥取所需信息。很显然，后者带来的好处更多，实现现有数据的更高效利用（但具体技术还在发展中，例如大数据技术），甚至可能挖掘出新的商业信息。因此，在任何关于日志数据挖掘的理论普及之前，好的实践应是尽量多的保存日志信息，因为其中可能蕴藏着未被发现的金矿。</p>

<h3>3. 监控系统</h3>

<h4>综合监控和语义化</h4>

<p>对日志系统的监控，通常要求实现一个对人的警告功能。但是在具体实践中，可能只是对CPU、内存甚至磁盘利用率设置一个阈值，一旦运维数据达到这个值就向运维人员发送警告。问题在于，现实中我们想在第一时间得到的信息其实是“系统在正常工作吗？”，而单个维度的超限可能无法等同于上述答案。</p>

<p>因此为了保证运维人员不至于整晚都睡不好觉，监控模型的改进还是很有必要的。除了针对底层运维数据的监控，从业务角度入手实现综合监控是未来发展的趋势，特别是在微服务架构下。例如，有时需要监控微服务间可用性，除了观察各服务的基础运维数据，还可以从业务角度入手，检查数据流的变化情况，以及健康度。这种更高层的监控，也被称作语义化监控，在实际中对运维人员实现了更高要求：理解业务和设计。</p>

<h4>关联IDs</h4>

<p>分布式系统的跟踪调试是一个世界性难题，实际中也很少有人能够遇到、甚至尝试解决此类问题。一般而言，微服务架构的日志系统至少应包含一个跟踪功能，否则一旦出现监控警告，我们能看到的只有直接服务代码，其上游成百上千的服务调用却一无所知——当然这种规模的系统没有几家公司拥有。Google在2010年发表了Dapper——其大规模分布式系统跟踪基础架构，Twitter随后在前者的研究基础上实现了zipkin——开源分布式系统跟踪框架。上述解决方案无一不具有“重”的特点，但基本原理类似：在服务间传递消息时，在消息头封装一个特殊的GUID，并将其写入日志或第三方系统。</p>

<p>目前来看，关联IDs是微服务架构必须要尽早考虑的问题，否则一旦出现问题就很难有充分的信息进行定位。而采用重量级框架带来的成本可能较高，理想方案是尽量简单实现类似功能，并集成进现有日志系统，如果需求复杂度进一步提升，就可以考虑引入大杀器灭之。</p>

<h4>层级关联</h4>

<p>本节开始提到了语义话监控的概念，它对于“系统正确运行”含义的代表可能要强于底层运维数据警告。但如果从定位问题的角度出发，即使发现问题存在，也不意味着立即定位问题，更谈不上提高可用性了。例如，两个独立服务分别运行良好，但服务间通信出现问题，导致数据无法正确传输，但现有功能依然存在，严重的话可能引起数据级别的错误，因此这种服务间集成点的监控成为必须要考虑解决的问题。</p>

<p>实践中针对服务层级的监控多引入一种名为断路器的工具，其用途是一旦发现通信中断，就立即断开当前服务与下游的通信，从而避免错误的持续传递造成灾难。Netflix的Hystrix是这一领域中基于JVM的开源框架。</p>

<h4>标准化和可读性</h4>

<p>日志/监控系统的重要内容就是标准化，当你采用微服务架构，标准化就更加重要——这恐怕是你唯一能够从整体上把握系统的切入点。另一方面，监控最终还是向人提供决策参考——因为角色的不同，不同的人对监控数据的理解也存在偏差，因此在设计监控系统时还需要考虑的重要问题：</p>

<ol>
<li><p>人们当前需要什么数据。</p></li>
<li><p>人们随后需要什么数据。</p></li>
<li><p>人们习惯于如何消费这些数据。</p></li>
</ol>


<p>至少你应该尝试读一下《Information Dashboard Design: Displaying Data for At-a-Glance Monitoring》，相信会把对监控的理解上升到人文的高度。</p>

<h4>4. 总结与未来</h4>

<p>监控领域非常得大，有时甚至超过产品本身——这会不会是一次大规模的历史性误区，目前还很难说。但现状是几乎所有的技术研发公司都在该领域发力，希望从此走上数据金矿的道路。传统上，我们利用自身简易工具监控系统应用，利用GA、Omniture抓取业务数据，利用日志系统定位问题。碎片化的工具方便实现精细化分工，但很大程度上阻碍了数据的进一步攥取，成为大数据之路上的绊脚石。</p>

<p>监控工具的融合进程目前依然缓慢，Riemann是现有的比较流行的分布式系统监控框架，其本质上是一个事件服务器，通过接收分布式系统的各类事件采集、聚合相关数据，但仍主要集中在系统应用监控方面。Suro是由Netflix开源的一套分布式数据管线框架，数据处理方式类似Riemann，但更集中于数据管线的功能，其下游应用包括Storm实时数据分析框架、Hadoop离线批处理框架以及Kibana日志分析平台。</p>

<p>当然，这种融合趋势并不会影响专注于不同领域的工具的发展，但统一数据接口是眼下应当开始的工作。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microservices陷阱: 测试]]></title>
    <link href="http://www.hanyi.name/blog/2015/06/01/microservice-testing/"/>
    <updated>2015-06-01T08:53:39+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/06/01/microservice-testing</id>
    <content type="html"><![CDATA[<p>层级良好的测试是保证持续集成/交付的前提。对微服务而言，随着系统复杂性的提高，原有测试层级会遭遇挑战，本文将在澄清现有基本概念的基础上讨论这一问题。</p>

<h3>1.测试自动化</h3>

<p>《Agile Testing》的测试四方图帮我们理清了测试开发的基本类型：验收测试、探索测试、单元测试和性能测试。当然这既包括手动、也包含自动化测试，这里本文只关注后者，但二者的地位应是同等重要的。</p>

<p>Mike Cohn在《Succeeding with Agile》中介绍了自动化测试的金字塔图，书中的金字塔把自动化测试划分为三种类型，从底向上分别是单元测试、服务测试和UI测试。其中，服务测试在大多数情况下指集成测试，而UI测试目前更多被称作End-to-End测试。关于这三种测试的概念这里就不再强调，但实践中更多会面临各自权重的问题。</p>

<h4>测试权重</h4>

<p>测试金字塔给出了以下结论：当层级越高，测试的范围越大，质量保证的可靠性也越高，但是反馈时间也越久，调试难度越大；层级越低则这种趋势会完全相反。因此，合理安排测试量是一个非常重要的问题。但应强调一点，上述测试并非是一次促成，固定不变的。例如服务/UI测试一旦出现问题，最便捷的解决方法是引入更多单元测试去覆盖上述边界条件。</p>

<p>但是无论如何也不要陷入误区：即使你有更多时间去完善UI测试，也不要忽视金字塔量级的规律。一种常见的反模式是测试量呈倒金字塔状，过度依赖UI测试，而忽略了单元测试。因为这么做忽略了反馈周期的因素，随着UI测试反馈周期的不断增加，必然降低整个团队的效率，也不利于代码质量的保证。</p>

<h4>实现服务测试</h4>

<p>微服务架构下的服务测试显得尤为重要，但实际上也面临集成的问题，特别是CI架构方面。例如采用CI实时启动一个构建好的消费者服务，或是直接在测试代码中stub——但需要花费额外代价去模拟stub的功能。提到stub，可能会引出mock的问题，后者关注行为，实际上是面向边际效应的一种测试手法，《Growing Object-Oriented Software, Guided by Tests》详细比较了上述二者。当然Martin Fowler的<a href="http://www.martinfowler.com/bliki/TestDouble.html">TestDouble</a>也是一个直观的介绍。</p>

<p>现今的服务测试工具已多如牛毛，其基本原理还是stub一个消费者服务，运行测试前启动该服务即可。其趋势已是跨平台、跨语言了（参考Mountebank）。</p>

<h4>微服务和UI测试</h4>

<p>UI测试本身具有一定复杂度，更别说和微服务架构集成了。如果采用单一系统的做法，需要在CI上分别部署全部服务，然后触发UI测试——这不仅仅是花钱的问题，更意味着时间成本的增加。</p>

<p>UI测试中普遍存在某些代码片段，是时断时续的，这就导致开发人员可能会不断重复运行测试，因为这可能是一个随机产生的现象。对于这种不确定性的Flaky测试，应当尽量移除出代码库。Martin Fowler在<a href="http://martinfowler.com/articles/nonDeterminism.html">Eradicating Non-Determinism in Tests</a>中详细讨论了UI测试中存在的这类问题，并给出了一些解决方案。</p>

<p>另一个问题是UI测试的ownership分配。一般情况下，单一系统可能是全部团队成员维护一个UI测试代码库。但是这会引发测试健康值下降的问题，有时会派QA监控这一问题。而对微服务而言，UI测试可能是跨团队的，因此必须限制测试代码的提交权限，以及明确ownership。最多应覆盖相关的服务开发团队。</p>

<p>当然，微服务带来的UI测试爆炸也是一个比较严重的问题。尽管已经有一些并发框架例如Selenium Grid减少时间的浪费，但这些并不能实际解决测试爆炸的问题。终极方法可能还是精简当前测试部署，当然这涉及到UI测试的代码架构。</p>

<h4>发布堆积</h4>

<p>由于微服务的相互隔离性，一旦UI测试失败，在某个源头微服务修复前，其它微服务将无法被正确部署，从而导致发布包产生堆积现象。一种解决方法是一旦UI测试失败，停止所有的代码提交——当然这并不容易，特别是当变更较大时，修复的成本较高，可能导致全部团队效率的下降。因此保证小变更的频繁提交是一个有效减小风险的实践。</p>

<p>到这里你可能会对微服务产生质疑，既然End-to-End测试通过后才能发布，岂不意味着某一批版本的微服务将被同步发布？那么，这种部署方式还遵循微服务的独立部署原则吗？实际上，尽管可能把End-to-End测试作为一个限值，但微服务的独立性并没有遭到破坏，如果采用同一个发布版本，无意间就意味着微服务间的耦合性在增加，反而失去微服务本身的优势。因此保持平衡十分重要。</p>

<h3>2.测试代码架构</h3>

<p>当测试代码开始融入codebase，就要考虑设计的问题了——因为你不得不去考虑功能代码中遇到的同样问题。但由于根本目的不同，测试代码的设计存在不一样的可能。</p>

<h4>用户轨迹，而非用例</h4>

<p>对于UI测试，按照敏捷的一般实践，可能很自然地实现针对每一个story的测试。面向用例的UI测试带来的是大量重复性测试，这显然不利于控制UI测试的成本。另外，从探索测试的角度来说，遵循普通用户轨迹的UI测试可能才是接近真实情况的。因此，UI测试的组织应尽量面向用户轨迹，特别是核心功能的用户轨迹。</p>

<h4>消费者驱动测试和Pact</h4>

<p>前文已经提到，基于stub/mock的服务集成测试已经比较常见，但是，尚没有一个能够保证集成测试尽快响应代码变更的机制。例如，当某个服务发生变更时，就需要考虑修改其它服务实现的它的stub，否则就会降低测试结果的可靠性。一种方法被称作<a href="http://martinfowler.com/articles/consumerDrivenContracts.html">消费者驱动的契约测试</a>，其基本出发点是开发能满足消费者需求的服务，同时尽可能保证实时同步相互之间的变更。其基本过程就是由消费者一方提出需求，并构建契约文本，再与服务提供方达成一致，实现相关功能。</p>

<p>Pact是一个开源的基于消费者驱动的测试工具，分别基于Ruby、Java和.Net。而Pact生成的契约文本通常是JSON格式，这就形成了跨功能的契约传递。Pact生成的契约可以通过任何形式存储，例如CI/CD的交付物、或Pact Broker版本管理工具，后者允许服务提供者能够同时对多个版本的消费者服务运行集成测试。另一个工具Pacto，实际上记录了服务间的交互过程，并形成消费者驱动的测试，比起Pact它更为静态化，而Pact则被嵌入到测试内容中，随着功能变更实时变化。</p>

<h4>集成测试和UI测试</h4>

<p>那么问题来了，前文提到UI测试是一项非常耗成本的工作，针对微服务尤为如此。随着集成测试的有效引入，是否就意味着降低甚至移除UI测试？答案是未必。</p>

<p>UI测试能够让许多问题在发布前最后一刻暴露出来，从而避免灾难的发生。因此，在发布时间不是非常紧急的情况下，运行UI测试反而能够降低人工成本。如果发布在即，可能来不及运行UI测试，那么撰写UI测试可能就显得没有特别必要了？现在针对微服务架构已经有一种做法，直接在生产环境中运行UI测试，而整个测试过程需要通过语义化监控技术记录全程（关于监控和语义化监控，我们会在下一篇文章中介绍）。</p>

<h3>3.发布后测试</h3>

<p>如果测试只在发布前进行，当出现线上bug，测试可能就显得无能为力，只能待开发人员修复功能后，预上线环境重测。问题的本质在于，测试很难完整模拟用户行为，特别是你永远都不完全了解用户是怎样使用系统的。为了尽早发现bug，在发布后测试是一个有效方法。</p>

<h4>一次发布，分别部署</h4>

<p>我们已经知道，微服务建议独立部署各自的服务，那么在发布前针对单一服务的测试可能显得无力。如果先部署服务，再运行针对该服务的测试，就能快速发现很多问题。这种测试方式的一个典型案例就是“冒烟测试”，即快速运行针对新部署服务的测试。</p>

<p>一个稍复杂的例子是蓝/绿部署。这种形式下，系统存在两份相同的拷贝，但只有其中一个真正接收外部请求。例如现有正常服务A，当更新A+发布时，先将A+部署到另一个环境，运行冒烟测试，通过后再把生产环境的流量导入到A+。该做法还能保证即使出现更多失误，也能快速回滚代码。然而，蓝/绿部署需要额外的成本，首先你需要大量环境容纳各种版本，并且能快速切换流量（基于DNS或LBS），当你采用弹性云服务实现这些就很方便。</p>

<h4>金丝雀发布</h4>

<p>相比蓝/绿部署，金丝雀发布则更为强大（要求也更高）。在这种发布形式下，新服务部署后，会由导流工具引入一部分生产环境中的流量，然后通过比较两个共存版本间的各种监控数据来保证功能的正确性，一旦新服务的出错率明显上升，则切断该部分的路由，反之则切断原有服务的路由。金丝雀发布的生产效率更高，但技术要求也更多，特别是针对幂等规则下的请求/响应通信，如何实现无缝导流会是一个难题。</p>

<h4>MTTR和MTBF</h4>

<p>MTTR指平均修复时间，MTBF指平均错误间隔时间，这两种概念实际上体现了不同的运维策略。无论是蓝/绿部署还是金丝雀发布，其出发点都是承认线上错误是不可避免的，因此MTTR是此类策略更关注的内容。而要保证MTBF，必须在上线前实现充分测试，其花费的成本也是十分可观。因此在实际中，MTTR和MTBF只能在权衡下保证，除非你有不计成本的投入（如重大的、允许失误率极低的项目）。而按照实际经验判断，MTTR可能是面向普通业务更为现实的选择。</p>

<h3>4.跨功能测试</h3>

<p>如今我们逐渐开始关注非功能测试，例如性能测试。但从词义上理解，把非功能测试看作是“非功能的”可能并不准确，因此这里采用“跨功能测试”一词。
跨功能测试在测试四方图中占有一席之地，因为其实际上十分重要，但在大多数项目中此类测试通常都启动得太晚，以至于出现了额外的超额工作。要知道，跨功能测试从重要性、架构复杂度和维护需求上几乎和所谓的功能测试并无差别，只是更容易在初期为人所忽视而已。</p>

<p>例如性能测试，在End-to-End测试阶段，这种测试更类似负载测试，而在单元测试阶段，则属于Benchmark测试。因此结合功能测试实现性能测试是一个有效途径。但应注意性能测试对环境的真实程度要求更高，可能会产生更多额外成本。但至少在当下，应尽量开始Benchmark及相关的尝试。</p>

<h3>5.小结</h3>

<p>总结上述这些与测试的有关经验，我们可以列出以下几点主要内容：</p>

<ol>
<li><p>通过细分测试，优化快速反馈流程，减少项目风险。</p></li>
<li><p>通过消费者驱动的测试避免微服务架构下的UI测试。</p></li>
<li><p>采用消费者驱动契约建立微服务团队间的沟通渠道。</p></li>
<li><p>理解MTTR和MTBF的区别，以及实际运维中的权衡。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[microservices陷阱: 部署微服务]]></title>
    <link href="http://www.hanyi.name/blog/2015/05/19/microservices-trap-deployment/"/>
    <updated>2015-05-19T09:44:53+08:00</updated>
    <id>http://www.hanyi.name/blog/2015/05/19/microservices-trap-deployment</id>
    <content type="html"><![CDATA[<p>对于单一系统而言，无论采用何种形式，基本的部署工作流是显而易见的。然而一旦采用微服务架构，服务间的相互依赖性会导致现有工作流发生混乱，这肯定是我们都不愿意看到的。本文谨对此展开讨论。</p>

<h3>1.持续集成（CI）和持续交付（CD）</h3>

<p>CI的核心目标是保证团队成员进行良好的同步协作。其工作方式通常是检查代码更新状态、获取最新代码并且编译和运行测试。CI的产出物通常是可用于直接进行部署或进一步测试的软件包，理想的情况是所有环境的部署都采用同一个软件包，也就是对于CI只产生唯一的交付物。</p>

<p>值得一提的是，采用CI工具和实践CI方法完全是两回事，例如：保证每天都能合并到主干、保证代码更新都有足够的测试、保证CI持续运行是团队第一要务等等，以上实践能充分发挥CI工具的效用。</p>

<p>CD则把CI和更多软件开发实践结合在一起，例如流水线、自动化部署、以及运维等，从而有效降低项目风险，提高团队效率。</p>

<h3>2.CI与微服务</h3>

<p>当采用微服务架构时，如何与CI结合则成为眼前的工作。最简单的实践可能是，把所有服务存储在同一个代码库中，任何提交都会出发任何服务的构建、测试乃至生成部署包。然而该做法是十分浪费的选择，简单改进是增加构建脚本的复杂度，在代码check out后，针对每个服务，由不同的工作流负责构建相应微服务，从而节约资源。</p>

<p>当微服务扩展至不同团队后，共享代码库的方式可能会带来很多问题，这时就需要直接拆分代码库、实现工作流－微服务一一对应的结构了。</p>

<h4>架构演进对CI的影响</h4>

<p>值得一提的是，CI工作流的设计应和应用架构保持一致。也就是说，在初创项目时期很可能都会是一个单独的代码库，CI的结构也应当比较简单。随着领域模型的逐渐建立和API保持稳定，划分服务并且分别构建会是自然的选择。</p>

<h4>特定平台的交付包</h4>

<p>鉴于微服务允许采用不同的技术栈实现，Jar/War、Gem、Egg等区别导致了CI技术的丰富性，实际上也增加了CI实践的复杂度。因此，采用现代配置管理工具如Chef、Ansible则是必须的。另一方面，当把上述各种不同的技术部署上线时，对配置管理的依赖就更加重要了。</p>

<h4>操作系统交付包</h4>

<p>在实践中，自动配置管理面临着跨平台的挑战。即使是面向Linux，也存在发行版之间差异的问题。如果把配置管理的结果从线上环境变为Rpm、Deb、甚至MSI包，则就大幅度降低了操作系统差异所带来的挑战。当然，最好还是尽量在线上环境采用统一的平台为好。</p>

<h4>自定义镜像</h4>

<p>自定义镜像的好处是，我们不再直接在环境上运行配置管理工具，而是在本地构建好虚拟环境，部署好应用，然后再打成镜像包发布，从而节约了线上部署的繁冗时间，消除此举给线上环境带来的潜在问题。虽然说节约时间，但生成镜像依然是十分耗时的工作，好在现有的配置管理工具几乎都支持VMWare、AWS AMI、Rackspace、Digital Ocean、以及Vagrant打包，你需要做的只是本地运行一遍脚本，然后随时发布镜像包即可。</p>

<h4>镜像即交付包</h4>

<p>当采用镜像部署成为常态，一个合理的选择是把镜像包作为交付物融入CI/CD工作流，真正实现业务和基础设施的分离。</p>

<h4>服务器的不变性</h4>

<p>如果你发现每次运行部署之后，都需要人工检查并修改某些已有配置时，那就意味着配置管理发生了偏差。这种情况发生的原因通常是服务器配置环境发生了变化，而应对措施是尽量不要人工维护或修改环境配置。一个好的实践是在每次代码更新都创建全新的镜像。当然也会损失一些时间成本。</p>

<h3>3.CD与微服务</h3>

<p>当微服务运行在CD工作流中时，需要注意更多潜在的问题。</p>

<h4>环境</h4>

<p>在CD实践中，我们通常会遇到Slow Tests、UAT、Performance Tests分别部署在不同环境上的例子，针对单一系统管理上述环境本身就是一个挑战。而当这些遇到微服务，其规模和工作量就可想而知了。
环境问题本质上还是配置管理的方式，当不同微服务所需环境互不相同时，选择合理的部署方式则变的非常重要。例如对每个环境构建一个交付包，同时包含环境配置文件，当部署时同时部署代码和配置&hellip;一切看起来顺理成章，但似乎有些违背CD的原则，特别是唯一交付物原则。这就会导致多环境带来的风险削弱好处大幅丧失。另一方面，合并交付包带来的效率、安全等问题是不得不考虑的。
一种较好的实践是通过一个统一的交付包单独管理不同环境下的配置文件，或是采用配置管理系统——这对微服务而言显得尤为必要。</p>

<h4>服务－主机映射</h4>

<p>部署的另一个问题就是，一个物理机（或容器）上能运行多少服务？面对不同的选项，部署方案也会有所区别。</p>

<p>首先是多服务－单主机模式，好处是简单、方便、成本低。困难在于，监控复杂、服务部署流程也会遇到很多问题，例如如何保证不同服务间的部署代码不存在冲突，而这在微服务系统中并不少见。</p>

<p>其次是应用容器模式，这里的应用容器主要是指.Net或Java Servlet Container等充当应用服务器的容器，适当的统一能避免冲突，但也会带来一些限制。服务间的独立性也难以保证。</p>

<p>单服务－单主机模式，该方法类似于把应用部署在一个类PaaS系统上，当然灵活性也比商用PaaS高，彻底独立带来的好处就是微服务的优势，成本也是显而易见。</p>

<p>PaaS模式，更加简单、便捷的方式，缺点是灵活度低，特别是当需要触及底层改动时，PaaS显得无能为力，但从宏观上说确实是一个发展趋势。</p>

<h4>自动化</h4>

<p>自动化的优势贯穿本文始终，也是微服务技术发展的核心。如果不能实现自动化，微服务提供的各种实践就无法带来任何收益。因此微服务部署的方向就是：一切自动化。</p>

<h3>4.虚拟化</h3>

<p>实现可扩展的现行趋势就是虚拟化。虚拟化能够带来隔离、提高资源利用率等好处，但随着资源利用需求的提高，传统虚拟化显得力不从心。其中共有两种类型的虚拟化，一类是直接基于硬件的虚拟化，另一种是构建在操作系统上，采用层级架构实现的虚拟化，如AWS、VMWare、VSphere、Xen和KVM，这些虚拟机实例基于hypervisor之上，由后者提供资源分配和调度，并向上层应用提供支撑。可以看到，hypervisor承担着核心且重要的任务，其本身的资源需求就很可观。</p>

<h4>Vagrant</h4>

<p>Vagrant本质上只是一个部署平台，其更多用于开发和测试环境而非产品环境。Vagrant方便在本地构建一个虚拟云，能够尽可能真实的模拟在AWS上的线上虚拟环境。然而由于Vagrant实际还是基于虚拟机应用如VirtualBox或VMware，其所占用的资源是相当可观的。特别是在开发环境，开发人员在本地几乎不可能模拟一套完整的生产环境，因此合适的stub仍然十分必要。</p>

<h4>Linux容器</h4>

<p>Linux容器的目标是进一步榨取系统资源，实现原理却很简单：每个容器都基于内核进程fork，并且自身形成一个进程子树。除了LXC，像Solaris Zones、OpenVZ这些都是类似概念的实现，但没有前者更出名。LXC的好处是省去了hypervisor（但并不意味着它就不需要此类功能），相比VM更加轻量、高效。同时容器还提供了更细粒度的资源配置选项。
不过容器也并非灵丹妙药，相比VM而言只是换了一个载体，在正式项目中，你仍然会遇到hypervisor、routing，甚至security等各种挑战，且并不比解决VM来的容易。</p>

<h4>Docker</h4>

<p>本质上Docker只是一个构建在轻量容器之上的集成平台，它的好处在于一次性集成了容器provision、network、storage和version等功能，面向用户更加友好，并使得容器技术逐渐普及。
与VM相比，Docker具有容器所拥有的所有优势，同时正在产生一个完整的生态圈，例如CoreOS。Docker面临的挑战也等同于容器，特别是当你真正需要采用Docker搭建PaaS时，毕竟Docker本身几乎没有解决任何已有的技术问题。目前能够帮助用户解决重点问题的包括Google的Kubernetes、CoreOS的集群技术、甚至Deis这种直接提供基于Docker的PaaS平台，就像Heroku一样。</p>

<p>Docker可能解决长期困扰PaaS方案的一系列问题，例如缺少自定义特性（类似Heroku这种已经算做得不错），当PaaS的provision完全向用户放开，且不失易用性，其可选度才真正高于IaaS，Docker目前是这一趋势的有力支撑。</p>

<h3>5.采用一致的部署界面</h3>

<p>微服务部署与普通部署并无二致，同样需要解决横向扩展、相互依赖等问题，而采用一致的部署界面能有效提高效率，无论是基于Windows Powershell、batch、bash、Python Fabric或Ruby Capistrano等各类平台。一般来说部署界面应包含以下内容：</p>

<h4>1.实体名称</h4>

<p>部署对象的名称，例如微服务名。</p>

<h4>2.实体版本</h4>

<h4>3.部署环境</h4>

<p>例如：“deploy artifact=catalog environment=local version=local”。</p>

<p>部署界面的另一个重要部分是环境管理，环境管理的具体内容主要是一些provision的配置信息，部署工具应不限于provision工具的选择，包括puppet、chef或是ansible。</p>

<p>当前，在Deployment和Provision之间依然存在着缝隙，且其中的界限还不明确。例如对于Provision Heroku、或者AWS，或者Provision和Deployment间的相互依赖问题的解决，还缺少一个有效且一致的方案。Terraform看起来是一个潜在的选择。</p>
]]></content>
  </entry>
  
</feed>
