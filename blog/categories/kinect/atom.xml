<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kinect | Wing of Dream 梦境之翼]]></title>
  <link href="http://www.hanyi.name/blog/categories/kinect/atom.xml" rel="self"/>
  <link href="http://www.hanyi.name/"/>
  <updated>2019-06-08T13:16:02-04:00</updated>
  <id>http://www.hanyi.name/</id>
  <author>
    <name><![CDATA[Han Yi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kinect的三维重建(1)]]></title>
    <link href="http://www.hanyi.name/blog/2012/06/22/Kinect-de-san-wei-chong-jian-1/"/>
    <updated>2012-06-22T20:35:07+08:00</updated>
    <id>http://www.hanyi.name/blog/2012/06/22/Kinect-de-san-wei-chong-jian-1</id>
    <content type="html"><![CDATA[<p>有关Kinect应用开发正日新月异，稍有懈怠就会被远远甩在身后。不过，Kinect目前带给我们的仍只是一个充满无限可能的远景，正如App store能吸引年仅<a href="http://www.cnbeta.com/articles/161695.htm">11岁的开发者</a>一样，Kinect未来将对“全民开发者”产生重要推动作用。另一方面，一些基于Kinect的应用研究仍颇复杂，主要是因为一些关键环节的滞后而导致的。2011年的siggraph talks上，<a href="http://research.microsoft.com/en-us/projects/surfacerecon/">KinectFusion</a>首次展示了实时、廉价、轻便的室内场景三维重建，使得我们向着“无处不在的数字化”迈进了一大步。该项目主要由微软剑桥研究院的研究人员发起实施，研究小组目前公开发表了两篇文章：</p>

<p>Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, and Andrew Fitzgibbon, <strong>KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera</strong>, ACM Symposium on User Interface Software and Technology, October 2011</p>

<p>Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon, <strong>KinectFusion: Real-Time Dense Surface Mapping and Tracking</strong>, in IEEE ISMAR, IEEE, October 2011</p>

<p>除了Microsoft Research外，国内外大量机构都借助Kinect进行相关研究。仅就实时场景三维重建这项应用而言，由开源机器人研发公司Willow Garage在2011年发起的Point Cloud Libary (PCL)就吸引了全世界数十家著名科研机构参与、十余家公司提供经济支持。PCL是一项集点云获取与处理、滤波、特征提取、关键点标定、表面重建和点云配准以及点云融合等功能为一体的开源点云处理库，PCL使用OpenNI作为系统IO接口，实际成为KinectFusion的开源实现项目，该项目的相关论文发表在<a href="http://www.pointclouds.org/assets/pdf/pcl_icra2011.pdf">ICRA2011</a>上。</p>

<p>上述文献可以说是迄今为止有关Kinect的三维重建应用的最佳切入点了。应注意的是这两篇文章讨论重点的区别，前者先是对KinectFusion整个项目进行了流水式说明，随后解释了文中采用的经典算法的GPU实现，开发者可能会比较感兴趣；而后一篇文章则重点讨论了新的并行算法的形式化描述和性能分析，这部分可能会更吸引一部分研究者。我们将采用一种自上而下的方法先对这两篇文章进行简要介绍。</p>

<p><strong> 概述</strong></p>

<p>有关Kinect的结构光技术读者可以参考这篇<a href="http://www.hanyi.name/blog/?p=335">文章</a>或直接去Google下PrimeSense的专利，这里不再赘述。不过，尽管Kinect在获取深度图方面几乎是达到了性能与质量的完美结合，但仍无法避免深度图中大量抖动的产生，如果我们直接对单张深度图进行bilateral filter双边滤波然后重建出网格模型，会发现重建出的模型表面质量较低，甚至出现孔洞的情况。更重要的是，单张深度图能够获得的只是模型在某个视角下所展现出的一部分，而非表示完整的模型。因此，如何改善基于Kinect深度图的模型重建质量，并实现物体的完整重建成为该项技术的关键。</p>

<p>KinectFusion的首要任务就是克服该难题，它允许用户手持Kinect设备在室内场景自由移动，并实现场景的高质量三维重建。同时，KinectFusion还提供了精彩的AR应用，包括前景、背景和人体的分割与重建、以及AR世界中的多点触摸识别技术。</p>

<p><strong>场景重建</strong></p>

<p>基于上述需求，KinectFusion允许用户手持Kinect设备自由探索室内环境，系统将自动跟踪Kinect摄像头的6DOF姿态，然后融合不同时序的深度图数据并重建出场景的全局模型。由于视角的不断变化，Kinect反馈的深度图也会发生改变，这里采用了类似图像超分辨率技术对深度图进行了细节优化，从而提高模型的重建质量。最后利用Kinect自带的RGB数据进行场景纹理映射。</p>

<p>首先如何利用单Kinect摄像头去跟踪它自己的6DOF姿态？这里采用点云模型的刚性配准对齐来计算摄像头在空间中的6DOF变换，点云配准首选经典的ICP[Besl92, RusinKiewicz01]。之前需要将深度图像数据变换至摄像机的空间坐标系中，并计算其对应点的法线信息；然后逐帧采用基于GPU的ICP实现进行模型配准和6DOF计算；在重建部分，KinectFusion并没有直接融合点云或生成网格模型，而是采用了[Curless96]的体集成算法，最终在全局坐标系中生成一个三维体素网格并进行不断更新，每个体素内最终保存了一段时间内从该体素到物理表面上某一点的平均距离。最后使用Raycast给出隐式表面的渲染结果，另一方面，以摄像机位置作为视点做Raycasting，同样能够得到一个具有真实细节的高质量的合成深度图，然后再对其进行下一轮ICP迭代。这就允许我们能够利用合成深度图与下一帧的深度图进行配准，使重建结果的精度不断提高。</p>

<p>具体算法实现采用了CUDA并行架构，算法基本流程如下：</p>

<p>1）深度坐标变换，对于每个像素启用一只CUDA线程，给定一个Kinect红外摄像头的内部校准矩阵K，使用如下公式</p>

<p><img class="aligncenter" title="\textup{v}_{i}(u) = \textup{D}_{i}(u)\textup{K}^{-1}[u ,1]" src="http://latex.codecogs.com/gif.latex?\textup{v}_{i}(u) = \textup{D}_{i}(u)\textup{K}^{-1}[u ,1]" alt="" /></p>

<p>计算坐标变换；相应的顶点法线则直接取其右、下邻向量的外积，并进行归一化。时间i时的6DOF摄像机姿态使用一个刚性变换矩阵T表示，其中T包括了摄像机的旋转矩阵R和平移矩阵t。变换后的顶点和法线通过T即可再变换至全局坐标系中。</p>

<p>2）摄像机跟踪，该部分的主要目的就是计算6DOF姿态。核心的ICP即迭代最近点算法，首要是需要逐帧计算不同朝向的点集的相关度。这里采用了projective data association方法计算相关度。</p>

<p>3）体集成，针对已配准的点云数据，需要执行后续的融合处理，这里采用了经典的[Curless96]体集成方法融合这些点云数据。</p>

<p>4）光线投射渲染，采用光线投射渲染前步生成的隐式表面。</p>

<p>上述整个管线均采用了并行GPU实现，在下文中，我们将重点解析上述算法的CUDA实现细节。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[有关Beta 2和Kinect for Windows商用前瞻]]></title>
    <link href="http://www.hanyi.name/blog/2011/11/13/you-guan-Beta-2-he-Kinect-for-Windows-shang-yong-qian-zhan/"/>
    <updated>2011-11-13T00:07:03+08:00</updated>
    <id>http://www.hanyi.name/blog/2011/11/13/you-guan-Beta-2-he-Kinect-for-Windows-shang-yong-qian-zhan</id>
    <content type="html"><![CDATA[<p>在本月初Kinect正式发布一周年之际，微软公司副总裁Frank X. Shaw在公司官方博客上发表纪念文章“<a href="http://blogs.technet.com/b/microsoft_blog/archive/2011/10/31/feeling-the-kinect-effect.aspx" target="_blank">Feeling the Kinect Effect</a>”，并透露Kinect的商用计划“Kinect for Windows commercial program”，该项目将于2012年初正式公布。文章称专门负责商用计划的团队已获得来自超过25个国家的知名公司共超过200项商业应用申请，这些应用覆盖了20多个不同领域，这恐怕会令即将到来的2012年成为名副其实的“Kinect年”。</p>

<p>而在上个月27号，微软注册了kinectforwindows.org域名并将其作为Kinect for Windows项目新的官方网站，紧接着Kinect for Windows SDK 1.0 Beta 2在本月5号正式发布。相较于8月份后就再没更新过的Beta 1，新版本SDK在以下功能上进行了持续改进：</p>

<p>1、骨骼跟踪功能得到显著增强，在跟踪精度、计算和传输性能上有了明显提升。此外骨骼跟踪现在能够正确支持多核处理器，在同时使用2个Kinect时，开发人员也可以指定专门用于骨骼跟踪的设备了；</p>

<p>2、API现在能够对设备的状态进行有效检测和管理了，例如device unplugged, device plugged in, power unplugged等，这样应用程序就能在系统从待机状态恢复时自动重新连接设备了。一个很好的程序示例被写进了新版本的Shape Game Demo里；</p>

<p>3、在WPF程序中使用语音功能的开发人员不需要从额外的线程访问DMO了，现在可以直接从UI线程创建KinectAudioSource，从而能够简化现有工程的代码。</p>

<p>4、新的驱动程序、运行时系统和SDK现在能够在<strong>Windows 8 Developer Preview</strong>中使用了。</p>

<p>5、现在能直接创建64位应用程序；</p>

<p>6、NuiImageBuffer被新的INuiFrameTexture代替,新定义在MSR_NuiImageCamera.h中，现有项目不再需要引用NuiImageBuffer.h了；</p>

<p>7、安装目录的结构进行了调整，现在的安装路径使用环境变量%KINECTSDK_DIR%定义，默认值为C:\Program Files\Microsoft SDKs\Kinect\v1.0 Beta2；</p>

<p>8、示例代码的更改包括：</p>

<p>一个新的C#程序示例：KinectAudioDemo；</p>

<p>示例程序现在被默认安装在C:\Program Files\Microsoft SDKs\Kinect\v1.0 Beta2\Samples，你需要Unzip以便查看源代码，而这里建议将源码Unzip到Program Files以外的目录去；</p>

<p>9、驱动程序和运行时系统的稳定性和性能进一步提升，<strong>尤其是在托管API层</strong>。</p>

<p>总的来看，Beta 2并未提供关键功能上的改进，更多是对现有问题的修正。此外，备受瞩目的Kinect for Windows杀手级应用仍未出现，社区反馈是目前很多未公开的商业项目在尝试OpenNI，对MS SDK反倒比较谨慎。不过从长远角度考虑，MS SDK在开发社区、系统平台和资源支撑方面拥有众多优势，明年商业化后必然会率先占据有利位置。由此联想到近期在4S上被热炒的Siri，看来自然交互的春天真的离我们不远了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KINECT来了——解析SDK(MS SDK 2)]]></title>
    <link href="http://www.hanyi.name/blog/2011/09/24/KINECT-lai-le-jie-xi-jie-xi-SDK/"/>
    <updated>2011-09-24T23:09:39+08:00</updated>
    <id>http://www.hanyi.name/blog/2011/09/24/KINECT-lai-le-jie-xi-jie-xi-SDK</id>
    <content type="html"><![CDATA[<p><strong>NUI图像数据流概述</strong></p>

<p>NUI的流数据是通过连续静态图像序列传递的。在上下文初始化阶段，应用程序将识别需要读取的流数据，并对其进行附加的流相关设置，包括数据解析度、图像类型、用于存储输入帧的缓冲区数量等内容。在应用程序检索并释放相关帧之前，如果运行时数据占满了缓冲区，那么系统将自动丢弃最旧的帧并重用缓冲区，也就是说，帧数据是可被丢弃的。同时系统最多允许请求四个缓冲区，而在大多数应用情形下通常只需要其中的两个。应用程序可通过API获取如下类型的图像信息：彩色图像数据、深度图像数据、用户分割数据等。下面将分别对上述三种类型的数据进行一些说明。</p>

<p>彩色图像数据：系统提供两种格式的彩色图像数据，包括32位X8R8G8B8格式的sRGB位图数据和16位的UYVY格式的YUV位图数据。由于两者实际来自同一图像数据，因此两种格式的最终图像实际上没有任何差别。不过后者要求图像保持640x480的分辨率和15FPS的帧率，同时内存需求更小。应注意，由于系统采用USB连接，传感器层会首先将1280x1024分辨率的Bayer彩色滤波马赛克图像压缩并转换为RGB格式传输，运行时系统再对该数据进行解压缩操作。上述特性可以保证数据帧率可达30FPS，但解码操作会损失一定的图像精度。</p>

<p>深度图像数据：深度图像帧中的每一个像素点都表示从摄像头所在平面到视野内最近物体的笛卡尔坐标系距离，单位为毫米。系统目前支持630x480、320x240、80x60三种规格的深度图像帧。应用程序可根据深度图像数据跟踪人物动作、识别并忽略背景物体。深度图像数据含有两种格式，其一是唯一表示深度值：那么像素的低12位表示一个深度值，高4位未使用；其二是既表示深度值又含有人物序号，则低三位保存人物序号，其余数据位表示深度值。应注意如果获取的深度值为0，则说明物体距离摄像头过近或过远以致超出了设备规格。</p>

<p>用户分割数据：SDK beta目前支持读取两个用户的分割映射数据，其数据帧的相关像素分别记录了用户的序号。尽管用户分割数据是独立生成的数据流，在实际应用中仍可以将深度数据和用户分割数据整合成一个帧，其中像素值的高13位保存了深度值，低三位保存用户序号，其中序号为0则表示无用户，1和2分别表示两个不同的用户。在实际应用中，应用程序往往利用用户分割数据在深度图像和原始彩色图像中获取ROI感兴趣信息。</p>

<p><strong> 基本编程模型</strong></p>

<p>基于NUI API的编程模型本质上就是获取传感器图像数据的过程。应用程序往往通过相关代码首先将图像的最后一帧读入至缓冲区中，如果该帧已预备好，那么其将进入缓冲区，如果帧数据尚未就绪，代码仍可选择是否继续挂起等待或暂时释放并稍后重试。NUI摄像头API绝对不会多次传递相同的图像数据。框架包含的基本编程模型如下：</p>

<p>1、POLLing模型，该模型较为基础易用。首先应开启图像数据流，然后请求并设置等待下一帧的时间，范围允许从0到无穷大，单位为毫秒；如果帧数据尚未就绪，则系统将等待刚才指定的时间然后返回。如果帧数据成功返回，则应用程序可请求下一帧数据并在同一线程执行其它操作。通常一个C++应用程序应调用NuiImageStreamOpen函数首先启动一个彩色或深度数据流，并忽略可选事件。托管代码则需调用ImageStream.Open方法。请求彩色或深度图像帧的C++函数为NuiImageStreamGetNextFrame，C#为ImageStream.GetNextFrame方法。</p>

<p>2、事件模型，事件模型允许将获取骨骼帧的功能精确、灵活地集成入应用程序引擎。在该模型中，C++程序首先调用NuiImageStreamOpen函数并传入一个事件句柄。每当一个新的图像帧数据可用时，事件信号将被触发。任何相关的等待线程将被唤醒并通过调用NuiImageGetNextFrame函数获取骨骼信息，与此同时事件将被系统重置。托管代码应绑定Runtime.DepthFrameReady和Runtime.ImageFrameReady事件到相关的处理函数，当新数据可用时，处理函数可调用ImageStream.GetNextFrame获取该数据。</p>

<p><strong>NUI骨骼跟踪应用</strong></p>

<p>NUI还包括一个Skeleton骨骼跟踪模块，该部分提供最多两名用户的详细位置和朝向信息。骨骼跟踪的输出是一个点集，称作Skeleton Positions，该点集表示了一个完整的人体骨骼信息，如下图所示。</p>

<p>[singlepic id=44 w=320 h=240 mode=watermark float=center]</p>

<p>骨骼位置信息表示了用户当前的位置和姿态，如果要使用骨骼跟踪功能，应用程序应在NUI初始化阶段设置相关内容。NUI骨骼数据获取的方式与NUI Image部分基本一致，其基本编程模型也包括Polling和Event两种。前者的C++函数为NuiSkeletonGetNextFrame，托管函数为SkeletonEngine.GetNextFrame；后者的C++句柄绑定操作由NuiSkeletonTrackingEnable完成，并在处理线程内调用NuiSkeletonGetNextFrame；C#则使用Runtime.SkeletonFrameReady绑定事件，然后调用SkeletonEngine.GetNextFrame获取相关信息。</p>

<p>骨骼跟踪模块通过深度数据计算地板裁切面，其基本方法将在后文进行介绍。如果应用程序在NUI初始化时开启了骨骼跟踪功能，则其每处理完一套深度数据则就放出相应的骨骼数据信号，而无论该深度数据中是否真的包含骨骼信息。应用程序使用地板裁切面数据获取骨骼框架，返回的骨架信息将携带一个时间戳以和相关的深度信息进行匹配。</p>

<p>NUI骨骼跟踪分主动和被动两种模式，提供最多两副完整的骨骼跟踪数据。主动模式下需要调用相关帧读取函数获得用户骨骼数据，而被动模式下还支持额外最多四人的骨骼跟踪，但是在该模式下仅包含了用户的位置信息。对于所有获取的骨骼数据，其至少包含以下信息：</p>

<p>1、相关骨骼的跟踪状态，被动模式时仅包括位置数据，主动模式包括完整的骨骼数据。</p>

<p>2、唯一的骨骼跟踪ID，用于分配给视野中的每个用户。</p>

<p>3、用户质心位置，该值仅在被动模式下可用。</p>

<p>4、对于主动模式下的骨骼跟踪数据，还包括用户完整的骨骼数据。</p>

<p>5、对于被动模式下的骨骼跟踪数据，仅包括用户位置信息，不包括详细的骨骼数据。</p>

<p><strong>NUI坐标变换原理</strong></p>

<p>深度图像空间：这是一个仅包含物体到传感器法平面的深度数据的投影空间，其z值是唯一有效的，而x、y值仅仅是进行了插值计算的结果，其数据实际并无任何物理意义；</p>

<p>骨骼空间：用户骨骼位置使用x、y、z三维坐标表示，与深度图像空间坐标系不同的是，该空间的单位为米，且是一个传感器朝向为z轴正向的右手系。应注意，骨骼空间坐标系与Kinect位置息息相关，如果传感器被放置在一个非水平面上，那么计算得到的坐标系可能并非是标准形式，最终的用户数据也有可能是倾斜的。</p>

<p>地板裁剪面的确定：骨骼数据均需要包含一个地板裁剪面向量，该向量保存了与地板平面方程有关的系数coefficients，骨骼跟踪模块通过地板裁剪平面除去背景，并将用户图像分割出来。对于平面的一般式方程Ax+By+Cz+D=0，该方程经过规范化即D值实际是指传感器到地板平面的高度值。如果地板不可见，那么地板裁剪平面向量实际为0。地板裁剪面向量值可在NUI_SKELETON_FRAME结构的vFloorClipPlane成员中获取。托管代码则保存在SkeletonFrame.FloorClipPlane域中。</p>

<p>骨骼镜像：默认的用户图像实际上是一个镜像数据，也就是说该数据表示了用户当前面朝屏幕内部。然而有时确实需要图像面向用户本人一侧，那么需要确定一个镜像变换矩阵以达到此类要求。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KINECT来了——解析SDK(MS SDK 1)]]></title>
    <link href="http://www.hanyi.name/blog/2011/09/17/KINECT-lai-le-jie-xi-jie-xi-SDK/"/>
    <updated>2011-09-17T23:41:18+08:00</updated>
    <id>http://www.hanyi.name/blog/2011/09/17/KINECT-lai-le-jie-xi-jie-xi-SDK</id>
    <content type="html"><![CDATA[<p>在8月初的<a href="http://www.hanyi.name/blog/?p=330" target="_blank">文章</a>里，我们曾对Microsoft推出的官方Kinect SDK beta进行了初步介绍，本文将在此基础上对其做出进一步说明。值得注意的是对托管代码和非托管代码的选择，从社区反馈看来似乎前者占据了绝对上风，coding4fun推荐的项目也基本都是以WPF编程为主。不过考虑到平台的潜在可扩展，这里还是推荐使用标准C++构建非托管程序。下面先给出两种编程方式在VS2010中的配置方法：</p>

<p>对于托管C#程序，添加Add Reference中.Net标签页的Microsoft.Research.Kinect.dll，使用using Microsoft.Research.Kinect.Nui引用NUI API的声明文件，如果还要引用音频API，还要加上using Microsoft.Research.Kinect.Audio。</p>

<p>对于标准C++程序，代码中必须加入&lt;windows.h&gt;头文件，其中NUI API需要包含MSR_NuiApi.h头文件、音频API需要包含MSRKinectAudio.h。另外需要添加相应的静态链接库，同时确保安装的DLL文件存放在PATH路径中。这里给出SDK所包含的头文件说明：</p>

<p>MSR_NuiApi.h，该文件包含所有关于NUI API声明，包括初始化调用和访问函数，如NuiInitialize、NuiShutdown、MSR_NuiXxx以及INuiInstance，其主要功能是枚举设备并对其进行访问调用。MSR_NuiImageCamera.h，该文件包含对NUI图像和摄像头功能API的声明，其一般形式为NuiCameraXxx和NuiImageXxx：此类API的功能为调整摄像头倾角和仰角，并启动数据流并读入图像帧。MSR_NuiProps.h，该文件包含对NUI属性枚举API函数的声明。MSR_NuiSkeleton.h，该文件包括NUI骨架API函数的声明，其一般形式为NuiSkeletonXxx and NuiTransformXxx：功能为打开/关闭骨架跟踪、获取骨架数据、将骨架数据进行变换以实现平滑绘制。MSRKinectAudio.h，该文件包括对音频API函数的声明，其中ISoundSourceLocalizer接口可返回波束方向和音源位置。NuiImageBuffer.h，该文件定义了一个帧缓冲器，其作用类似DirectX9的纹理缓冲。</p>

<p><strong>Kinect for Windows应用程序架构</strong></p>

<p>[singlepic id=43 w=320 h=240 mode=watermark float=center]</p>

<p>上图给出了beta SDK的整体组件架构，硬件层我们已经在上月进行了详细介绍。驱动的内核模式包含了设备驱动程序，上层的数据交互统一使用WinUSB数据栈，其中设备栈主要用于设备的配置和访问，摄像头栈用于视频数据流控制，USBAudio栈用于音频数据流控制；用户模式为API提供了访问和控制接口。应用层API上包括了三部分组件，其中MS SDK beta直接提供了NUI API即基本API集和KinectAudio DMO，后者的功能主要是提供波束成形和音源定位功能。此外，如果要使用Kinect的所有功能，还需要自己准备Windows 7 SDK中的音频、语音、媒体API集以及微软语音识别SDK，上述组件并未包含在SDK中。近两篇文章会主要介绍NUI API部分的设计原理和编程说明。</p>

<p><strong> NUI API概述</strong></p>

<p>NUI API是Kinect SDK的核心组成部分，其主要功能包括：提供连接至PC的Kinect传感器元件的访问接口、提供对由Kinect成像传感器生成的图像和深度数据流访问接口、通过经处理的图像和深度数据实现骨骼跟踪。下面我们分别使用托管和非托管代码介绍整个Kinect API上下文环境的创建和销毁。</p>

<p>与OpenNI不同的是，Kinect SDK允许方便地同时使用多台Kinect设备，但是每台设备（或传感器）同一时间只能被一个程序实例使用。<strong>下面是使用C++代码实现传感器枚举和访问的整个过程：</strong></p>

<p>情形一：应用程序同一时间仅使用一台Kinect设备：</p>

<p>1、调用NuiInitialize，该函数首先会初始化一个Kinect传感器设备实例；</p>

<p>2、调用NuiXxx，该函数集的功能是传输图像和骨骼数据流，并管理并配置相关摄像头；</p>

<p>3、调用NuiShutdown结束。</p>

<p>情形二：应用程序同一时间使用多台Kinect设备：</p>

<p>1、调用MSR_NuiDeviceCount，查看可用Kinect设备的数量；</p>

<p>2、调用MSR_NuiCreateInstanceByIndex创建指定设备索引的实例，该函数将返回该实例的INuiInstance接口指针；</p>

<p>3、调用INuiInstance::NuiInitialize，该函数用于初始化针对该实例的NUI API上下文；</p>

<p>4、调用针对INuiInstance接口的其它方法，用于传输图像和骨骼数据流并管理相关摄像头；</p>

<p>5、调用INuiInstance::NuiShutdown，销毁某个设备实例的NUI API上下文；</p>

<p>6、调用MSR_NuiDestroyInstance函数销毁该实例；</p>

<p><strong>接下来是使用C#代码实现传感器枚举和访问的整个过程：</strong></p>

<p><strong></strong>情形一：应用程序同一时间仅使用一台Kinect设备：</p>

<p>1、创建一个新的运行时对象，将其参数列表留空，例如nui = new Runtime()，调用该构造函数表示在系统中创建了一个Kinect传感器设备实例；</p>

<p>2、调用Runtime.Initialize函数初始化NUI API上下文；</p>

<p>3、调用其它托管接口，用于传输图像和骨骼数据流，并管理和配置相关摄像头；</p>

<p>4、调用Runtime.Shutdown销毁实例。</p>

<p>情形二：应用程序同一时间使用多台Kinect设备：</p>

<p>1、调用MSR_NuiDeviceCount函数获取可用设备数量；</p>

<p>2、创建一个新的运行时对象，参数为设备索引号，如nui = new Runtime(index)，调用该构造函数表示创建了针对某个Kinect传感器的系统实例；</p>

<p>3、调用Runtime.Initialize函数初始化NUI API上下文；</p>

<p>4、调用其它托管接口方法，用于传输图像和深度数据、并管理和配置相关摄像头；</p>

<p>5、调用Runtime.Shutdown函数销毁实例。</p>

<p>NUI API的数据处理是通过一个多级管线实现的，在初始化阶段，应用程序需要指定相应级别的子系统，这样运行时系统才能系统所需的部分管线。在初始化时系统通常允许进行以下配置：</p>

<p>彩色：应用程序指定从设备中获取彩色图像数据流；</p>

<p>深度：应用程序指定从设备中获取深度图像数据流；</p>

<p>深度和人物索引：应用程序指定从设备中获取深度数据流，并请求骨骼跟踪引擎当前生成的人物索引；</p>

<p>骨骼：获取骨骼位置数据；</p>

<p>上述设置指定了有效数据流类型和解析度，例如当一个应用程序没有在NUI API初始化阶段指定深度数据时，它随后也无法开启一个深度数据流。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kinect来了——解析SDK(OpenNI Framework 4)]]></title>
    <link href="http://www.hanyi.name/blog/2011/09/03/Kinect-lai-le-jie-xi-jie-xi-SDK/"/>
    <updated>2011-09-03T21:48:15+08:00</updated>
    <id>http://www.hanyi.name/blog/2011/09/03/Kinect-lai-le-jie-xi-jie-xi-SDK</id>
    <content type="html"><![CDATA[<p><strong>配置工作节点的一般方法</strong></p>

<p>前文的例子中已经说明了在程序中动态配置节点信息的一般过程，节点配置完成的主要标志一般是对xn::Generator::StartGenerating()函数的调用。除此之外，OpenNI还提供了基于外置XML文件的节点配置方法，使用XML配置文件能够显著降低代码复杂度，同时提高应用程序的可用性和灵活性，本文将主要讨论上述方式结合OpenNI编程。</p>

<p>OpenNI支持的XML脚本功能非常强大，仅使用外置的文件就能够实现节点的创建、配置，甚至操作上下文属性（例如增加相应的许可证等）。通常可以使用xn::Context::RunXmlScript()直接执行脚本字符串，或通过xn::Context::RunXmlScriptFromFile()调用外部文件。</p>

<p>一个XML内容中必须包含一个OpenNI结点，该结点内共包含三个子结点：Licenses，Log和Production Nodes。其中Licenses结点提供了系统需要的额外的许可证，其格式一般如下：</p>

<p>[c]&lt;Licenses&gt;
&lt;License vendor=&quot;vendor1&quot; key=&quot;key1&quot;/&gt;
&lt;License vendor=&quot;vendor2&quot; key=&quot;key2&quot;/&gt;
&lt;/Licenses&gt;[/c]</p>

<p>Log结点用于配置OpenNI的日志系统，该结点内可添加下列可选元素属性：writeToConsole，设置日志信息是否在控制台中显示，默认为false。writeToFile:，设置日志信息是否将被写入文件，该文件将被放置在工作目录下，默认为false。writeLineInfo，设置是否每一条日志记录都需要包含文件名和行信息，默认为true。此外，Log还包含三个子结点：LogLevel，携带value属性，其值为0 (verbose), 1 (info), 2 (warnings) or 3 (errors)，其中3为默认值，该结点决定了日志记录的粒度。Masks，包含一个mask元素列表，设置相应mask的开启和关闭。Dumps，包含一个dump元素列表，设置相应dump的开启和关闭。下面的XML脚本演示了一段日志配置信息：</p>

<p>[c]&lt;Log writeToConsole=&quot;false&quot; writeToFile=&quot;false&quot; writeLineInfo=&quot;true&quot;&gt;
&lt;LogLevel value=&quot;3&quot;/&gt;
&lt;Masks&gt; &lt;Mask name=&quot;ALL&quot; on=&quot;true&quot; /&gt; &lt;/Masks&gt;
&lt;Dumps&gt; &lt;Dump name=&quot;SensorTimestamps&quot; on=&quot;false&quot; /&gt; &lt;/Dumps&gt;
&lt;/Log&gt;[/c]</p>

<p>ProductionNodes结点包含了工作结点的创建和配置信息，它包括了若干个子结点：
GlobalMirror，设置Global Mirror属性的开启和关闭，相当于调用xn::Context::SetGlobalMirror()函数；
Recording，设置是否启动一个记录，其file属性包括了记录文件名；
Node，该子结点可以包含多个，它将引导OpenNI枚举并创建一个工作结点，其作用类似xn::Context::CreateAnyProductionTree()函数。其type属性表示枚举类型，值可包括下列内容：Device (XN_NODE_TYPE_DEVICE)</p>

<p>Depth (XN_NODE_TYPE_DEPTH)</p>

<p>Image (XN_NODE_TYPE_IMAGE)</p>

<p>IR (XN_NODE_TYPE_IR)</p>

<p>Audio (XN_NODE_TYPE_AUDIO)</p>

<p>Gesture (XN_NODE_TYPE_GESTURE)</p>

<p>User (XN_NODE_TYPE_USER)</p>

<p>Scene (XN_NODE_TYPE_SCENE)</p>

<p>Hands (XN_NODE_TYPE_HANDS)</p>

<p>Recorder (XN_NODE_TYPE_RECORDER)</p>

<p>Node结点的name属性允许设置一个工作节点名称。</p>

<p>Query，Node还含有一个Query子结点，用于枚举时的查询操作。Query可包含下列子结点：&#8221;Vendor&#8221;、&#8221;Name&#8221;、&#8221;MinVersion&#8221;、&#8221;MaxVersion&#8221;、&#8221;Capabilities&#8221;、&#8221;MapOutputModes&#8221;、&#8221;MinUserPositions&#8221;、&#8221;NeededNodes&#8221;，上述结点均可设置不同的子结点和属性，用于查询条件的选择，如果有多个子结点同时出现，那么OpenNI将按照AND进行逻辑整合。下列XML脚本演示了创建一个自定义属性的深度生成器：</p>

<p>[c]&lt;Node type=&quot;Depth&quot; name=&quot;MyDepth&quot;&gt;
&lt;Query&gt;
&lt;Vendor&gt;vendor1&lt;/Vendor&gt;
&lt;Name&gt;name1&lt;/Name&gt;
&lt;MinVersion&gt;1.0.0.0&lt;/MinVersion&gt;
&lt;MaxVersion&gt;3.1.0.5&lt;/MaxVersion&gt;
&lt;Capabilities&gt; &lt;Capability&gt;UserPosition&lt;/Capability&gt; &lt;Capability&gt;Mirror&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;/MapOutputModes&gt; &lt;MinUserPositions&gt;2&lt;/MinUserPositions&gt;
&lt;NeededNodes&gt; &lt;Node&gt;MyDevice&lt;/Node&gt;&lt;/NeededNodes&gt;
&lt;/Query&gt;
&lt;/Node&gt;[/c]</p>

<p>Configuration子结点实际上代表了对工作节点的动态配置，其指令内容将是顺序执行的。同时，该结点几乎对应了OpenNI所有的Set配置操作，下面的例子演示了分别创建图像、深度和音频节点的过程：</p>

<p>[c]&lt;ProductionNodes&gt; &lt;Node type=&quot;Image&quot;&gt;
&lt;Query&gt;
&lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;320&quot; yRes=&quot;240&quot; FPS=&quot;60&quot;/&gt; &lt;/MapOutputModes&gt;
&lt;Capabilities&gt; &lt;Capability&gt;Cropping&lt;/Capability&gt; &lt;Capability&gt;Mirror&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;/Query&gt;
&lt;Configuration&gt; &lt;MapOutputMode xRes=&quot;320&quot; yRes=&quot;240&quot; FPS=&quot;60&quot;/&gt; &lt;PixelFormat&gt;RGB24&lt;/PixelFormat&gt; &lt;Cropping enabled=&quot;true&quot; xOffset=&quot;28&quot; yOffset=&quot;28&quot; xSize=&quot;200&quot; ySize=&quot;160&quot; /&gt; &lt;Mirror on=&quot;true&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt; &lt;Node type=&quot;Depth&quot;&gt;
&lt;Query&gt; &lt;Vendor&gt;VendorX&lt;/Vendor&gt; &lt;MapOutputModes&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;/MapOutputModes&gt;&lt;Capabilities&gt; &lt;Capability&gt;UserPosition&lt;/Capability&gt; &lt;/Capabilities&gt;
&lt;/Query&gt;
&lt;Configuration&gt; &lt;MapOutputMode xRes=&quot;640&quot; yRes=&quot;480&quot; FPS=&quot;30&quot;/&gt; &lt;UserPosition index=&quot;0&quot;&gt; &lt;Min x=&quot;128&quot; y=&quot;128&quot; z=&quot;500&quot;/&gt; &lt;Max x=&quot;600&quot; y=&quot;400&quot; z=&quot;2000&quot;/&gt; &lt;/UserPosition&gt; &lt;Property type=&quot;int&quot; name=&quot;VendorXDummyProp&quot; value=&quot;3&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt;
&lt;Node type=&quot;Audio&quot;&gt;
&lt;Configuration&gt; &lt;WaveOutputMode sampleRate=&quot;44100&quot; bitsPerSample=&quot;16&quot; channels=&quot;2&quot; /&gt; &lt;/Configuration&gt;
&lt;/Node&gt;
&lt;/ProductionNodes&gt;[/c]</p>

<p>通常在上述配置信息载入后，应手动调用xn::Context::StartGeneratingAll()函数启动数据流。然而也可以在XML中加入默认的Start信息，即在ProductionNodes和其Node子结点中均包含有startGenerating属性，其中如果前者的该属性为true，那么将意味着执行StartGeneratingAll()，否则只执行相应为true的工作节点。</p>
]]></content>
  </entry>
  
</feed>
